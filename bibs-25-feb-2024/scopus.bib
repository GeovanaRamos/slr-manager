Scopus
EXPORT DATE: 25 February 2024

@CONFERENCE{Siddik2022146,
	author = {Siddik, Sayed Abu Noman and Arifuzzaman, B.M. and Kalam, Abul},
	title = {Psyche Conversa - A Deep Learning Based Chatbot Framework to Detect Mental Health State},
	year = {2022},
	journal = {2022 10th International Conference on Information and Communication Technology, ICoICT 2022},
	pages = {146 – 151},
	doi = {10.1109/ICoICT55009.2022.9914844},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141561034&doi=10.1109%2fICoICT55009.2022.9914844&partnerID=40&md5=70d694347e2d34765f1de197d37a837a},
	abstract = {Mental health is one of the most pressing challenges in today's modern world. Traditional thinking, family pressure, unemployment issues, homesickness, and an unhappy relationship are the most common reasons for mental illness, which may also lead to suicidal attempts. People in impoverished countries do not give this issue enough attention. Furthermore, many are hesitant to seek professional counseling from a psychiatrist to retain their privacy. Monitoring social media activities, in addition to common symptoms, is critical for improving the accuracy of detecting early signs of mental illness because it impacts mental health. So, rather than a psychiatrist, a user-friendly deep learning-based mobile application that will chat with them to support and monitor their social media behaviors to determine their mental health status will be an effective way of handling the situation. This study proposes a deep learning-based chatbot framework to help mentally ill individuals identify their mental health conditions and provide appropriate therapy. This framework comprises three components: a keylogger module, a chat module, and a deep learning-based mental illness detection module. In the background, the keylogger collects data from the user's keyboard to track social media activity. The chatbot converses with users and stores their daily chat history in real-time. Both modules pass the data to a deep learning model to determine mental health conditions. This study also compared the accuracy of multiple deep learning classifiers such as Conv-LSTM and BERT for the Reddit Mental Health Dataset.  © 2022 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Macdonald2023,
	author = {Macdonald, Calum and Adeloye, Davies and Sheikh, Aziz and Rudan, Igor},
	title = {Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis},
	year = {2023},
	journal = {Journal of Global Health},
	volume = {13},
	doi = {10.7189/JOGH.13.01003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148258728&doi=10.7189%2fJOGH.13.01003&partnerID=40&md5=c23fae3c463e3087a61de2c24bde1f93},
	abstract = {We reflect on our experiences of using Generative Pre-trained Transformer ChatGPT, a chatbot launched by OpenAI in November 2022, to draft a research article. We aim to demonstrate how ChatGPT could help researchers to accelerate drafting their papers. We created a simulated data set of 100 000 health care workers with varying ages, Body Mass Index (BMI), and risk profiles. Simulation data allow analysts to test statistical analysis techniques, such as machine-learning based approaches, without compromising patient privacy. Infections were simulated with a randomized probability of hospitalisation. A subset of these fictitious people was vaccinated with a fictional vaccine that reduced this probability of hospitalisation after infection. We then used ChatGPT to help us decide how to handle the simulated data in order to determine vaccine effectiveness and draft a related research paper. AI-based language models in data analysis and scientific writing are an area of growing interest, and this exemplar analysis aims to contribute to the understanding of how ChatGPT can be used to facilitate these tasks © 2023 THE AUTHOR(S)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 64}
}

@CONFERENCE{Foosherian202356,
	author = {Foosherian, Mina and Purwins, Hendrik and Rathnayake, Purna and Alam, Touhidul and Teimao, Rui and Thoben, Klaus-Dieter},
	title = {Enhancing Pipeline-Based Conversational Agents with Large Language Models},
	year = {2023},
	journal = {Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the Era of Interactive Assistants!, TLLM 2023},
	pages = {56 – 67},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180830503&partnerID=40&md5=4d9348d8aef9d698b88b98f659f035c3},
	abstract = {The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT- 4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example. Companies may be hesitant to replace their pipeline-based agents with LLMs entirely due to privacy concerns and the need for deep integration within their existing ecosystems. A hybrid approach in which LLMs' are integrated into the pipeline-based agents allows them to save time and costs of building and running agents by capitalizing on the capabilities of LLMs while retaining the integration and privacy safeguards of their existing systems.  © 2023 Association for Computational Linguistics.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zheng2022,
	author = {Zheng, Qingxiao and Tang, Yiliu and Liu, Yiren and Liu, Weizi and Huang, Yun},
	title = {UX Research on Conversational Human-AI Interaction: A Literature Review of the ACM Digital Library},
	year = {2022},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3491102.3501855},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130572358&doi=10.1145%2f3491102.3501855&partnerID=40&md5=eb84b38295ceed7d799a491b95120418},
	abstract = {Early conversational agents (CAs) focused on dyadic human-AI interaction between humans and the CAs, followed by the increasing popularity of polyadic human-AI interaction, in which CAs are designed to mediate human-human interactions. CAs for polyadic interactions are unique because they encompass hybrid social interactions, i.e., human-CA, human-to-human, and human-to-group behaviors. However, research on polyadic CAs is scattered across different fields, making it challenging to identify, compare, and accumulate existing knowledge. To promote the future design of CA systems, we conducted a literature review of ACM publications and identified a set of works that conducted UX (user experience) research. We qualitatively synthesized the effects of polyadic CAs into four aspects of human-human interactions, i.e., communication, engagement, connection, and relationship maintenance. Through a mixed-method analysis of the selected polyadic and dyadic CA studies, we developed a suite of evaluation measurements on the effects. Our findings show that designing with social boundaries, such as privacy, disclosure, and identification, is crucial for ethical polyadic CAs. Future research should also advance usability testing methods and trust-building guidelines for conversational AI. © 2022 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22}
}

@ARTICLE{Contu2021,
	author = {Contu, Francesco and Demontis, Andrea and Dessì, Stefano and Muscas, Marco and Riboni, Daniele},
	title = {AI-based analysis of policies and images for privacy-conscious content sharing},
	year = {2021},
	journal = {Future Internet},
	volume = {13},
	number = {6},
	doi = {10.3390/fi13060139},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116072033&doi=10.3390%2ffi13060139&partnerID=40&md5=06e9aaa53bdcb0c5edb534c03cb114c2},
	abstract = {Thanks to the popularity of personal mobile devices, more and more of the different types of private content, such as images and videos, are shared on social networking applications. While content sharing may be an effective practice to enhance social relationships, it is also a source of relevant privacy issues. Unfortunately, users find it difficult to understanding the terms and implications of the privacy policies of apps and services. Moreover, taking privacy decisions about content sharing on social networks is cumbersome and prone to errors that could determine privacy leaks. In this paper, we propose two techniques aimed at supporting the user in taking privacy choices about sharing personal content online. Our techniques are based on machine learning and natural language processing to analyze privacy policies, and on computer vision to assist the user in the privacy-conscious sharing of multimedia content. Experiments with real-world data show the potential of our solutions. We also present ongoing work on a system prototype and chatbot for natural language user assistance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{May2024123,
	author = {May, Richard and Denecke, Kerstin},
	title = {Conversational Agents in Healthcare: A Variability Perspective},
	year = {2024},
	journal = {ACM International Conference Proceeding Series},
	pages = {123 – 128},
	doi = {10.1145/3634713.3634717},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184279614&doi=10.1145%2f3634713.3634717&partnerID=40&md5=fda32355b30697cdab050af19b545d32},
	abstract = {Conversational agents in healthcare are gaining popularity, for example, in the context of eliciting medical histories. Furthermore, due to the growing diversity of use cases and stakeholders, they are becoming increasingly configurable and are often based on variability mechanisms. In this paper, we present a high-level perspective on typical variability aspects and describe common challenges based on our research and practical experience in developing and evaluating conversational agents in the healthcare domain. We introduce variability aspects that are classified into technology-related (e.g., intelligence framework, input/output mode) and user-related aspects (e.g., careflow integration, health literacy). Moreover, these aspects are described in a case study on the Digital Medical Interview Assistant (DMIA) for radiology. We highlight main challenges that arise in the context of evolution, verification, input processing, privacy and security compliance, as well as ethical considerations. Our findings are intended to help developers, researchers, and healthcare professionals understand the importance and impact of configurability and to spur further discussions on variability aspects of conversational agents.  © 2024 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Augusma2023750,
	author = {Augusma, Anderson and Vaufreydaz, Dominique and Letué, Frédérique},
	title = {Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant Features},
	year = {2023},
	journal = {ACM International Conference Proceeding Series},
	pages = {750 – 754},
	doi = {10.1145/3577190.3616546},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175837394&doi=10.1145%2f3577190.3616546&partnerID=40&md5=ff0997c06e2749420cc3465ddf1433ea},
	abstract = {This paper explores privacy-compliant group-level emotion recognition "in-the-wild"within the EmotiW Challenge 2023. Group-level emotion recognition can be useful in many fields including social robotics, conversational agents, e-coaching and learning analytics. This research imposes itself using only global features avoiding individual ones, i.e. all features that can be used to identify or track people in videos (facial landmarks, body poses, audio diarization, etc.). The proposed multimodal model is composed of a video and an audio branches with a cross-attention between modalities. The video branch is based on a fine-tuned ViT architecture. The audio branch extracts Mel-spectrograms and feed them through CNN blocks into a transformer encoder. Our training paradigm includes a generated synthetic dataset to increase the sensitivity of our model on facial expression within the image in a data-driven way. The extensive experiments show the significance of our methodology. Our privacy-compliant proposal performs fairly on the EmotiW challenge, with 79.24% and 75.13% of accuracy respectively on validation and test set for the best models. Noticeably, our findings highlight that it is possible to reach this accuracy level with privacy-compliant features using only 5 frames uniformly distributed on the video.  © 2023 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Antonio2022296,
	author = {Antonio, Randy and Tyandra, Nadya and Nusantara, Linggar Tembus and Anderies and Agung Santoso Gunawan, Alexander},
	title = {Study Literature Review: Discovering the Effect of Chatbot Implementation in E-commerce Customer Service System Towards Customer Satisfaction},
	year = {2022},
	journal = {2022 International Seminar on Application for Technology of Information and Communication: Technology 4.0 for Smart Ecosystem: A New Way of Doing Digital Business, iSemantic 2022},
	pages = {296 – 301},
	doi = {10.1109/iSemantic55962.2022.9920434},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141507114&doi=10.1109%2fiSemantic55962.2022.9920434&partnerID=40&md5=ae03b135befbc3b877a94b17cd6f3974},
	abstract = {Customer service plays a crucial role for a company. As an important aspect of e-commerce companies, they would be required to directly interact and try to solve customers' problems that might occur anywhere and anytime. However, the limitation of human man hours became a barrier to overcome customers' problems. On one hand, the rapid development of technology was predicted to replace the traditional human customer service with an Artificial Intelligence agent. On the other hand, this replacement affects the customer satisfaction. This paper performed a study literature review to discover the effect of chatbots and its impact towards customer satisfaction. In an e-commerce customer service use case, chatbots could be implemented in a number of methods. The methods implemented by chatbots are avatar-based, verbal-based, text-based, and menu-based. Research showed that text-based chatbot is the most commonly used methodology and has advanced the most, where some are implementing higher level machine learning methods, such as deep learning. The usage of such chatbot in e-commerce customer service systems will lower the cost but might also lower customer satisfaction, due to reasons such as unsatisfying answers and inhuman behavior. Research showed that even a more sophisticated chatbot doesn't always mean higher customer satisfaction, even with high accuracy ratings. To look into customer satisfaction, this paper has identified 4 aspects of a chatbot that are relevant to customer satisfaction, which are privacy, reliability, personalization, and responsiveness. Chatbots currently excel in some of these quality measures, but require further research to effectively replace human customer service agents.  © 2022 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pedrosa2021391,
	author = {Pedrosa, Antonio de Paula and Pereira, José Cristiano and Póvoas, Marcelo and Marinato, Davi da Fonseca Vieira Junior and Bastos, Matheus Bastos de Almeida and da Costa, Jose Luís Corrêa},
	title = {Risk Assessment of Non-Compliance with General Data Protection Law (LGPD): A Necessary Adjustment for Healthcare Companies That Use Chatbots For Automated Care},
	year = {2021},
	journal = {Proceedings of the 31st European Safety and Reliability Conference, ESREL 2021},
	pages = {391 – 398},
	doi = {10.3850/978-981-18-2016-8_221-cd},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135490917&doi=10.3850%2f978-981-18-2016-8_221-cd&partnerID=40&md5=ae977d496e9998d9241751a3bec23d81},
	abstract = {With the publication of the General Data Protection Law-(LGPD) many companies having their headquarters in Brazil need to work on adapting their processes. Most companies are seeking for compliance, however, many still do not know how to proceed. The risk of legal and financial issues related to non-compliance is high. This study reviews the current percentage of companies that use a Chatbot service and are compliant with the LGPD. It also reviews the steps to adjust the Chatbot service used by companies in the European Union to be compliant with the General Data Protection Regulation-(GDPR). As a methodological approach, a search in the state-of-the-art literature was conducted to identify the most recent published related content. A survey was conducted with several companies based in the Rio de Janeiro city which use a Chatbot service and are compliant with the LGPD. As a result, a flowchart showing the steps for adapting a Chatbot service to the LGPD is presented. The risks of non-compliance are also presented. This study addresses a gap observed in the literature since no specific previous work has been found covering this topic. Many companies may benefit from this study by knowing the steps to adapt their Chatbot service to the LGPD requirements, and avoid the risks associated to non-compliance. © ESREL 2021. Published by Research Publishing, Singapore.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Følstad20212915,
	author = {Følstad, Asbjørn and Araujo, Theo and Law, Effie Lai-Chong and Brandtzaeg, Petter Bae and Papadopoulos, Symeon and Reis, Lea and Baez, Marcos and Laban, Guy and McAllister, Patrick and Ischen, Carolin and Wald, Rebecca and Catania, Fabio and Meyer von Wolff, Raphael and Hobert, Sebastian and Luger, Ewa},
	title = {Future directions for chatbot research: an interdisciplinary research agenda},
	year = {2021},
	journal = {Computing},
	volume = {103},
	number = {12},
	pages = {2915 – 2942},
	doi = {10.1007/s00607-021-01016-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117326888&doi=10.1007%2fs00607-021-01016-7&partnerID=40&md5=ad4cf045204dac3a2701bf5e852b01be},
	abstract = {Chatbots are increasingly becoming important gateways to digital services and information—taken up within domains such as customer service, health, education, and work support. However, there is only limited knowledge concerning the impact of chatbots at the individual, group, and societal level. Furthermore, a number of challenges remain to be resolved before the potential of chatbots can be fully realized. In response, chatbots have emerged as a substantial research area in recent years. To help advance knowledge in this emerging research area, we propose a research agenda in the form of future directions and challenges to be addressed by chatbot research. This proposal consolidates years of discussions at the CONVERSATIONS workshop series on chatbot research. Following a deliberative research analysis process among the workshop participants, we explore future directions within six topics of interest: (a) users and implications, (b) user experience and design, (c) frameworks and platforms, (d) chatbots for collaboration, (e) democratizing chatbots, and (f) ethics and privacy. For each of these topics, we provide a brief overview of the state of the art, discuss key research challenges, and suggest promising directions for future research. The six topics are detailed with a 5-year perspective in mind and are to be considered items of an interdisciplinary research agenda produced collaboratively by avid researchers in the field. © 2021, The Author(s).},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 65}
}

@ARTICLE{Abdulquadri2021258,
	author = {Abdulquadri, Abdulazeez and Mogaji, Emmanuel and Kieu, Tai Anh and Nguyen, Nguyen Phong},
	title = {Digital transformation in financial services provision: a Nigerian perspective to the adoption of chatbot},
	year = {2021},
	journal = {Journal of Enterprising Communities},
	volume = {15},
	number = {2},
	pages = {258 – 281},
	doi = {10.1108/JEC-06-2020-0126},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101505207&doi=10.1108%2fJEC-06-2020-0126&partnerID=40&md5=016d3b8dd34e7ccbd5c814db173caf13},
	abstract = {Purpose: Recognising the high numbers of unbanked and financially excluded adults in Nigeria, this study aims to position chatbot as a digital transformation tool to radically change business model, improve customer experience and enhance financial inclusion in emerging markets. Design/methodology/approach: The Search-Access-Test (S-A-T) model was adopted to understand how Nigerian banks are adopting chatbots. Findings: A majority of Nigerian banks now have chatbots that enhance customer engagement and financial inclusion. WhatsApp was the most frequently used platform. Chatbots were often branded and presented with female gender identification. The chatbots were less responsive beyond their predefined path. While Nigeria is a multilingual country with English being the original language, none of the chatbots used any of the Nigerian’s local languages. Practical implications: Brands need to re-evaluate their chatbots with regard to responsiveness, predefined questions, verification and privacy. There are also possibilities of branding the chatbot and developing content creation strategies for proper engagement. Beyond English, the integration of African languages into chatbot is essential for digital transformation. Digital literacy and skills, particularly in the field of science, technology, engineering and mathematics, should be supported to equip future developers and create more jobs. Originality/value: While many theoretically based models for investigating the adoption of digital technologies have often placed focus on users’ ability to engage, this study takes an alternative perspective; by using the S-A-T model, it lays the responsibilities on the banks and chatbot developer to ensure that their chatbots are secure, responsive and able to meet the needs of the customers. © 2021, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 74}
}

@ARTICLE{Galatzer-Levy2023228,
	author = {Galatzer-Levy, Isaac R. and Aranovich, Gabriel J. and Insel, Thomas R.},
	title = {Can Mental Health Care Become More Human by Becoming More Digital?},
	year = {2023},
	journal = {Daedalus},
	volume = {152},
	number = {4},
	pages = {228 – 244},
	doi = {10.1162/daed_a_02040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177866983&doi=10.1162%2fdaed_a_02040&partnerID=40&md5=0419313127c1469c872c4f5ed531d115},
	abstract = {Over the past two decades, advances in digital technologies have begun to transform three aspects of mental health care. The use of sensors and artificial intelligence (AI) have provided new, objective measures of how we think, feel, and behave. The ease of connecting and communicating remotely has transformed the brick-and-mortar practice of mental health care into a telehealth service, increasing access and convenience for both patients and providers. And the advent of digital therapeutics, from virtual reality for treating phobias to conversational agents for delivering structured therapies, promises to alter how treatments will be delivered in the future. These digital transformations can help to solve many of the key challenges facing mental health care, including access, quality, and accountability. But digital technology introduces a new set of challenges around trust, privacy, and equity. Despite high levels of investment and promotion, there remain profound questions about efficacy and safety of digital mental health technologies. We share our experiences from the front lines creating digital innovations for mental health, with a focus on what a digital transformation of care could deliver for millions with a serious mental illness. © 2023 by Isaac R. Galatzer-Levy, Gabriel J. Aranovich & Thomas R. Insel.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Montagna2023205,
	author = {Montagna, Sara and Ferretti, Stefano and Klopfenstein, Lorenz Cuno and Florio, Antonio and Pengo, Martino Francesco},
	title = {Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management},
	year = {2023},
	journal = {ACM International Conference Proceeding Series},
	pages = {205 – 212},
	doi = {10.1145/3582515.3609536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174067299&doi=10.1145%2f3582515.3609536&partnerID=40&md5=ce9788fbce956ac870a9dd94a3e882b5},
	abstract = {Chronic patient self-management is crucial for maintaining physical and psychological health, reducing pressure on healthcare systems, and promoting patient empowerment. Digital technologies, particularly chatbots, have emerged as powerful tools for supporting patients in managing their chronic conditions. Large language models (LLMs), such as GPT-4, have shown potential in improving chatbot-based systems in healthcare. However, their adoption in clinical practice faces challenges, including reliability, the need for clinical trials, and privacy concerns. This paper proposes a general architecture for developing an LLM-based chatbot system that supports chronic patients while addressing privacy and security concerns. The architecture is designed to be independent of specific technologies and health conditions, focusing on data protection legislation compliance. A prototype of the system has been developed for hypertension management, demonstrating its potential for motivating patients to monitor their blood pressure and adhere to prescriptions.  © 2023 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Sun2023,
	author = {Sun, Zhuo and Zang, Guoquan and Wang, ZongShui and Zhao, Hong and Liu, Wei},
	title = {VCAs as partners or servants? The effects of information sensitivity and anthropomorphism roles on privacy concerns},
	year = {2023},
	journal = {Technological Forecasting and Social Change},
	volume = {192},
	doi = {10.1016/j.techfore.2023.122560},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152229604&doi=10.1016%2fj.techfore.2023.122560&partnerID=40&md5=c63fd49e6c829e7d3387a2d514634c80},
	abstract = {Advances in machine learning and natural language processing have driven the growing popularity of virtual conversational agents (VCAs). This anthropomorphic communication approach relies on user information sharing and real-time feedback from VCAs, and has raised privacy concerns while affecting various social interactions and relationships. Previous research on reducing user privacy concerns has mainly focused on user information mining, sensitive user information requests and privacy policies, while little is known about the anthropomorphic roles of partners and servants at the human-machine social hierarchy level. Therefore, this study, based on human-computer interaction (service) anthropomorphism at social level, develops a framework to investigate the impact of information sensitivity and VCAs' anthropomorphic roles, including partner and servant, on users' privacy concerns, as well as the mediating effects of competence- and integrity-based trust. The results show that when highly sensitive information is requested, user privacy concerns are greater for a partner VCA than a servant VCA, and vice-versa. Meanwhile, when a VCA requests highly sensitive information, integrity-based trust mediates the relationship between servant VCAs and privacy concerns, and when a VCA requests low-sensitivity information, competence-based trust mediates the same relationship. These insights provide actionable implications for managers. © 2023 Elsevier Inc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Toache202385,
	author = {Toache, Eugenio Arguelles and Rosales, Marcela Amaro},
	title = {ETHICAL CONCERNS IN THE USE OF ARTIFICIAL INTELLIGENCE, TRANSPARENCY, AND THE RIGHT OF ACCESS TO INFORMATION. THE CASE OF CHATBOTS IN THE MEXICAN GOVERNMENT, IN THE CONTEXT OF COVID-19; [PREOCUPACIONES ÉTICAS EN EL USO DE INTELIGENCIA ARTIFICIAL, TRANSPARENCIA Y DERECHO DE ACCESO A LA INFORMACIÓN. EL CASO DE LOS CHATBOTS EN EL GOBIERNO DE MÉXICO, EN EL CONTEXTO DE LA COVID-19]},
	year = {2023},
	journal = {Revista de Estudios en Derecho a la Informacion},
	volume = {2023},
	number = {15},
	pages = {85 – 111},
	doi = {10.22201/iij.25940082e.2023.15.17472},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183169288&doi=10.22201%2fiij.25940082e.2023.15.17472&partnerID=40&md5=36129b3ca94ec379f3a2c46392e56722},
	abstract = {The use of artificial intelligence in governments has increased in the last decade in several countries, acquiring greater importance in the context of the COVID-19 pandemic. Chatbots are one of the main tools that work based on artificial intelligence and have been used by governments to provide information and ser-vices to citizens, as well as for the follow-up, monitoring, and control of COVID-19. The use of these tools has generated ethical debates on the use of personal data, privacy, transparency, accountability, and the right of access to information. The objective of this work is to identify and analyze the main ethical concerns that emerge around the chatbots implemented in the mexican government in the context of COVID-19; the cases of Susana Distancia and Dr. Armando Vaccuno are analyzed. The methodology consists of an open survey of citizen perception. The results show that the main ethical concerns are transparency, accountability, and privacy, which have generated a lack of trust on the part of citizens towards chatbots, that have resulted in a low level of use. To remedy this, it is necessary to eliminate the regulatory gaps around transparency and data protection involved in these modern technologies. © 2023, Universidad Nacional Autonoma de Mexico. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Adetayo2023,
	author = {Adetayo, Adebowale Jeremy and Oyeniyi, Wosilat Omolara},
	title = {Revitalizing reference services and fostering information literacy: Google Bard’s dynamic role in contemporary libraries},
	year = {2023},
	journal = {Library Hi Tech News},
	doi = {10.1108/LHTN-08-2023-0137},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168317354&doi=10.1108%2fLHTN-08-2023-0137&partnerID=40&md5=849b1b0d0b73a5e15d121b5f8d8708e8},
	abstract = {Purpose: This paper aims to explore the transformative potential of Google Bard, an artificial intelligence (AI)-powered chatbot, in reshaping contemporary library reference services and advancing information literacy. Design/methodology/approach: In this perspective piece, a qualitative research approach is used to explore the capabilities of Google Bard within library contexts. Real-world case studies and insights are used to critically examine Bard’s evolving role as a virtual assistant, its impact on enhancing information literacy and the multifaceted challenges it introduces, including biases and privacy concerns. Findings: The research reveals that Google Bard, leveraging natural language processing and machine learning, engages users in dynamic conversational interactions. It provides contextually relevant responses and personalized guidance, leading to an enriched library experience. The symbiotic relationship between AI-driven technology and traditional librarian expertise is highlighted, contributing to interactive knowledge exploration and collaborative learning. Originality/value: This study contributes to the literature by exploring the multifaceted impact of Google Bard on library services and information literacy. It uncovers novel insights into the integration of AI-powered chatbots in traditional library settings. © 2023, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Fan2022967,
	author = {Fan, Hua and Han, Bing and Gao, Wei and Li, Wenqian},
	title = {How AI chatbots have reshaped the frontline interface in China: examining the role of sales–service ambidexterity and the personalization–privacy paradox},
	year = {2022},
	journal = {International Journal of Emerging Markets},
	volume = {17},
	number = {4},
	pages = {967 – 986},
	doi = {10.1108/IJOEM-04-2021-0532},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122748717&doi=10.1108%2fIJOEM-04-2021-0532&partnerID=40&md5=f85c4217a247f7d015bb796bd7009586},
	abstract = {Purpose: This study serves two purposes: (1) to evaluate the effects of organizational ambidexterity by examining how the balanced and the combined sales–service configurations of chatbots differ in their abilities to enhance customer experience and patronage and (2) to apply information boundary theory to assess the contingent role that chatbot sales–service ambidexterity can play in adapting to customers' personalization–privacy paradox. Design/methodology/approach: An online survey of artificial intelligence chatbots users was conducted, and a mixed-methods research design involving response surface analysis and polynomial regression was adopted to address the research aim. Findings: The results of polynomial regressions on survey data from 507 online customers indicated that as the benefits of personalization decreased and the risk to privacy increased, the inherently negative (positive) effects of imbalanced (combined) chatbots' sales–service ambidexterity had an increasing (decreasing) influence on customer experience. Furthermore, customer experience fully mediated the association of chatbots' sales–service ambidexterity with customer patronage. Originality/value: First, this study enriches the literature on frontline ambidexterity and extends it to the setting of human–machine interaction. Second, the study contributes to the literature on the personalization–privacy paradox by demonstrating the importance of frontline ambidexterity for adapting to customer concerns. Third, the study examines the conduit between artificial intelligence (AI) chatbots' ambidexterity and sales performance, thereby helping to reconcile the previously inconsistent evidence regarding this relationship. © 2022, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@ARTICLE{Kronemann20232,
	author = {Kronemann, Bianca and Kizgin, Hatice and Rana, Nripendra and K. Dwivedi, Yogesh},
	title = {How AI encourages consumers to share their secrets? The role of anthropomorphism, personalisation, and privacy concerns and avenues for future research},
	year = {2023},
	journal = {Spanish Journal of Marketing - ESIC},
	volume = {27},
	number = {1},
	pages = {2 – 19},
	doi = {10.1108/SJME-10-2022-0213},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146279636&doi=10.1108%2fSJME-10-2022-0213&partnerID=40&md5=87514f4c8e81c76e1b831a607d0ef16a},
	abstract = {Purpose: This paper aims to explore the overall research question “How can artificial intelligence (AI) influence consumer information disclosure?”. It considers how anthropomorphism of AI, personalisation and privacy concerns influence consumers’ attitudes and encourage disclosure of their private information. Design/methodology/approach: This research draws upon the personalisation-privacy paradox (PPP) and privacy calculus theory (PCT) to address the research question and examine how AI can influence consumer information disclosure. It is proposed that anthropomorphism of AI and personalisation positively influence consumer attitudes and intentions to disclose personal information to a digital assistant, while privacy concerns negatively affect attitude and information disclosure. Findings: This paper develops a conceptual model based on and presents seven research propositions (RPs) for future research. Originality/value: Building upon PPP and PCT, this paper presents a view on the benefits and drawbacks of AI from a consumer perspective. This paper contributes to literature by critically reflecting upon on the question how consumer information disclosure is influenced by AI. In addition, seven RPs and future research areas are outlined in relation to privacy and consumer information disclosure in relation to AI. © 2022, Bianca Kronemann, Hatice Kizgin, Nripendra Rana and Yogesh K. Dwivedi.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Wang2023339,
	author = {Wang, Xuequn and Lin, Xiaolin and Shao, Bin},
	title = {Artificial intelligence changes the way we work: A close look at innovating with chatbots},
	year = {2023},
	journal = {Journal of the Association for Information Science and Technology},
	volume = {74},
	number = {3},
	pages = {339 – 353},
	doi = {10.1002/asi.24621},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123950868&doi=10.1002%2fasi.24621&partnerID=40&md5=a8a26a3a9d8e2353efd50bdb7b8c3d0b},
	abstract = {An enhanced understanding of the innovative use of artificial intelligence (AI) is essential for organizations to improve work design and daily business operations. This study's purpose is to offer insights into how AI can transform organizations' work practices through diving deeply into its innovative use in the context of a primary AI tool, a chatbot, and examining the antecedents of innovative use by conceptualizing employee trust as a multidimensional construct and exploring employees' perceived benefits. In particular, we have conceptualized employee trust in chatbots as a second-order construct, including three first-order variables: trust in functionality, trust in reliability, and trust in data protection. We collected data from 202 employees. The results supported our conceptualization of trust in chatbots and showed that three dimensions of first-order trust beliefs have relatively the same level of importance. Further, both knowledge support and work–life balance enhance trust in chatbots, which in turn leads to innovative use of chatbots. Our study contributes to the existing literature by introducing the new conceptualization of trust in chatbots and examining its antecedents and outcomes. The results can provide important practical insights regarding how to support innovative use of chatbots as the new way we organize work. © 2022 Association for Information Science and Technology.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Langevin2021,
	author = {Langevin, Raina and Lordon, Ross and Avrahami, Thi},
	title = {Heuristic evaluation of conversational agents},
	year = {2021},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3411764.3445312},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106676286&doi=10.1145%2f3411764.3445312&partnerID=40&md5=97eb7045f0d827c64ee03c2d91ea7002},
	abstract = {Conversational interfaces have risen in popularity as businesses and users adopt a range of conversational agents, including chatbots and voice assistants. Although guidelines have been proposed, there is not yet an established set of usability heuristics to guide and evaluate conversational agent design. In this paper, we propose a set of heuristics for conversational agents adapted from Nielsen's heuristics and based on expert feedback. We then validate the heuristics through two rounds of evaluations conducted by participants on two conversational agents, one chatbot and one voice-based personal assistant. We fnd that, when using our heuristics to evaluate both interfaces, evaluators were able to identify more usability issues than when using Nielsen's heuristics. We propose that our heuristics successfully identify issues related to dialogue content, interaction design, help and guidance, human-like characteristics, and data privacy. © 2021 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 46}
}

@ARTICLE{Vervier2023213,
	author = {Vervier, Luisa and Brauner, Philipp and Ziefle, Martina},
	title = {Perception of Privacy and Willingness to Share Personal Data in the Smart Factory},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14045 LNCS},
	pages = {213 – 231},
	doi = {10.1007/978-3-031-35822-7_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171420795&doi=10.1007%2f978-3-031-35822-7_15&partnerID=40&md5=5461603947048cddf4ccdf4866f2b0f3},
	abstract = {By optimising data-driven processes and improving automation, the digital transformation in production aims to increase effectiveness, efficiency and improve the working conditions of employees. In such a networked working environment, the performance and actions of workers need to be captured in form of digital data. However, the collection of personal data is a sensitive issue. More research, not only from a techno-centric but also from a human-centric perspective, is needed. Using a multi-method approach, this study examines the motives, barriers and acceptance of technologies that use personal data in a production context. A qualitative pre-study (n= 7 ) identified motives (e.g. data offering personal benefit) and barriers (e.g. privacy concerns) of personal data disclosure. In the subsequent quantitative main study (n= 152 ), these key elements were operationalised in a scenario-based online survey, and two different working scenarios – cobot and chatbot – were additionally assessed using the Technology Acceptance Model (TAM and UTAUT2). The results show: The more fun it is to use and the higher the expected performance, the higher the acceptance of technology using personal data. Trust in automation followed by expected effort were important. Views on the disclosure of personal data and the expected benefit to the organisation varied widely. Out of seven categories, work-related and demographic data were considered to be disclosable, while five categories were considered important to the organisation. The article concludes with actionable recommendations on how the collection and use of personal data can be well aligned with stakeholder interests. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Limna202364,
	author = {Limna, Pongsakorn and Kraiwanit, Tanpat and Jangjarat, Kris and Klayklung, Prapasiri and Chocksathaporn, Piyawatjana},
	title = {The use of ChatGPT in the digital era: Perspectives on chatbot implementation},
	year = {2023},
	journal = {Journal of Applied Learning and Teaching},
	volume = {6},
	number = {1},
	pages = {64 – 74},
	doi = {10.37074/jalt.2023.6.1.32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162975175&doi=10.37074%2fjalt.2023.6.1.32&partnerID=40&md5=f66ca072652a80d33decf88547c88ffc},
	abstract = {The rapid advancement of technology has led to the integration of ChatGPT, an artificial intelligence (AI)-powered chatbot, in various sectors, including education. This research aims to explore the perceptions of educators and students on the use of ChatGPT in education during the digital era. This study adopted a qualitative research approach, using in-depth interviews to gather data. A purposive sampling technique was used to select ten educators and 15 students from different academic institutions in Krabi, Thailand. The data collected was analysed using content analysis and NVivo. The findings revealed that educators and students generally have a positive perception of using ChatGPT in education. The chatbot was perceived to be a helpful tool for providing immediate feedback, answering questions, and providing support to students. Educators noted that ChatGPT could reduce their workload by answering routine questions and enabling them to focus on higher-order tasks. However, the findings also showed some concerns regarding the use of ChatGPT in education. Participants were worried about the accuracy of information provided by the chatbot and the potential loss of personal interaction with teachers. The need for privacy and data security was also raised as a significant concern. The results of this study could help educators and policymakers make informed decisions about using ChatGPT in education. © 2023. Pongsakorn Limna, Tanpat Kraiwanit, Kris Jangjarat, Prapasiri Klayklung and Piyawatjana Chocksathaporn.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26}
}

@ARTICLE{Moore2022,
	author = {Moore, Nathan and Ahmadpour, Naseem and Brown, Martin and Poronnik, Philip and Davids, Jennifer},
	title = {Designing Virtual Reality-Based Conversational Agents to Train Clinicians in Verbal De-escalation Skills: Exploratory Usability Study},
	year = {2022},
	journal = {JMIR Serious Games},
	volume = {10},
	number = {3},
	doi = {10.2196/38669},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134403821&doi=10.2196%2f38669&partnerID=40&md5=966aaef5bd2482e1ee9c39e11e1f1197},
	abstract = {Background: Violence and aggression are significant workplace challenges faced by clinicians worldwide. Traditional methods of training consist of “on-the-job learning” and role-play simulations. Although both approaches can result in improved skill levels, they are not without limitation. Interactive simulations using virtual reality (VR) can complement traditional training processes as a cost-effective, engaging, easily accessible, and flexible training tool. Objective: In this exploratory study, we aimed to determine the feasibility of and barriers to verbal engagement with a virtual agent in the context of the Code Black VR application. Code Black VR is a new interactive VR-based verbal de-escalation trainer that we developed based on the Clinical Training Through VR Design Framework. Methods: In total, 28 participants with varying clinical expertise from 4 local hospitals enrolled in the Western Sydney Local Health District Clinical Initiative Nurse program and Transition to Emergency Nursing Programs and participated in 1 of 5 workshops. They completed multiple playthroughs of the Code Black VR verbal de-escalation trainer application and verbally interacted with a virtual agent. We documented observations and poststudy reflection notes. After the playthroughs, the users completed the System Usability Scale and provided written comments on their experience. A thematic analysis was conducted on the results. Data were also obtained through the application itself, which also recorded the total interactions and successfully completed interactions. Results: The Code Black VR verbal de-escalation training application was well received. The findings reinforced the factors in the existing design framework and identified 3 new factors-motion sickness, perceived value, and privacy-to be considered for future application development. Conclusions: Verbal interaction with a virtual agent is feasible for training staff in verbal de-escalation skills. It is an effective medium to supplement clinician training in verbal de-escalation skills. We provide broader design considerations to guide further developments in this area. © Nathan Moore, Naseem Ahmadpour, Martin Brown, Philip Poronnik, Jennifer Davids. Originally published in JMIR Serious Games (https://games.jmir.org), 06.07.2022.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Yorita2021,
	author = {Yorita, Akihiro and Egerton, Simon and Chan, Carina and Kubota, Naoyuki},
	title = {Development of Esteem Support based on Psychodrama and Design Thinking approach},
	year = {2021},
	journal = {2021 IEEE Symposium Series on Computational Intelligence, SSCI 2021 - Proceedings},
	doi = {10.1109/SSCI50451.2021.9660118},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125803809&doi=10.1109%2fSSCI50451.2021.9660118&partnerID=40&md5=aed3cb6b616405ffc4df91c741236c78},
	abstract = {So far, we have proposed the Stress management framework and developed methods for stress measurement and coping. There are three types of coping in this framework: information support, emotional support, and esteem support. This time, we developed esteem support. In order to increase self-esteem, we focused on psychodrama techniques and developed a system to realize psychodrama using robots. The advantages of using robots include the ability to carry out a psychodrama alone and to provide privacy-friendly support. In the experiment, a scenario was created for medical staff and a robot psychodrama was performed. As a result, we showed that robots can be used for psychodrama and esteem support can be carried out by combination of robots and the chatbot. © 2021 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Siddiqui2023,
	author = {Siddiqui, Taha and Sharma, Arun and Arora, Mamta},
	title = {Adaptive Mental Health And Mood Uplifting Chatbot},
	year = {2023},
	journal = {2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023},
	doi = {10.1109/ICCCNT56998.2023.10308236},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179855215&doi=10.1109%2fICCCNT56998.2023.10308236&partnerID=40&md5=19c277a3f8f0388190da3a5db9572f52},
	abstract = {Mental health concerns are on the rise worldwide, and the stigma and lack of confidentiality surrounding therapy make it difficult for many people to seek the help they need. In response to this problem, my research proposes the development of an AI chatbot that acts as a virtual psychologist, providing accessible and confidential mental health support. Using NLP technology, the chatbot will be able to understand and respond to user queries, offering personalized therapy sessions based on individual input. My paper explores various NLP models and techniques that can be used for the chatbot's development, as well as ethical and privacy considerations. We are truly passionate about the potential of this chatbot to revolutionize the mental health industry, providing affordable and accessible therapy while breaking down the stigma associated with seeking help. While there is still much work to be done to ensure the chatbot's reliability and effectiveness, We are excited to be at the forefront of this innovative new field, working towards a brighter future for mental health support. © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2023,
	author = {Liu, Yu-li and Hu, Bo and Yan, Wenjia and Lin, Zhi},
	title = {Can chatbots satisfy me? A mixed-method comparative study of satisfaction with task-oriented chatbots in mainland China and Hong Kong},
	year = {2023},
	journal = {Computers in Human Behavior},
	volume = {143},
	doi = {10.1016/j.chb.2023.107716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150767976&doi=10.1016%2fj.chb.2023.107716&partnerID=40&md5=c9cf512ad105a11edcd8d2426371daf6},
	abstract = {Task-oriented chatbots are gradually being used across the globe. Most notably, while chatbots have for a long time penetrated users’ daily lives in mainland China, Hong Kong is still struggling to improve and promote its chatbot services. To determine whether antecedents of satisfaction and usage intention differ based on different stages of chatbot adoption and development, we conduct a comparative study based on a research model that integrates the Delone and McLean Information System success model and privacy concerns. The model is developed and examined using a mixed-method approach. After conducting focus group interviews (N = 15) in both regions, online surveys were conducted in mainland China (N = 637) and Hong Kong (N = 647), respectively. Based on qualitative exploration, we identified critical factors of perceived quality and privacy concerns. The quantitative findings further illuminate the different roles of the antecedents in the two regions. The results show that usage intention can be positively influenced by satisfaction, and satisfaction can be increased by relevance, completeness, pleasure and assurance in both regions. However, response time and empathy are factors influencing satisfaction only in mainland China. Privacy concerns cannot influence satisfaction in both regions. © 2023 Elsevier Ltd},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@CONFERENCE{D'Urso2023210,
	author = {D'Urso, Stefano and Sciarrone, Filippo and Temperini, Marco},
	title = {Boulez: A Chatbot-Based Federated Learning System for Distance Learning},
	year = {2023},
	journal = {Proceedings of the International Conference on Information Visualisation},
	pages = {210 – 215},
	doi = {10.1109/IV60283.2023.00045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178509292&doi=10.1109%2fIV60283.2023.00045&partnerID=40&md5=f3afb46a66790a78678851232aa96857},
	abstract = {In recent years, also due to the covid-19 pandemic, the possibilities for distance learning have increased considerably, through web-based learning platforms, available on the Internet without space and time limits. As a result, the offer of courses and the number of enrolled students has grown exponentially. In order to be able to guarantee students a better learning support service, one of the proposals regards the intelligent Chatbots. These well known interactive applications are based mainly on machine or deep learning and in this paper we present Boulez, a system allowing the orchestration of a community of individual chatbots, each one with its algorithm and its private training dataset. We apply a technique called Federated Learning, where several individual chatbots, collaborate. In particular, here the approach is 'centralized', meaning that a main system orchestrates the collaboration of the federated systems. By addressing the communication inefficiencies and privacy issues of conventional federated learning, Boulez offers a more efficient and effective approach to chatbot interaction, ultimately leading to improved user experience. The paper presents the Boulez system, its operation principle, methods used, and potential benefits, along with a use case of its application. © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Farah20221634,
	author = {Farah, Juan Carlos and Spaenlehauer, Basile and Sharma, Vandit and Rodriguez-Triana, Maria Jesus and Ingram, Sandy and Gillet, Denis},
	title = {Impersonating Chatbots in a Code Review Exercise to Teach Software Engineering Best Practices},
	year = {2022},
	journal = {IEEE Global Engineering Education Conference, EDUCON},
	volume = {2022-March},
	pages = {1634 – 1642},
	doi = {10.1109/EDUCON52537.2022.9766793},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130493554&doi=10.1109%2fEDUCON52537.2022.9766793&partnerID=40&md5=d4fa40bf638c4210a392bbdbce4a96d1},
	abstract = {Over the past decade, the use of chatbots for educational purposes has gained considerable traction. A similar trend has been observed in social coding platforms, where automated agents support software developers with tasks such as performing code reviews. While incorporating code reviews and social coding platforms into software engineering education has been found to be beneficial, challenges such as steep learning curves and privacy considerations are barriers to their adoption. Furthermore, no study has addressed the role chatbots play in supporting code reviews as a pedagogical tool. To help address this gap, we developed an online learning application that simulates the code review features available on social coding platforms and allows instructors to interact with students using chatbot identities. We then embedded this application within a lesson on software engineering best practices and conducted a controlled in-class experiment. This experiment examined the effect that explaining content via chatbot identities had on three aspects: (i) students' perceived usability of the lesson, (ii) their engagement with the code review process, and (iii) their learning gains. While our findings show that it is feasible to simulate the code review process within an online learning platform and achieve good usability, our quantitative analysis did not yield significant differences across treatment conditions for any of the aspects considered. Nevertheless, our qualitative results suggest that students expect explicit feedback when performing this type of exercise and could thus benefit from automated replies provided by an interactive chatbot. We propose to build on our current findings to further explore this line of research in future work. © 2022 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Miao2023,
	author = {Miao, Hongyu and Li, Chengdong and Wang, Jing},
	title = {A Future of Smarter Digital Health Empowered by Generative Pretrained Transformer},
	year = {2023},
	journal = {Journal of Medical Internet Research},
	volume = {25},
	number = {1},
	doi = {10.2196/49963},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172204717&doi=10.2196%2f49963&partnerID=40&md5=ed0550ce64050ecdd722ef7cd0c4d01e},
	abstract = {Generative pretrained transformer (GPT) tools have been thriving, as ignited by the remarkable success of OpenAI’s recent chatbot product. GPT technology offers countless opportunities to significantly improve or renovate current health care research and practice paradigms, especially digital health interventions and digital health–enabled clinical care, and a future of smarter digital health can thus be expected. In particular, GPT technology can be incorporated through various digital health platforms in homes and hospitals embedded with numerous sensors, wearables, and remote monitoring devices. In this viewpoint paper, we highlight recent research progress that depicts the future picture of a smarter digital health ecosystem through GPT-facilitated centralized communications, automated analytics, personalized health care, and instant decision-making. © 2023 Journal of Medical Internet Research. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{De Arizón20232314,
	author = {De Arizón, Leonor Fayos and Viera, Elizabeth R. and Pilco, Melissa and Perera, Alexandre and De Maeztu, Gabriel and Nicolau, Anna and Furlano, Monica and Torra, Roser},
	title = {Artificial intelligence: a new field of knowledge for nephrologists},
	year = {2023},
	journal = {Clinical Kidney Journal},
	volume = {16},
	number = {12},
	pages = {2314 – 2326},
	doi = {10.1093/ckj/sfad182},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179321121&doi=10.1093%2fckj%2fsfad182&partnerID=40&md5=70e1871cce641b2cbf190c119b4a58a9},
	abstract = {Artificial intelligence (AI) is a science that involves creating machines that can imitate human intelligence and learn. AI is ubiquitous in our daily lives, from search engines like Google to home assistants like Alexa and, more recently, OpenAI with its chatbot. AI can improve clinical care and research, but its use requires a solid understanding of its fundamentals, the promises and perils of algorithmic fairness, the barriers and solutions to its clinical implementation, and the pathways to developing an AI-competent workforce. The potential of AI in the field of nephrology is vast, particularly in the areas of diagnosis, treatment and prediction. One of the most significant advantages of AI is the ability to improve diagnostic accuracy. Machine learning algorithms can be trained to recognize patterns in patient data, including lab results, imaging and medical history, in order to identify early signs of kidney disease and thereby allow timely diagnoses and prompt initiation of treatment plans that can improve outcomes for patients. In short, AI holds the promise of advancing personalized medicine to new levels. While AI has tremendous potential, there are also significant challenges to its implementation, including data access and quality, data privacy and security, bias, trustworthiness, computing power, AI integration and legal issues. The European Commission’s proposed regulatory framework for AI technology will play a significant role in ensuring the safe and ethical implementation of these technologies in the healthcare industry. Training nephrologists in the fundamentals of AI is imperative because traditionally, decision-making pertaining to the diagnosis, prognosis and treatment of renal patients has relied on ingrained practices, whereas AI serves as a powerful tool for swiftly and confidently synthesizing this information. plans, leading to better patient outcomes. However, the implementation of AI in healthcare faces several challenges. The European Commission’s proposed regulatory framework aims to promote the safe and ethical use of AI in healthcare. To fully leverage the benefits of AI, nephrologists and other healthcare professionals need to be educated about its fundamentals and its potential applications in routine patient care. This will enable them to effectively utilize AI technologies and provide better care for kidney patients. © The Author(s) 2023. Published by Oxford University Press on behalf of the ERA.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Leschanowsky2023296,
	author = {Leschanowsky, Anna and Popp, Birgit and Peters, Nils},
	title = {Privacy Strategies for Conversational AI and their Influence on Users' Perceptions and Decision-Making},
	year = {2023},
	journal = {ACM International Conference Proceeding Series},
	pages = {296 – 311},
	doi = {10.1145/3617072.3617106},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175400905&doi=10.1145%2f3617072.3617106&partnerID=40&md5=4de3c303d16c8fada2fa816b4931b0ea},
	abstract = {Conversational AI (CAI) systems are on the rise and have been widely adopted in homes, cars and public spaces. Yet, people report privacy concerns and mistrust in these systems. Current data protection regulations ask providers to communicate data practices transparently and provide users with options to control their data. However, even if users are given control, their decisions can be subject to heuristics and biases leaving people frustrated and regretful. Based on the idea of conversational privacy and debiasing, we design three privacy strategies for CAI that allow people to have their data deleted while at the same time promoting rational decision-making. We conduct a user study to test our strategies in two widespread scenarios using a text-based CAI system and evaluate their impact on peoples' privacy perception, usability and attitude-behaviour alignment. We find that our strategies can significantly change people's behaviour, but do not influence peoples' privacy perception. Finally, we discuss evaluation metrics and future research directions to investigate privacy controls in Conversational AI systems. © 2023 Owner/Author.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2023,
	author = {Li, Qingchuan and Luximon, Yan and Zhang, Jiaxin},
	title = {The Influence of Anthropomorphic Cues on Patients’ Perceived Anthropomorphism, Social Presence, Trust Building, and Acceptance of Health Care Conversational Agents: Within-Subject Web-Based Experiment},
	year = {2023},
	journal = {Journal of Medical Internet Research},
	volume = {25},
	doi = {10.2196/44479},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167532440&doi=10.2196%2f44479&partnerID=40&md5=4a322c9dafd65a54dee3814ad16b43d8},
	abstract = {Background: The last decade has witnessed the rapid development of health care conversational agents (CAs); however, there are still great challenges in making health care CAs trustworthy and acceptable to patients. Objective: Focusing on intelligent guidance CAs, a type of health care CA for web-based patient triage, this study aims to investigate how anthropomorphic cues influence patients’ perceived anthropomorphism and social presence of such CAs and evaluate how these perceptions facilitate their trust-building process and acceptance behavior. Methods: To test the research hypotheses, the video vignette methodology was used to evaluate patients’ perceptions and acceptance of various intelligent guidance CAs. The anthropomorphic cues of CAs were manipulated in a 3×2 within-subject factorial experiment with 103 participants, with the factors of agent appearance (high, medium, and low anthropomorphic levels) and verbal cues (humanlike and machine-like verbal cues) as the within-subject variables. Results: The 2-way repeated measures ANOVA analysis indicated that the higher anthropomorphic level of agent appearance significantly increased mindful anthropomorphism (high level>medium level: 4.57 vs 4.27; P=.01; high level>low level: 4.57 vs 4.04; P<.001; medium level>low level: 4.27 vs 4.04; P=.04), mindless anthropomorphism (high level>medium level: 5.39 vs 5.01; P<.001; high level>low level: 5.39 vs 4.85; P<.001), and social presence (high level>medium level: 5.19 vs 4.83; P<.001; high level>low level: 5.19 vs 4.72; P<.001), and the higher anthropomorphic level of verbal cues significantly increased mindful anthropomorphism (4.83 vs 3.76; P<.001), mindless anthropomorphism (5.60 vs 4.57; P<.001), and social presence (5.41 vs 4.41; P<.001). Meanwhile, a significant interaction between agent appearance and verbal cues (.004) was revealed. Second, the partial least squares results indicated that privacy concerns were negatively influenced by social presence (β=−.375; t312=4.494) and mindful anthropomorphism (β=−.112; t312=1.970). Privacy concerns (β=−.273; t312=9.558), social presence (β=.265; t312=4.314), and mindless anthropomorphism (β=.405; t312=7.145) predicted the trust in CAs, which further promoted the intention to disclose information (β=.675; t312=21.163), the intention to continuously use CAs (β=.190; t312=4.874), and satisfaction (β=.818; t312=46.783). Conclusions: The findings show that a high anthropomorphic level of agent appearance and verbal cues could improve the perceptions of mindful anthropomorphism and mindless anthropomorphism as well as social presence. Furthermore, mindless anthropomorphism and social presence significantly promoted patients’ trust in CAs, and mindful anthropomorphism and social presence decreased privacy concerns. It is also worth noting that trust was an important antecedent and determinant of patients’ acceptance of CAs, including their satisfaction, intention to disclose information, and intention to continuously use CAs. © 2023 Journal of Medical Internet Research. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lian202398,
	author = {Lian, Andrew T. and Costilla Reyes, Alfredo and Hu, Xia},
	title = {CAPTAIN: An AI-Based Chatbot for Cyberbullying Prevention and Intervention},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14051 LNAI},
	pages = {98 – 107},
	doi = {10.1007/978-3-031-35894-4_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173015942&doi=10.1007%2f978-3-031-35894-4_7&partnerID=40&md5=e9ea6590c9ed57e59f5471b6cd770c0e},
	abstract = {Cyberbullying is a widespread and growing problem that can cause various psychological and health well-being outcomes in youth and is considered a serious public health threat. Cutting-edge informatics technology would enable us to identify and stop cyberbullying to prevent harm, death, and privacy violations. However, current cyberbullying prevention approaches offer limited interactions, individualized education, and in-time intervention. With the current emerging technologies in Artificial Intelligence (AI), the use of chatbots have become increasingly popular in health promotion. However, there are current technological challenges that need to be addressed, such as detecting and preventing cyberbullying in real-time, providing personalized responses and intervention, as well as developing the chatbot with a user-friendly interface. This paper introduces CAPTAIN (Cyberbullying Awareness and Prevention Through Artificial INtelligence), an AI-based chatbot for cyberbullying prevention that can provide anytime interaction for personalized intervention. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Aslam20233781,
	author = {Aslam, Wajeeha and Ahmed Siddiqui, Danish and Arif, Imtiaz and Farhat, Kashif},
	title = {Chatbots in the frontline: drivers of acceptance},
	year = {2023},
	journal = {Kybernetes},
	volume = {52},
	number = {9},
	pages = {3781 – 3810},
	doi = {10.1108/K-11-2021-1119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132678993&doi=10.1108%2fK-11-2021-1119&partnerID=40&md5=1bfa8169e67e5bcd4d8732f5bb314dd0},
	abstract = {Purpose: By extending the service robot acceptance model (sRAM), this study aims to explore and enhance the acceptance of chatbots. The study considered functional, relational, social, user and gratification elements in determining the acceptance of chatbots. Design/methodology/approach: By using the purposive sampling technique, data of 321 service customers, gathered from millennials through a questionnaire and subsequent PLS-SEM modeling, was applied for hypotheses testing. Findings: Findings revealed that the functional elements, perceived usefulness and perceived ease of use affect acceptance of chatbots. However, in social elements, only perceived social interactivity affects the acceptance of chatbots. Moreover, both user and gratification elements (hedonic motivation and symbolic motivation) significantly influence the acceptance of chatbots. Lastly, trust is the only contributing factor for the acceptance of chatbots in the relational elements. Practical implications: The study extends the literature related to chatbots and offers several guidelines to the service industry to effectively employ chatbots. Originality/value: This is one of the first studies that used newly developed sRAM in determining chatbot acceptance. Moreover, the study extended the sRAM by adding user and gratification elements and privacy concerns as originally sRAM model was limited to functional, relational and social elements. © 2022, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@ARTICLE{Levantino2023,
	author = {Levantino, Francesco Paolo},
	title = {Generative and AI-powered oracles: “What will they say about you?”},
	year = {2023},
	journal = {Computer Law and Security Review},
	volume = {51},
	doi = {10.1016/j.clsr.2023.105898},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174621599&doi=10.1016%2fj.clsr.2023.105898&partnerID=40&md5=92c747b525c659a28b8c3e5c94bb8308},
	abstract = {In less than one year from its launch, the chatbot ChatGPT has captured widespread public attention, thanks to its ease of use and remarkable performance. However, part of this interest is due to its involvement in some data protection and data security issues. In the context of the ongoing debate surrounding similar technologies, such as generative AI, this contribution will first introduce the "ChatGPT phenomenon" (Sections 1 and 2). Then, it will analyse the various positions taken by some key stakeholders on the issues of above and the regulation of the design and use of such technologies, examining these perspectives through the lenses of “Digital Constitutionalism”. Particularly, this paper will emphasise the role that civil society can play in such dynamics (Section 3). Subsequently, it will further promote an active and forward-looking approach in addressing the looming threats these and other AI-based technologies could pose to fundamental rights and society as a whole (Section 4). As we already approach the next AI era without even noticing, the question worth asking ourselves is: “What will generative and AI-powered oracles reveal about us?” (Section 5). © 2023 Elsevier Ltd},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Guleria20231292,
	author = {Guleria, Ankita and Krishan, Kewal and Sharma, Vishal and Kanchan, Tanuj},
	title = {ChatGPT: ethical concerns and challenges in academics and research},
	year = {2023},
	journal = {Journal of Infection in Developing Countries},
	volume = {17},
	number = {9},
	pages = {1292 – 1299},
	doi = {10.3855/jidc.18738},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174233915&doi=10.3855%2fjidc.18738&partnerID=40&md5=0f14c5ffe000c5e2808759c9ccaa9173},
	abstract = {Introduction: The emergence of artificial intelligence (AI) has presented several opportunities to ease human work. AI applications are available for almost every domain of life. A new technology, Chat Generative Pre-Trained Transformer (ChatGPT), was introduced by OpenAI in November 2022, and has become a topic of discussion across the world. ChatGPT-3 has brought many opportunities, as well as ethical and privacy considerations. ChatGPT is a large language model (LLM) which has been trained on the events that happened until 2021. The use of AI and its assisted technologies in scientific writing is against research and publication ethics. Therefore, policies and guidelines need to be developed over the use of such tools in scientific writing. The main objective of the present study was to highlight the use of AI and AI assisted technologies such as the ChatGPT and other chatbots in the scientific writing and in the research domain resulting in bias, spread of inaccurate information and plagiarism. Methodology: Experiments were designed to test the accuracy of ChatGPT when used in research and academic writing. Results: The information provided by ChatGPT was inaccurate and may have far-reaching implications in the field of medical science and engineering. Critical thinking should be encouraged among researchers to raise awareness about the associated privacy and ethical risks. Conclusions: Regulations for ethical and privacy concerns related to the use of ChatGPT in academics and research need to be developed. Copyright © 2023 Guleria et al.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mirbabaie2021365,
	author = {Mirbabaie, Milad and Stieglitz, Stefan and Frick, Nicholas R. J.},
	title = {Hybrid intelligence in hospitals: towards a research agenda for collaboration},
	year = {2021},
	journal = {Electronic Markets},
	volume = {31},
	number = {2},
	pages = {365 – 387},
	doi = {10.1007/s12525-021-00457-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100249142&doi=10.1007%2fs12525-021-00457-4&partnerID=40&md5=54413627acf4e6f1ba641f6fea97b0e9},
	abstract = {Successful collaboration between clinicians is particularly relevant regarding the quality of care process. In this context, the utilization of hybrid intelligence, such as conversational agents (CAs), is a reasonable approach for the coordination of diverse tasks. While there is a great deal of literature involving collaboration, little effort has been made to integrate previous findings and evaluate research when applying CAs in hospitals. By conducting an extended and systematic literature review and semi-structured expert interviews, we identified four major challenges and derived propositions where in-depth research is needed: 1) audience and interdependency; 2) connectivity and embodiment; 3) trust and transparency; and 4) security, privacy, and ethics. The results are helpful for researchers as we discuss directions for future research on CAs for collaboration in a hospital setting enhancing team performance. Practitioners will be able to understand which difficulties must be considered before the actual application of CAs. © 2021, The Author(s).},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@CONFERENCE{Bang2021,
	author = {Bang, Junseong and Kim, Sineae and Nam, Jang Won and Yang, Dong-Geun},
	title = {Ethical Chatbot Design for Reducing Negative Effects of Biased Data and Unethical Conversations},
	year = {2021},
	journal = {2021 International Conference on Platform Technology and Service, PlatCon 2021 - Proceedings},
	doi = {10.1109/PlatCon53246.2021.9680760},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126254756&doi=10.1109%2fPlatCon53246.2021.9680760&partnerID=40&md5=c1adfcc34ee611e791a3514721a1fafe},
	abstract = {AI technology is being introduced into various public and private service domains, transforming existing computing systems or creating new ones. While AI technologies can provide benefits to humans and society, the unexpected consequences (e.g., malfunctions) of AI systems can cause social losses. For this reason, research on ethical design for the development of AI-based systems is becoming important. In this paper, from existing studies on AI ethics, general guidelines such as transparency, explainability, predictability, accountability, fairness, privacy, and control for the ethical design of AI systems are reviewed. And, based on the ethical design guidelines, we discuss ethical design to reduce the negative effects of biased data and unethical dialogues in AI-based conversational chatbots.  © 2021 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Fan2021,
	author = {Fan, Xiangmin and Chao, Daren and Zhang, Zhan and Wang, Dakuo and Li, Xiaohua and Tian, Feng},
	title = {Utilization of self-diagnosis health chatbots in real-world settings: Case study},
	year = {2021},
	journal = {Journal of Medical Internet Research},
	volume = {23},
	number = {1},
	doi = {10.2196/19928},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099267127&doi=10.2196%2f19928&partnerID=40&md5=099f5ac81a2627bfa35f9f2f449a14e0},
	abstract = {Background: Artificial intelligence (AI)-driven chatbots are increasingly being used in health care, but most chatbots are designed for a specific population and evaluated in controlled settings. There is little research documenting how health consumers (eg, patients and caregivers) use chatbots for self-diagnosis purposes in real-world scenarios. Objective: The aim of this research was to understand how health chatbots are used in a real-world context, what issues and barriers exist in their usage, and how the user experience of this novel technology can be improved. Methods: We employed a data-driven approach to analyze the system log of a widely deployed self-diagnosis chatbot in China. Our data set consisted of 47,684 consultation sessions initiated by 16,519 users over 6 months. The log data included a variety of information, including users' nonidentifiable demographic information, consultation details, diagnostic reports, and user feedback. We conducted both statistical analysis and content analysis on this heterogeneous data set. Results: The chatbot users spanned all age groups, including middle-aged and older adults. Users consulted the chatbot on a wide range of medical conditions, including those that often entail considerable privacy and social stigma issues. Furthermore, we distilled 2 prominent issues in the use of the chatbot: (1) a considerable number of users dropped out in the middle of their consultation sessions, and (2) some users pretended to have health concerns and used the chatbot for nontherapeutic purposes. Finally, we identified a set of user concerns regarding the use of the chatbot, including insufficient actionable information and perceived inaccurate diagnostic suggestions. Conclusions: Although health chatbots are considered to be convenient tools for enhancing patient-centered care, there are issues and barriers impeding the optimal use of this novel technology. Designers and developers should employ user-centered approaches to address the issues and user concerns to achieve the best uptake and utilization. We conclude the paper by discussing several design implications, including making the chatbots more informative, easy-to-use, and trustworthy, as well as improving the onboarding experience to enhance user engagement. © Xiangmin Fan, Daren Chao, Zhan Zhang, Dakuo Wang, Xiaohua Li, Feng Tian.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 46}
}

@ARTICLE{Mazzola202262,
	author = {Mazzola, Luca and Waldis, Andreas and Shankar, Atreya and Argyris, Diamantis and Denzler, Alexander and Van Roey, Michiel},
	title = {Privacy and Customer’s Education: NLP for Information Resources Suggestions and Expert Finder Systems},
	year = {2022},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {13333 LNCS},
	pages = {62 – 77},
	doi = {10.1007/978-3-031-05563-8_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133244714&doi=10.1007%2f978-3-031-05563-8_5&partnerID=40&md5=46ce4454eea16e04683c2ffffa641ed1},
	abstract = {Privacy is one of the key issues for citizen’s everyday online activities, with the United Nations defining it as “a human right in the digital age”. Despite the introduction of data privacy regulations almost everywhere around the globe, the biggest barrier to effectiveness is the customer’s capacity to map the privacy statement received with the regulation in force and understand their terms. This study advocates the creation of a convenient and cost-efficient question-answering service for answering customers’ queries on data privacy. It proposes a dual step approach, allowing consumers to ask support to a conversational agent boosted by a smart knowledge base, attempting to answer the question using the most appropriate legal document. Being the self-help approach insufficient, our system enacts a second step suggesting a ranked list of legal experts for focused advice. To achieve our objective, we need large enough and specialised dataset and we plan to apply state-of-the-art Natural Language Processing (NLP) techniques in the field of open domain question answering. This paper describes the initial steps and some early results we achieved in this direction and the next steps we propose to develop a one-stop solution for consumers privacy needs. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Hendrickx2021,
	author = {Hendrickx, Iris and Waterschoot, Jelte Van and Khan, Arif and Bosch, Louis Ten and Cucchiarini, Catia and Strik, Helmer},
	title = {Take Back Control: User Privacy and Transparency Concerns in Personalized Conversational Agents},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2903},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109210222&partnerID=40&md5=4d68a3e4e6c59a8bce9b0decdc10c184},
	abstract = {We reflect on user privacy concerns, transparency and informed consent for long-term interactions with personalized conversational agents. We argue that the common practice of asking users to sign an informed consent form is insufficient to accommodate the privacy concerns of the user. We propose that long-term engaging personalized conversational agents must include an explicit mechanism in their conversations to allow users to have control over their personal information and to have transparency, i.e. about what is stored and who is allowed to view the stored personal information. c 2020  Copyright for this paper by its authors.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Mitsuno2022,
	author = {Mitsuno, Seiya and Yoshikawa, Yuichiro and Ban, Midori and Ishiguro, Hiroshi},
	title = {Evaluation of a Daily Interactive Chatbot That Exchanges Information about Others through Long-Term Use in a Group of Friends Investigating Dialogue Experience and Privacy Concern},
	year = {2022},
	journal = {Transactions of the Japanese Society for Artificial Intelligence},
	volume = {37},
	number = {3},
	doi = {10.1527/tjsai.37-3_IDS-I},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132854367&doi=10.1527%2ftjsai.37-3_IDS-I&partnerID=40&md5=729355c35849bc1ad568bc0b63a8bb2a},
	abstract = {The goal of this study is to realize a non-task-oriented dialogue agent that is accepted by people in the long term. One approach is using a dialogue strategy in which an agent shares information about other users who are not participating in the current dialogue. This study aims to develop a chatbot that is capable of sharing information about others and to examine its usefulness as well as its problems such as privacy concerns using a long-term empirical experiment in a real-world environment. The result of a 14-day experiment with 120 participants suggested that the usefulness of this dialogue strategy lies in its ability to maintain users’ motivation to interact with the agent and prevent them from having the impression that the agent is mechanical. However, irrespective of the presence of this dialogue strategy, it was suggested that the users were concerned about their privacy to the agent that collected their information on a daily basis. Based on these results, we discussed the relationship between the interestingness of the shared information and the users’ privacy concerns. © 2022, Japanese Society for Artificial Intelligence. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Peng2022,
	author = {Peng, Mary L. and Wickersham, Jeffrey A. and Altice, Frederick L. and Shrestha, Roman and Azwa, Iskandar and Zhou, Xin and Halim, Mohd Akbar Ab and Ikhtiaruddin, Wan Mohd and Tee, Vincent and Kamarulzaman, Adeeba and Ni, Zhao},
	title = {Formative Evaluation of the Acceptance of HIV Prevention Artificial Intelligence Chatbots by Men Who Have Sex with Men in Malaysia: Focus Group Study},
	year = {2022},
	journal = {JMIR Formative Research},
	volume = {6},
	number = {10},
	doi = {10.2196/42055},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140263659&doi=10.2196%2f42055&partnerID=40&md5=f295f7cec4fbca5a3d082042f254a7ac},
	abstract = {Background: Mobile technologies are being increasingly developed to support the practice of medicine, nursing, and public health, including HIV testing and prevention. Chatbots using artificial intelligence (AI) are novel mobile health strategies that can promote HIV testing and prevention among men who have sex with men (MSM) in Malaysia, a hard-to-reach population at elevated risk of HIV, yet little is known about the features that are important to this key population. Objective: The aim of this study was to identify the barriers to and facilitators of Malaysian MSM’s acceptance of an AI chatbot designed to assist in HIV testing and prevention in relation to its perceived benefits, limitations, and preferred features among potential users. Methods: We conducted 5 structured web-based focus group interviews with 31 MSM in Malaysia between July 2021 and September 2021. The interviews were first recorded, transcribed, coded, and thematically analyzed using NVivo (version 9; QSR International). Subsequently, the unified theory of acceptance and use of technology was used to guide data analysis to map emerging themes related to the barriers to and facilitators of chatbot acceptance onto its 4 domains: performance expectancy, effort expectancy, facilitating conditions, and social influence. Results: Multiple barriers and facilitators influencing MSM’s acceptance of an AI chatbot were identified for each domain. Performance expectancy (ie, the perceived usefulness of the AI chatbot) was influenced by MSM’s concerns about the AI chatbot’s ability to deliver accurate information, its effectiveness in information dissemination and problem-solving, and its ability to provide emotional support and raise health awareness. Convenience, cost, and technical errors influenced the AI chatbot’s effort expectancy (ie, the perceived ease of use). Efficient linkage to health care professionals and HIV self-testing was reported as a facilitating condition of MSM’s receptiveness to using an AI chatbot to access HIV testing. Participants stated that social influence (ie, sociopolitical climate) factors influencing the acceptance of mobile technology that addressed HIV in Malaysia included privacy concerns, pervasive stigma against homosexuality, and the criminalization of same-sex sexual behaviors. Key design strategies that could enhance MSM’s acceptance of an HIV prevention AI chatbot included an anonymous user setting; embedding the chatbot in MSM-friendly web-based platforms; and providing user-guiding questions and options related to HIV testing, prevention, and treatment. Conclusions: This study provides important insights into key features and potential implementation strategies central to designing an AI chatbot as a culturally sensitive digital health tool to prevent stigmatized health conditions in vulnerable and systematically marginalized populations. Such features not only are crucial to designing effective user-centered and culturally situated mobile health interventions for MSM in Malaysia but also illuminate the importance of incorporating social stigma considerations into health technology implementation strategies. ©Mary L Peng, Jeffrey A Wickersham, Frederick L Altice, Roman Shrestha, Iskandar Azwa, Xin Zhou, Mohd Akbar Ab Halim, Wan Mohd Ikhtiaruddin, Vincent Tee, Adeeba Kamarulzaman, Zhao Ni.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Duduka2022481,
	author = {Duduka, Jacint and Reis, Arsénio and Pereira, Rodrigo and Pires, Eduardo and Sousa, José and Pinto, Tiago},
	title = {The Impact of Artificial Intelligence on Chatbot Design},
	year = {2022},
	journal = {Communications in Computer and Information Science},
	volume = {1720 CCIS},
	pages = {481 – 486},
	doi = {10.1007/978-3-031-22918-3_39},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148035567&doi=10.1007%2f978-3-031-22918-3_39&partnerID=40&md5=4b9fb92c5295e4cf024d190bbbb0c494},
	abstract = {Artificial intelligence is transforming the way chatbots are created and used. The recent boom of artificial intelligence development is creating a whole new generation of intelligent approaches that enable a more efficient and effective design of chatbots. On the other hand, the increasing need and interest from the industry in artificial intelligence based solutions, is guaranteeing the necessary investment and applicational know-how that is pushing such solutions to a new dimension. Some relevant examples are e-commerce, health or education, which is the main focus of this work. This paper studies and analyses the impact that artificial intelligence models and solutions is having on the design and development of chatbots, when compared to the previously used approaches. Some of the most relevant current and future challenges in this domain are highlighted, which include language learning, sentiment interpretation, integration with other services, or data security and privacy issues. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Dhinagaran2022,
	author = {Dhinagaran, Dhakshenya Ardhithy and Martinengo, Laura and Ho, Moon-Ho Ringo and Joty, Shafiq and Kowatsch, Tobias and Atun, Rifat and Car, Lorainne Tudor},
	title = {Designing, Developing, Evaluating, and Implementing a Smartphone-Delivered, Rule-Based Conversational Agent (DISCOVER): Development of a Conceptual Framework},
	year = {2022},
	journal = {JMIR mHealth and uHealth},
	volume = {10},
	number = {10},
	doi = {10.2196/38740},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139572670&doi=10.2196%2f38740&partnerID=40&md5=a1475a83b0b14e62b570a1a4d92e6faa},
	abstract = {Background: Conversational agents (CAs), also known as chatbots, are computer programs that simulate human conversations by using predetermined rule-based responses or artificial intelligence algorithms. They are increasingly used in health care, particularly via smartphones. There is, at present, no conceptual framework guiding the development of smartphone-based, rule-based CAs in health care. To fill this gap, we propose structured and tailored guidance for their design, development, evaluation, and implementation. Objective: The aim of this study was to develop a conceptual framework for the design, evaluation, and implementation of smartphone-delivered, rule-based, goal-oriented, and text-based CAs for health care. Methods: We followed the approach by Jabareen, which was based on the grounded theory method, to develop this conceptual framework. We performed 2 literature reviews focusing on health care CAs and conceptual frameworks for the development of mobile health interventions. We identified, named, categorized, integrated, and synthesized the information retrieved from the literature reviews to develop the conceptual framework. We then applied this framework by developing a CA and testing it in a feasibility study. Results: The Designing, Developing, Evaluating, and Implementing a Smartphone-Delivered, Rule-Based Conversational Agent (DISCOVER) conceptual framework includes 8 iterative steps grouped into 3 stages, as follows: design, comprising defining the goal, creating an identity, assembling the team, and selecting the delivery interface; development, including developing the content and building the conversation flow; and the evaluation and implementation of the CA. They were complemented by 2 cross-cutting considerations-user-centered design and privacy and security-that were relevant at all stages. This conceptual framework was successfully applied in the development of a CA to support lifestyle changes and prevent type 2 diabetes. Conclusions: Drawing on published evidence, the DISCOVER conceptual framework provides a step-by-step guide for developing rule-based, smartphone-delivered CAs. Further evaluation of this framework in diverse health care areas and settings and for a variety of users is needed to demonstrate its validity. Future research should aim to explore the use of CAs to deliver health care interventions, including behavior change and potential privacy and safety concerns. © 2022 JMIR Publications. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Adel2022,
	author = {Adel, Kareem and Elhakeem, Ahmed and Marzouk, Mohamed},
	title = {Chatbot for construction firms using scalable blockchain network},
	year = {2022},
	journal = {Automation in Construction},
	volume = {141},
	doi = {10.1016/j.autcon.2022.104390},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131422074&doi=10.1016%2fj.autcon.2022.104390&partnerID=40&md5=3b208292cf2b97c9afbed442e231ed80},
	abstract = {Information and Communication Technologies (ICT), including multimedia tools, email services, voice-based tools, and handheld computing tools, have been extensively used for automating and digitalizing different construction processes and activities. However, these technologies are subjected to single-point attacks or failures, manipulation, and lack of privacy and traceability. This study introduces a novel information exchange and management system for construction firms based on blockchain technology and chatbots. The system leverages the characteristics of blockchain technology in terms of peer-to-peer operation mode, data integrity, structuring, and privacy, and the chatbots' merits regarding ease of use and degree of automation. The system is developed for tracking work progress in construction projects as a generic use-case using a four-step approach. First, a private blockchain network is configured for data distribution and storage. Second, a smart contract is coded for regulating data writing/reading operations. Third, a chatbot is developed for data collection and retrieval through textual conversations. Fourth, serverless cloud function and cloudant database are configured to allow the linkage between the blockchain network and the chatbot. A prototype of the system is built and applied to a case study of non-residential construction project to test and verify its capabilities. Further, the system's performance is assessed in terms of the writing and reading latencies and the storage size. The system features can be extended by embedding mathematical algorithms to simultaneously analyze data and employing Inter-Planetary File System (IPFS) to maintain visuals and large-size data. © 2022 Elsevier B.V.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Calvaresi20211,
	author = {Calvaresi, Davide and Calbimonte, Jean-Paul and Siboni, Enrico and Eggenschwiler, Stefan and Manzo, Gaetano and Hilfiker, Roger and Schumacher, Michael},
	title = {EREBOTS: Privacy-compliant agent-based platform for multi-scenario personalized health-assistant chatbots},
	year = {2021},
	journal = {Electronics (Switzerland)},
	volume = {10},
	number = {6},
	pages = {1 – 30},
	doi = {10.3390/electronics10060666},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102393472&doi=10.3390%2felectronics10060666&partnerID=40&md5=54dafd673b5c9a1bc4464dbb158a4a28},
	abstract = {Context. Asynchronous messaging is increasingly used to support human–machine inter-actions, generally implemented through chatbots. Such virtual entities assist the users in activities of different kinds (e.g., work, leisure, and health-related) and are becoming ingrained into humans’ habits due to factors including (i) the availability of mobile devices such as smartphones and tablets, (ii) the increasingly engaging nature of chatbot interactions, (iii) the release of dedicated APIs from messaging platforms, and (iv) increasingly complex AI-based mechanisms to power the bots’ behav-iors. Nevertheless, most of the modern chatbots rely on state machines (implementing conversational rules) and one-fits-all approaches, neglecting personalization, data-stream privacy management, multi-topic management/interconnection, and multimodal interactions. Objective. This work ad-dresses the challenges above through an agent-based framework for chatbot development named EREBOTS. Methods. The foundations of the framework are based on the implementation of (i) multi-front-end connectors and interfaces (i.e., Telegram, dedicated App, and web interface), (ii) enabling the configuration of multi-scenario behaviors (i.e., preventive physical conditioning, smoking cessa-tion, and support for breast-cancer survivors), (iii) online learning, (iv) personalized conversations and recommendations (i.e., mood boost, anti-craving persuasion, and balance-preserving physical exercises), and (v) responsive multi-device monitoring interface (i.e., doctor and admin). Results. EREBOTS has been tested in the context of physical balance preservation in social confinement times (due to the ongoing pandemic). Thirteen individuals characterized by diverse age, gender, and country distribution have actively participated in the experimentation, reporting advancements in the physical balance and overall satisfaction of the interaction and exercises’ variety they have been proposed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18}
}

@ARTICLE{Lu202342,
	author = {Lu, Qinghua and Luo, Yuxiu and Zhu, Liming and Tang, Mingjian and Xu, Xiwei and Whittle, Jon},
	title = {Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible Artificial Intelligence Engineering Approach},
	year = {2023},
	journal = {IEEE Intelligent Systems},
	volume = {38},
	number = {6},
	pages = {42 – 51},
	doi = {10.1109/MIS.2023.3320437},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165127803&doi=10.1109%2fMIS.2023.3320437&partnerID=40&md5=8315036640a9c833c146ede69c0be028},
	abstract = {The recent release of ChatGPT has gained huge attention and discussion worldwide, with responsible artificial intelligence (RAI) being a crucial topic of discussion. One key question is, "How can we ensure that AI systems, like ChatGPT, are developed and adopted in a responsible way?"To tackle RAI challenges, various ethical principles have been released by governments, organizations, and companies. However, those principles are very abstract and not practical enough. Further, significant efforts have been put on algorithm-level solutions that only address a narrow set of principles, such as fairness and privacy. To fill the gap, we adopt a pattern-oriented RAI engineering approach and build an RAI pattern catalog to operationalize RAI from a system perspective. In this article, we first summarize the major challenges in operationalizing RAI at scale and introduce how we use the RAI pattern catalog to address those challenges. We then examine the risks at each stage of the chatbot development process and recommend pattern-driven mitigations to evaluate the usefulness of the RAI pattern catalog in a real-world setting.  © 2001-2011 IEEE.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Arpaci20241714,
	author = {Arpaci, Ibrahim},
	title = {A Multianalytical SEM-ANN Approach to Investigate the Social Sustainability of AI Chatbots Based on Cybersecurity and Protection Motivation Theory},
	year = {2024},
	journal = {IEEE Transactions on Engineering Management},
	volume = {71},
	pages = {1714 – 1725},
	doi = {10.1109/TEM.2023.3339578},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179829296&doi=10.1109%2fTEM.2023.3339578&partnerID=40&md5=05c0059b46fa21ab5fc9c495e20c743a},
	abstract = {—With a primary focus on cybersecurity risks, this study endeavors to explore the sustainable deployment of artificial intelligence (AI) chatbots and, ultimately, to promote their social sustainability. The study introduces an enhanced model built upon the “Protection Motivation Theory” (PMT) to explore the factors that predict the social sustainability of AI chatbots. The proposed model is evaluated using both “structural equation modeling” and “artificial neural network” (ANN) analyses, leveraging data obtained from 1741 participants. The findings reveal that PMT factors significantly predict the sustainable use of AI chatbots. Moreover, cybersecurity concerns, including confidentiality and privacy, have emerged as significant predictors of sustainable use, impacting the social sustainability of AI chatbots. The indicated paths in the model explain 70% and 74% of the variance in sustainable use and social sustainability, respectively. The results from the ANN analysis also emphasize the critical role of confidentiality as the primary predictor. The significance of this study lies in the development of a unified model that integrates cybersecurity and PMT, offering a distinctive framework. In addition to its theoretical contributions, the study offers practical insights for service providers, application developers, and decision-makers in the field, thereby influencing the future of AI chatbots. © 2023 IEEE.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Fournier-Tombs2023,
	author = {Fournier-Tombs, Eleonore and McHardy, Juliette},
	title = {A Medical Ethics Framework for Conversational Artificial Intelligence},
	year = {2023},
	journal = {Journal of Medical Internet Research},
	volume = {25},
	doi = {10.2196/43068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165788275&doi=10.2196%2f43068&partnerID=40&md5=4bab0dcf8b77bbaf4f834a1bdf405019},
	abstract = {The launch of OpenAI’s GPT-3 model in June 2020 began a new era for conversational chatbots. While there are chatbots that do not use artificial intelligence (AI), conversational chatbots integrate AI language models that allow for back-and-forth conversation between an AI system and a human user. GPT-3, since upgraded to GPT-4, harnesses a natural language processing technique called sentence embedding and allows for conversations with users that are more nuanced and realistic than before. The launch of this model came in the first few months of the COVID-19 pandemic, where increases in health care needs globally combined with social distancing measures made virtual medicine more relevant than ever. GPT-3 and other conversational models have been used for a wide variety of medical purposes, from providing basic COVID-19–related guidelines to personalized medical advice and even prescriptions. The line between medical professionals and conversational chatbots is somewhat blurred, notably in hard-to-reach communities where the chatbot replaced face-to-face health care. Considering these blurred lines and the circumstances accelerating the adoption of conversational chatbots globally, we analyze the use of these tools from an ethical perspective. Notably, we map out the many types of risks in the use of conversational chatbots in medicine to the principles of medical ethics. In doing so, we propose a framework for better understanding the effects of these chatbots on both patients and the medical field more broadly, with the hope of informing safe and appropriate future developments. © Eleonore Fournier-Tombs, Juliette McHardy.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Figueroa2021,
	author = {Figueroa, Caroline A. and Luo, Tiffany C. and Jacobo, Andrea and Munoz, Alan and Manuel, Minx and Chan, David and Canny, John and Aguilera, Adrian},
	title = {Conversational Physical Activity Coaches for Spanish and English Speaking Women: A User Design Study},
	year = {2021},
	journal = {Frontiers in Digital Health},
	volume = {3},
	doi = {10.3389/fdgth.2021.747153},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131227176&doi=10.3389%2ffdgth.2021.747153&partnerID=40&md5=17837b264b5c443418797030ad583d4f},
	abstract = {Introduction: Digital technologies, including text messaging and mobile phone apps, can be leveraged to increase people's physical activity and manage health. Chatbots, powered by artificial intelligence, can automatically interact with individuals through natural conversation. They may be more engaging than one-way messaging interventions. To our knowledge, physical activity chatbots have not been developed with low-income participants, nor in Spanish—the second most dominant language in the U.S. We recommend best practices for physical activity chatbots in English and Spanish for low-income women. Methods: We designed a prototype physical activity text-message based conversational agent based on various psychotherapeutic techniques. We recruited participants through SNAP-Ed (Supplemental Nutrition Assistance Program Education) in California (Alameda County) and Tennessee (Shelby County). We conducted qualitative interviews with participants during testing of our prototype chatbot, held a Wizard of Oz study, and facilitated a co-design workshop in Spanish with a subset of our participants. Results: We included 10 Spanish- and 8 English-speaking women between 27 and 41 years old. The majority was Hispanic/Latina (n = 14), 2 were White and 2 were Black/African American. More than half were monolingual Spanish speakers, and the majority was born outside the US (>50% in Mexico). Most participants were unfamiliar with chatbots and were initially skeptical. After testing our prototype, most users felt positively about health chatbots. They desired a personalized chatbot that addresses their concerns about privacy, and stressed the need for a comprehensive system to also aid with nutrition, health information, stress, and involve family members. Differences between English and monolingual Spanish speakers were found mostly in exercise app use, digital literacy, and the wish for family inclusion. Conclusion: Low-income Spanish- and English-speaking women are interested in using chatbots to improve their physical activity and other health related aspects. Researchers developing health chatbots for this population should focus on issues of digital literacy, app familiarity, linguistic and cultural issues, privacy concerns, and personalization. Designing and testing this intervention for and with this group using co-creation techniques and involving community partners will increase the probability that it will ultimately be effective. © Copyright © 2021 Figueroa, Luo, Jacobo, Munoz, Manuel, Chan, Canny and Aguilera.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Bouhia20221159,
	author = {Bouhia, Mariem and Rajaobelina, Lova and PromTep, Sandrine and Arcand, Manon and Ricard, Line},
	title = {Drivers of privacy concerns when interacting with a chatbot in a customer service encounter},
	year = {2022},
	journal = {International Journal of Bank Marketing},
	volume = {40},
	number = {6},
	pages = {1159 – 1181},
	doi = {10.1108/IJBM-09-2021-0442},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130135261&doi=10.1108%2fIJBM-09-2021-0442&partnerID=40&md5=67e13d769c9207a4239369d88b4bd042},
	abstract = {Purpose: This study aims to examine the antecedents of privacy concerns in the era of artificial intelligence. Specifically, it focuses on the impact of various factors related to interactions with a chatbot (creepiness and perceived risk) and individual traits (familiarity with chatbots and need for privacy) in relation to privacy when interacting with a chatbot in the context of financial services. The moderating effect of gender on these relationships was also examined. Design/methodology/approach: A total of 430 Canadians responded to an online questionnaire after interacting with a chatbot in the context of a simulated auto insurance quote. A structural equation model was used to test the hypotheses. Findings: The results showed that privacy concerns are influenced primarily by creepiness, followed by perceived risk and the need for privacy. The last two relationships are moderated by gender. Conversely, familiarity with chatbots does not affect privacy concerns in this context. Originality/value: This study is the first to consider the influence of creepiness as an antecedent of privacy concerns arising from interactions with AI tools and highlight its key impacts. It also shows how gender moderates specific relationships in this context. © 2022, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Motger2021347,
	author = {Motger, Quim and Franch, Xavier and Marco, Jordi},
	title = {Integrating Adaptive Mechanisms into Mobile Applications Exploiting User Feedback},
	year = {2021},
	journal = {Lecture Notes in Business Information Processing},
	volume = {415 LNBIP},
	pages = {347 – 355},
	doi = {10.1007/978-3-030-75018-3_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111164309&doi=10.1007%2f978-3-030-75018-3_23&partnerID=40&md5=f5a523a708437fe0be1f015acfd94b75},
	abstract = {Mobile applications have become a commodity in multiple daily scenarios. Their increasing complexity has led mobile software ecosystems to become heterogeneous in terms of hardware specifications, features and context of use, among others. For their users, fully exploiting their potential has become challenging. While enacting software systems with adaptation mechanisms has proven to ease this burden from users, mobile devices present specific challenges related to privacy and security concerns. Nevertheless, rather than being a limitation, users can play a proactive role in the adaptation loop by providing valuable feedback for runtime adaptation. To this end, we propose the use of chatbots to interact with users through a human-like smart conversational process. We depict a work-in-progress proposal of an end-to-end framework to integrate semi-automatic adaptation mechanisms for mobile applications. These mechanisms include the integration of both implicit and explicit user feedback for autonomous user categorization and execution of enactment action plans. We illustrate the applicability of such techniques through a set of scenarios from the Mozilla mobile applications suite. We envisage that our proposal will improve user experience by bridging the gap between users’ needs and the capabilities of their mobile devices through an intuitive and minimally invasive conversational mechanism. © 2021, Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Duvvuri2022111,
	author = {Duvvuri, Venkata and Guan, Qihan and Daddala, Swetha and Harris, Mitch and Kaushik, Sudhakar},
	title = {Predicting Depression Symptoms from Discord Chat Messaging Using AI Medical Chatbots},
	year = {2022},
	journal = {ACM International Conference Proceeding Series},
	pages = {111 – 119},
	doi = {10.1145/3523150.3523168},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128732969&doi=10.1145%2f3523150.3523168&partnerID=40&md5=f375d93733d928d970b322fe472202fe},
	abstract = {Depression is a chronic illness with even Olympic athletes [1] and top tennis players [2] withdrawing from competitions due to it. It's important to diagnose depression early. Traditional methods rely on questionnaires to evaluate depression. But they have their limitations due to inherent bias and inhibitions in self reporting. We propose an intelligent chatbot powered by AI approach to assist medical professionals diagnose depression. Specifically, the AI could predict depression symptoms in conversations with pre-care health care personnel and/or personal chats. Notably, due to sensitivity and privacy regulation (HIPPA) such data is not readily available [3]. To use AI driven medical assistants, and to overcome this limitation in training AI models, we propose to seed the models with recent chat engine (Discord) conversation dataset in the channel #depression. We achieve at least 73% accuracy in predicting seven key symptoms for depression using our best ML models. Secondly, with our ensemble random forest model we could recall 30-69% of depression symptoms with 60-99% depression diagnosis accuracy which could be further tuned by medical professional if they know which ones of these symptoms is a key predictor of depression.  © 2022 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Kayalı202320,
	author = {Kayalı, Bünyami and Yavuz, Mehmet and Balat, Şener and Çalışan, Mücahit},
	title = {Investigation of student experiences with ChatGPT-supported online learning applications in higher education},
	year = {2023},
	journal = {Australasian Journal of Educational Technology},
	volume = {39},
	number = {5},
	pages = {20 – 39},
	doi = {10.14742/ajet.8915},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184199144&doi=10.14742%2fajet.8915&partnerID=40&md5=b9dd2fa4a9cc465424d4bcb26f4ba8f3},
	abstract = {The purpose of this study was to determine university students' experiences with the use of ChatGPT in online courses. The sample consisted of 84 associate degree students from a state university in Turkey. A multi-method approach was used in the study. Although quantitative data were collected using the Chatbot Usability Scale, qualitative data were collected using a semi-structured interview form that we developed. The data were analysed using descriptive and content analysis methods. According to the findings, ChatGPT exhibits advantages such as a user-friendly interface and fast, concise, relevant responses. Moreover, emphasizing its contribution to the learning process, the information provided was sufficient and topic-oriented. The understandability of the chatbot’s functions and the clarity of their communication were emphasized. However, there are disadvantages such as performance issues, frequency of errors and the risk of providing misleading information. Concerns have also been raised about the potential difficulties chatbots may face in ambiguous conversations and providing insufficient information on privacy issues. In conclusion, ChatGPT is recognised as a potentially valuable tool in education based on positive usability impressions; however, more research is needed for its safe use. Implications for practice or policy • Based on positive usability impressions, students and instructors can use ChatGPT to support educational activities. • ChatGPT can promote and enhance students' personalised learning experiences. • ChatGPT can be used in all higher education courses. • Users should be cautious about the accuracy and reliability of the answers provided by ChatGPT. • Decision-makers should take precautions against risks such as privacy, ethics, confidentiality and security that may arise from using artificial intelligence in education. © Articles published in the Australasian Journal of Educational Technology (AJET) are available under Creative Commons Attribution Non-Commercial No Derivatives Licence (CC BY-NC-ND 4.0). Authors retain copyright in their work and grant AJET right of first publication under CC BY-NC-ND 4.0.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Martinengo2023,
	author = {Martinengo, Laura and Lin, Xiaowen and Jabir, Ahmad Ishqi and Kowatsch, Tobias and Atun, Rifat and Car, Josip and Car, Lorainne Tudor},
	title = {Conversational Agents in Health Care: Expert Interviews to Inform the Definition, Classification, and Conceptual Framework},
	year = {2023},
	journal = {Journal of Medical Internet Research},
	volume = {25},
	doi = {10.2196/50767},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175877496&doi=10.2196%2f50767&partnerID=40&md5=820a29f8f0b85cb62ee627f7987646c7},
	abstract = {Background: Conversational agents (CAs), or chatbots, are computer programs that simulate conversations with humans. The use of CAs in health care settings is recent and rapidly increasing, which often translates to poor reporting of the CA development and evaluation processes and unreliable research findings. We developed and published a conceptual framework, designing, developing, evaluating, and implementing a smartphone-delivered, rule-based conversational agent (DISCOVER), consisting of 3 iterative stages of CA design, development, and evaluation and implementation, complemented by 2 cross-cutting themes (user-centered design and data privacy and security). Objective: This study aims to perform in-depth, semistructured interviews with multidisciplinary experts in health care CAs to share their views on the definition and classification of health care CAs and evaluate and validate the DISCOVER conceptual framework. Methods: We conducted one-on-one semistructured interviews via Zoom (Zoom Video Communications) with 12 multidisciplinary CA experts using an interview guide based on our framework. The interviews were audio recorded, transcribed by the research team, and analyzed using thematic analysis. Results: Following participants’ input, we defined CAs as digital interfaces that use natural language to engage in a synchronous dialogue using ≥1 communication modality, such as text, voice, images, or video. CAs were classified by 13 categories: response generation method, input and output modalities, CA purpose, deployment platform, CA development modality, appearance, length of interaction, type of CA-user interaction, dialogue initiation, communication style, CA personality, human support, and type of health care intervention. Experts considered that the conceptual framework could be adapted for artificial intelligence–based CAs. However, despite recent advances in artificial intelligence, including large language models, the technology is not able to ensure safety and reliability in health care settings. Finally, aligned with participants’ feedback, we present an updated iteration of the conceptual framework for health care conversational agents (CHAT) with key considerations for CA design, development, and evaluation and implementation, complemented by 3 cross-cutting themes: ethics, user involvement, and data privacy and security. Conclusions: We present an expanded, validated CHAT and aim at guiding researchers from a variety of backgrounds and with different levels of expertise in the design, development, and evaluation and implementation of rule-based CAs in health care settings. © 2023 Journal of Medical Internet Research. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Huallpa2023105,
	author = {Huallpa, Jorge Jinchuña and Flores Arocutipa, Javier Pedro and Panduro, Walker Diaz and Huete, Luis Chauca and Flores Limo, Fernando Antonio and Herrera, Edward Espinoza and Alba Callacna, Rafael Arturo and Ariza Flores, Victor Andre and Medina Romero, Miguel Ángel and Quispe, Isaac Merino and Hernández Hernández, Fredy Alberto},
	title = {Exploring the ethical considerations of using Chat GPT in university education},
	year = {2023},
	journal = {Periodicals of Engineering and Natural Sciences},
	volume = {11},
	number = {4},
	pages = {105 – 115},
	doi = {10.21533/pen.v11i4.3770},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172990878&doi=10.21533%2fpen.v11i4.3770&partnerID=40&md5=d257b7a9f07ce86bf5112b47d7986733},
	abstract = {This study investigates the moral dilemmas that arise with incorporating Chat GPT into higher education, with a focus on the situation in Latinoamerican institutions of higher learning. The study surveyed 220 people via online questionnaire to learn more about their experiences with and motivations for using AI-powered conversational agents. An overview of the demographics of the participants was provided through descriptive statistics. This investigation of the subject at hand lays the groundwork for further research. It also reveals the hidden meanings of the observed phenomena, and it suggests possible solutions to the problems that have been uncovered. This research looks at how AI systems and chatbots can supplement human knowledge and judgment, as well as their potential drawbacks. The results showed that participants thought Chat GPT integration was moderately accessible and had moderately positive social attitudes. They understood the value and responsibility of Chat GPT in creating individualized educational opportunities. Participants stressed the necessity for explicit institutional standards regarding privacy and data security. Gender, age, sense of accessibility, social attitude, opinions, and personal experience, privacy and data security, institutional guidelines, and individualized learning were also found to affect participants' reliance on AI through regression analysis. The findings shed light on how the integration of Chat GPT into Latinoamerican higher education is complicated by factors such as individual beliefs, cultural norms, and ethical problems. The busy schedules of students may be accommodated and the resources they need to succeed can be made available thanks to this adaptability. In addition, natural language processing models can offer students instantaneous help via text chat, voice, or video. To fully grasp the ethical consequences and lead the creation of responsible implementation techniques, the research proposes that additional qualitative investigations, longitudinal studies, and comparative research across diverse contexts is required. Closing these knowledge gaps will help move the conversational AI field forward in ways that are ethical and beneficial to the classroom. © The Author 2023. This work is licensed under a Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/) that allows others to share and adapt the material for any purpose (even commercially), in any medium with an acknowledgement of the work's authorship and initial publication in this journal. All Rights Reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Alt2021165,
	author = {Alt, Mónika-Anetta and Ibolya, Vizeli},
	title = {IDENTIFYING RELEVANT SEGMENTS OF POTENTIAL BANKING CHATBOT USERS BASED ON TECHNOLOGY ADOPTION BEHAVIOR; [IDENTIFICIRANJE RELEVANTNIH SEGMENATA POTENCIJALNIH KORISNIKA CHATBOTA U BANKARSTVU NA TEMELJU PONAŠANJA PRI PRIHVAĆANJU TEHNOLOGIJE]},
	year = {2021},
	journal = {Market-Trziste},
	volume = {33},
	number = {2},
	pages = {165 – 183},
	doi = {10.22598/mt/2021.33.2.165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123023220&doi=10.22598%2fmt%2f2021.33.2.165&partnerID=40&md5=da181df15bc8f67a24635f379453c7d9},
	abstract = {Purpose – Chatbot technology is expected to revolution-ize customer service in financial institutions. However, the adoption of customer service chatbots in banking remains low. Therefore, the aim of this paper is to identify relevant segments of potential banking chatbot users based on technology adoption behavior. Design/Methodology/Approach – Data for the research was collected through an online questionnaire in Romania using the non-probability sampling method. The 287 questionnaires were analyzed using hierarchical and k-means cluster analysis. Findings and implications – The analysis revealed three distinct segments: Innovators (26%), consisting of highly educated young women employed in the business sec-tor; the Late Majority (55%), consisting of young women with higher education degrees who work in services-re-lated fields; and Laggards (19%), consisting of educated middle-aged men employed in the business sector. New significant differences among demographic and banking behavior variables were observed across the profiles of potential banking chatbot user segments. Limitations – The study is based on a non-probability sample collected from only one country, with a rather small sample size. Originality – Technology acceptance variables (perceived usefulness, perceived ease of use), expanded to include constructs such as awareness of service, perceived privacy risk, and perceived compatibility, were found to be appro-priate for customer segmentation purposes in the context of chatbot applications based on artificial intelligence. The study also revealed a new innovator demographic profile. © 2021, University of Zagreb, Faculty of Economics and Business Zagreb. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Morsi2023156,
	author = {Morsi, Shereen},
	title = {Artificial Intelligence in Electronic Commerce: Investigating the Customers' Acceptance of Using Chatbots},
	year = {2023},
	journal = {Journal of System and Management Sciences},
	volume = {13},
	number = {3},
	pages = {156 – 176},
	doi = {10.33168/JSMS.2023.0311},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162777495&doi=10.33168%2fJSMS.2023.0311&partnerID=40&md5=dc55a2eed4f57fca6c72a3910b76d5c8},
	abstract = {Artificial intelligence (AI) has become an important tool for companies trying to gain a competitive edge in the online market. Business to Consumer (B2C) e-commerce firms are increasingly integrating chatbots as virtual shopping assistants for providing more personalized and efficient shopping experiences to online customers. Chatbots are AI-powered software programs that can communicate with users through text or voice interfaces. However, there is a lack of research on the acceptance of chatbots in B2C e-commerce in Egypt. Therefore, this research tries to fill the gap in e-commerce chatbot literature by investigating the acceptance of the use of chatbots in online shopping among Egyptian users by applying the Use and Gratification theory. This is an exploratory study using a quantitative survey-based approach for collecting data from online Egyptian customers on their attitudes or intentions to use Chatbots in online shopping. The data were analysed by using regression analysis for identifying factors that influence user acceptance and usage of chatbots. The results revealed that both of hedonic and technology factors have positive influence on users' behavioural intention. While the risk factor which has two sub-factors namely; privacy and immature technology has negative influence on customers' behavioural intentions. The study contributes to the expanding body of literature on the acceptance and use of Chatbots among customers in B2C e-commerce context. In addition, the study's findings give significant insights for Egyptian online retailers looking to implement Chatbots in their customer service strategy. © 2023, Success Culture Press. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Biswas2020179,
	author = {Biswas, Debmalya},
	title = {Privacy Preserving Chatbot Conversations},
	year = {2020},
	journal = {Proceedings - 2020 IEEE 3rd International Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2020},
	pages = {179 – 182},
	doi = {10.1109/AIKE48582.2020.00035},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101913467&doi=10.1109%2fAIKE48582.2020.00035&partnerID=40&md5=a1377d90a202e134c79524b49f7c47b9},
	abstract = {With chatbots gaining traction and their adoption growing in different verticals, e.g. Health, Banking, Dating; and users sharing more and more private information with chatbots - studies have started to highlight the privacy risks of chatbots. In this paper, we propose two privacypreserving approaches for chatbot conversations. The first approach applies 'entity' based privacy filtering and transformation, and can be applied directly on the app (client) side. It however requires knowledge of the chatbot design to be enabled. We present a second scheme based on Searchable Encryption that is able to preserve user chat privacy, without requiring any knowledge of the chatbot design. Finally, we present some experimental results based on a real-life employee Help Desk chatbot that validates both the need and feasibility of the proposed approaches. © 2020 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Liu2023115,
	author = {Liu, Liyuan and Kong, Ying and Li, Gaolei and Han, Meng},
	title = {FairShare: An Incentive-Based Fairness-Aware Data Sharing Framework for Federated Learning},
	year = {2023},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {14268 LNAI},
	pages = {115 – 126},
	doi = {10.1007/978-981-99-6486-4_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175984865&doi=10.1007%2f978-981-99-6486-4_10&partnerID=40&md5=c7f91e4a46c8ce99e503972eceafda6a},
	abstract = {Federated learning protects sensitive data during AI model training, enabling collaboration without sharing raw data. Ensuring fairness and addressing un-shared decisions are crucial for reliable federated learning. This study introduces “FairShare”, an incentive mechanism enhancing reliability in applications like financial fraud detection and chatbot customer service. FairShare has two stages: stage one uses the Vickrey-Clarke-Groves (VCG) auction to estimate clients’ true costs, ensuring truthfulness, individual rationality, and computational efficiency. Stage two employs the Shapley Value method to allocate fair payments, promoting high-quality data usage. FairShare encourages clients to balance local datasets, reducing bias and increasing reliability. Theoretical proofs and experiments demonstrate FairShare’s effectiveness in ensuring fairness and protecting data privacy, leading to reliable outcomes. Experiments confirm the VCG-based mechanism’s truthfulness and Shapley Value’s fairness in payment allocation. In conclusion, FairShare addresses fairness challenges and fosters reliability in federated learning, facilitating dependable human-machine interactions. Implementing FairShare can enhance sensitive data protection, promote fairness, and improve reliability in human-machine interactions, supporting federated learning adoption across sectors. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Chametka2023,
	author = {Chametka, Paulina and Maqsood, Sana and Chiasson, Sonia},
	title = {Security and Privacy Perceptions of Mental Health Chatbots},
	year = {2023},
	journal = {2023 20th Annual International Conference on Privacy, Security and Trust, PST 2023},
	doi = {10.1109/PST58708.2023.10320174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179559207&doi=10.1109%2fPST58708.2023.10320174&partnerID=40&md5=efdf9a19049939d4deccfdce4fcce43f},
	abstract = {Mental health chatbots are AI chatbots that aim to mimic human conversations about how a user feels, help a user work through issues they are facing, suggest wellness exercises to complete, and help track a user's mood over time. We compare the information disclosure practices and security and privacy concerns of adopters and non-adopters of mental health chatbots. We conducted a survey with 180 participants (30 adopters, 150 non-adopters), collecting data about what information they would hypothetically disclose to mental health chatbots, and concerns they had related to chatbots. We found that compared to non-adopters, adopters were more trusting of chatbots, were willing to reveal more information, perceived security and privacy risks to be less likely, and took fewer precautions.  © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Følstad2020671,
	author = {Følstad, Asbjørn and Halvorsrud, Ragnhild},
	title = {Communicating Service Offers in a Conversational User Interface: An Exploratory Study of User Preferences in Chatbot Interaction},
	year = {2020},
	journal = {ACM International Conference Proceeding Series},
	pages = {671 – 676},
	doi = {10.1145/3441000.3441046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101734030&doi=10.1145%2f3441000.3441046&partnerID=40&md5=0aa3188c5e7fb2197ee6f07be5dde0c4},
	abstract = {The increased interest in chatbots accentuates the importance of conversational design. A key conversational design challenge concerns how to communicate available service offers to users. We present an exploratory study, conducted in the context of financial service provision. Here, we first detailed four alternative approaches to communicate available service offers, reflecting different levels of proactivity. We then gathered feedback on user preference through interviews with 17 users following their interactions with prototypes representing the four approaches. Proactivity in the communication of service offers was found to be potentially valuable, provided that the offer is relevant to the conversation, do not compromise conversational efficiency, and is easy to discard. However, proactive communication of service offers may also entail challenges concerning perceptions of privacy and invasiveness, and, hence, needs to be designed with great care. Based on our findings, we summarize implications for theory and practice and propose directions for future research.  © 2020 Owner/Author.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Tian20221119,
	author = {Tian, Wensheng and Lu, Yifan and Yu, Jinhao and Fan, Jiafeng and Tang, Panpan and Zhang, Lei},
	title = {A Privacy-Preserving Framework for Mental Health Chatbots Based on Confidential Computing},
	year = {2022},
	journal = {Proceedings - 2022 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Autonomous and Trusted Vehicles, Scalable Computing and Communications, Digital Twin, Privacy Computing, Metaverse, SmartWorld/UIC/ATC/ScalCom/DigitalTwin/PriComp/Metaverse 2022},
	pages = {1119 – 1124},
	doi = {10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00160},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168083316&doi=10.1109%2fSmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00160&partnerID=40&md5=f30dfb863c56273a453922ae620daa4f},
	abstract = {Mental health chatbots can provide psychological counseling services to patients at any time regardless of time and location, which can not only relieve patients' ailments but also reduce the workload of psychologists. In order to provide patients with accurate diagnosis and treatment services, mental health robots inevitably collect patient-related information during the communication process with patients, which is often very sensitive and must be well protected. There is a lack of targeted research on how mental health chatbots can provide systematic privacy preserving for patients in the process of providing mental health services to them. In this paper, we propose a privacy preserving framework based on blockchain and confidential computing that can provide comprehensive privacy preserving for patients during mental health chatbot services. We conduct tests using existing mental health chatbots, and the experimental results demonstrate that our proposed framework can meet the requirements for privacy preserving and computational performance of mental health chatbots. © 2022 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Siemon2022,
	author = {Siemon, Dominik and Ahmad, Rangina and Harms, Henrik and de Vreede, Triparna},
	title = {Requirements and Solution Approaches to Personality-Adaptive Conversational Agents in Mental Health Care},
	year = {2022},
	journal = {Sustainability (Switzerland)},
	volume = {14},
	number = {7},
	doi = {10.3390/su14073832},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127554480&doi=10.3390%2fsu14073832&partnerID=40&md5=582917541399e326caf1a183aa6bbbb2},
	abstract = {Artificial intelligence (AI) technologies enable Conversational Agents (CAs) to perform highly complex tasks in a human-like manner and may help people cope with anxiety to improve their mental health and well-being. To support patients with their mental well-being in an authentic way, CAs need to be imbued with human-like behavior, such as personality. In this paper we cover an innovative form of CA, so-called Personality-Adaptive Conversational Agents (PACAs) that automatically infer users’ personality traits and adapt accordingly to their personality. We empirically investigate their benefits and caveats in mental health care. The results of our study show that PACAs can be beneficial for mental health support, but they also raise concerns about trust and privacy issues. We present a set of relevant requirements for designing PACAs and provide solution approaches that can be followed when designing and implementing PACAs for mental health care. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Rao2023,
	author = {Rao, Milind and Chennupati, Gopinath and Tiwari, Gautam and Kumar Sahu, Anit and Raju, Anirudh and Rastrow, Ariya and Droppo, Jasha},
	title = {Federated Self-Learning with Weak Supervision for Speech Recognition},
	year = {2023},
	journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	doi = {10.1109/ICASSP49357.2023.10096983},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168630770&doi=10.1109%2fICASSP49357.2023.10096983&partnerID=40&md5=29db6be499fa7491e096ff973dbe5d76},
	abstract = {Automatic speech recognition (ASR) models with low-footprint are increasingly being deployed on edge devices for conversational agents, which enhances privacy. We study the problem of federated continual incremental learning for recurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancing scheme of learning on-device, without access to ground truth human transcripts or machine transcriptions from a stronger ASR model. In particular, we study the performance of a self-learning based scheme, with a paired teacher model updated through an exponential moving average of ASR models. Further, we propose using possibly noisy weak-supervision signals such as feedback scores and natural language understanding semantics determined from user behavior across multiple turns in a session of interactions with the conversational agent. These signals are leveraged in a multitask policy-gradient training approach to improve the performance of self-learning for ASR. Finally, we show how catastrophic forgetting can be mitigated by combining on-device learning with a memory-replay approach using selected historical datasets. These innovations allow for 10% relative improvement in WER on new use cases with minimal degradation on other test sets in the absence of strong-supervision signals such as ground-truth transcriptions.  © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Pizzi20231372,
	author = {Pizzi, Gabriele and Vannucci, Virginia and Mazzoli, Valentina and Donvito, Raffaele},
	title = {I, chatbot! the impact of anthropomorphism and gaze direction on willingness to disclose personal information and behavioral intentions},
	year = {2023},
	journal = {Psychology and Marketing},
	volume = {40},
	number = {7},
	pages = {1372 – 1387},
	doi = {10.1002/mar.21813},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150974118&doi=10.1002%2fmar.21813&partnerID=40&md5=7586c860885fd96262417ee9e6157c20},
	abstract = {The present research focuses on the interplay between two common features of the customer service chatbot experience: gaze direction and anthropomorphism. Although the dominant approach in marketing theory and practice is to make chatbots as human-like as possible, the current study, built on the humanness-value-loyalty model, addresses the chain of effects through which chatbots' nonverbal behaviors affect customers' willingness to disclose personal information and purchase intentions. By means of two experiments that adopt a real chatbot in a simulated shopping environment (i.e., car rental and travel insurance), the present work allows us to understand how to reduce individuals' tendency to see conversational agents as less knowledgeable and empathetic compared with humans. The results show that warmth perceptions are affected by gaze direction, whereas competence perceptions are affected by anthropomorphism. Warmth and competence perceptions are found to be key drivers of consumers’ skepticism toward the chatbot, which, in turn, affects consumers’ trust toward the service provider hosting the chatbot, ultimately leading consumers to be more willing to disclose their personal information and to repatronize the e-tailer in the future. Building on the Theory of Mind, our results show that perceiving competence from a chatbot makes individuals less skeptical as long as they feel they are good at detecting others’ ultimate intentions. © 2023 The Authors. Psychology & Marketing published by Wiley Periodicals LLC.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Meskó2023,
	author = {Meskó, Bertalan and Topol, Eric J.},
	title = {The imperative for regulatory oversight of large language models (or generative AI) in healthcare},
	year = {2023},
	journal = {npj Digital Medicine},
	volume = {6},
	number = {1},
	doi = {10.1038/s41746-023-00873-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164295991&doi=10.1038%2fs41746-023-00873-0&partnerID=40&md5=657da2c6a8b235b18551d0094cd60b67},
	abstract = {The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated large language models (LLMs) such as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention because of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization, summarizing research papers, or working as a chatbot to answer questions for patients about their specific data and concerns. While offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-based medical technologies that are regulated already, especially within the critical context of caring for patients. The newest version, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and risks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read texts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare without damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical standards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use LLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what we can expect from regulators to bring this vision to reality. © 2023, The Author(s).},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 56}
}

@ARTICLE{Sanabria2023382,
	author = {Sanabria, Gabriella and Greene, Karah Y. and Tran, Jennifer T. and Gilyard, Shelton and DiGiovanni, Lauren and Emmanuel, Patricia J. and Sanders, Lisa J. and Kosyluk, Kristin and Galea, Jerome T.},
	title = {“A Great Way to Start the Conversation”: Evidence for the Use of an Adolescent Mental Health Chatbot Navigator for Youth at Risk of HIV and Other STIs},
	year = {2023},
	journal = {Journal of Technology in Behavioral Science},
	volume = {8},
	number = {4},
	pages = {382 – 391},
	doi = {10.1007/s41347-023-00315-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159115973&doi=10.1007%2fs41347-023-00315-4&partnerID=40&md5=caecf3131f5f4bd49caa6a995119866f},
	abstract = {Chatbot use is increasing for mobile health interventions on sensitive and stigmatized topics like mental health because of their anonymity and privacy. This anonymity provides acceptability to sexual and gendered minority youth (ages 16–24) at increased risk of HIV and other STIs with poor mental health due to higher levels of stigma, discrimination, and social isolation. This study evaluates the usability of Tabatha-YYC, a pilot chatbot navigator created to link these youth to mental health resources. Tabatha-YYC was developed using a Youth Advisory Board (n = 7). The final design underwent user testing (n = 20) through a think-aloud protocol, semi-structured interview, and a brief survey post-exposure which included the Health Information Technology Usability Evaluation Scale. The chatbot was found to be an acceptable mental health navigator by participants. This study provides important design methodology considerations and key insights into chatbot design preferences of youth at risk of STIs seeking mental health resources. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Chowdhury2023362,
	author = {Chowdhury, Md Naseef-Ur-Rahman and Haque, Ahshanul and Soliman, Hamdy},
	title = {Chatbots: A Game Changer in mHealth},
	year = {2023},
	journal = {Proceedings - 2023 6th International Symposium on Computer, Consumer and Control, IS3C 2023},
	pages = {362 – 366},
	doi = {10.1109/IS3C57901.2023.00103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171465780&doi=10.1109%2fIS3C57901.2023.00103&partnerID=40&md5=7718e3624130620a90fe4a1cd59e5b49},
	abstract = {Chatbots have emerged as a promising tool in healthcare for improving patient engagement, providing education and support, and delivering interventions for a variety of health conditions. In the field of mHealth (mobile health), chatbots are being increasingly used to support self-management, provide remote monitoring, and offer personalized coaching to patients with chronic conditions. A growing body of research has demonstrated the feasibility, acceptability, and effectiveness of chatbots in mHealth, with many studies reporting positive outcomes such as improved patient adherence, increased physical activity, and reduced hospital readmissions. However, there are also limitations and challenges to the use of chatbots in mHealth, such as concerns around data privacy and security, the need for effective natural language processing and machine learning algorithms, and ensuring that chatbots are designed with the end user in mind. Future research is needed to further explore the potential of chatbots in mHealth, and to develop best practices for their design, implementation, and evaluation.  © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Mohammed20231007,
	author = {Mohammed, Belghachi},
	title = {A Review on Explainable Artificial Intelligence Methods, Applications, and Challenges},
	year = {2023},
	journal = {Indonesian Journal of Electrical Engineering and Informatics},
	volume = {11},
	number = {4},
	pages = {1007 – 1024},
	doi = {10.52549/ijeei.v11i4.5151},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183189918&doi=10.52549%2fijeei.v11i4.5151&partnerID=40&md5=2f016dfe7bf6939a45cdd3a081206efe},
	abstract = {Explainable Artificial Intelligence (XAI) has emerged as a critical area of research and development in the field of artificial intelligence. This abstract provides an overview of XAI, covering its methods, applications, and challenges. XAI Methods: XAI methods aim to enhance the transparency and interpretability of complex machine learning models. Model-agnostic techniques like LIME and model-specific methods like SHAP have gained prominence in providing explanations for AI predictions. The field also explores interpretable deep learning architectures and approaches to make neural networks more transparent. XAI Applications: XAI finds applications across diverse domains. In healthcare, XAI assists in interpreting medical diagnoses and treatment recommendations. In finance, it aids in risk assessment and regulatory compliance. XAI is crucial in autonomous vehicles to explain decision-making processes, contributing to safety and trust. In customer service, it improves chatbot interactions by providing understandable responses. Moreover, XAI has relevance in agriculture, manufacturing, energy efficiency, education, content recommendation, and more. XAI Challenges: Despite its significance, XAI faces several challenges. Balancing model complexity with interpretability remains a fundamental trade-off. Detecting and mitigating bias in AI systems is crucial, especially in sensitive domains. Ensuring ethical considerations, data privacy, and user consent are paramount. Challenges also include providing explanations for high-stakes decisions, addressing the need for human oversight, and adapting to international and cultural norms. In conclusion, XAI plays a pivotal role in making AI systems more transparent, fair, and accountable. As it continues to evolve, it is poised to shape the future of AI by enabling users to understand and trust AI systems, fostering responsible AI development, and addressing ethical and practical challenges in various applications. © 2023 Institute of Advanced Engineering and Science. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Rastogi2022311,
	author = {Rastogi, Dhruv and Thakur, Shubhangi and Singh, Leena},
	title = {Eunoia: A Website for Self-CBT and Psychotherapy},
	year = {2022},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {939},
	pages = {311 – 323},
	doi = {10.1007/978-981-19-4364-5_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144560520&doi=10.1007%2f978-981-19-4364-5_24&partnerID=40&md5=f695972849919f4c931812359cd88ba4},
	abstract = {Cognitive behaviour therapy (CBT) is an effective treatment strategy for a variety of mental and emotional health disorders, such as anxiety and depression. CBT aims to assist you in recognising and challenging harmful ideas as well as learning practical self-help techniques. Internet technology is a viable addition to traditional therapy delivery because of its high penetration, capacity to gather and analyse data, and ability to engage with people. With the various CBT apps and websites, it is discovered that most of them required membership or hiring a therapist online in order to use CBT. They had resources added to them, but most of them lacked an appropriate structure or methodology. The objective of this paper is to achieve privacy, accessibility, and time-saving. In this paper, the online support for the delivery of Self-CBT is provided by a website called “Eunoia,” which gives numerous questionnaires to help patients identify difficult events or conditions in their lives, as well as worksheets and other resources such as chatbot for the patient’s therapy. This website focuses on interpersonal issues, emotional dysregulation, relationship issues, and workplace issues, and it can help in figuring out where the negative thoughts come from and how to utilise the worksheets as homework to work on emotions and thoughts. The questionnaire findings are calculated quite precisely. There is no correct answer in these questionnaires, thus the answer choice involves different points. Users of this Self-CBT website can complete their homework without the burden of lugging papers and worksheets around with them. Eunoia, the Self-CBT website, has yielded unexpected outcomes. Participants in this study went through stage 1 of Self-CBT, where they are required to complete questionnaires that assisted them in recognising their negative thoughts. They are quite pleased with the outcome. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xu2022,
	author = {Xu, Shan and Li, Wenbo},
	title = {A tool or a social being? A dynamic longitudinal investigation of functional use and relational use of AI voice assistants},
	year = {2022},
	journal = {New Media and Society},
	doi = {10.1177/14614448221108112},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134288653&doi=10.1177%2f14614448221108112&partnerID=40&md5=fc64551b8d2cc48489b2e70c009f07f9},
	abstract = {This study integrates two lines of research: technologies as tools and technologies as social beings, under the theoretical framework of dynamic systems, to investigate the reciprocal dynamics between functional use and relational use of artificial intelligence (AI) voice assistants, and the mediating roles of self-disclosure and privacy concerns. A two-wave longitudinal survey was conducted among 354 AI voice assistant users across 2 months. Factor analysis results supported the conceptualization and operationalization of functional use and relational use of voice assistants. Results from the cross-lagged panel model confirmed that functional use and relational use reinforced themselves over time, respectively. Relational use increased subsequent functional use, and relational use reinforced itself through self-disclosure. Surprisingly, functional use did not increase subsequent relational use; instead, longitudinal mediation analysis showed that functional use reduced subsequent relational use due to the lack of self-disclosure. Furthermore, while self-disclosure increased subsequent privacy concerns, privacy concerns did not reduce subsequent self-disclosure. © The Author(s) 2022.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Sayegh-Jodehl2022,
	author = {Sayegh-Jodehl, Sabine and Mukowski-Kickhöfel, Rebecca and Linke, Diane and Müller-Birn, Claudia and Rose, Matthias},
	title = {Use of Instant Messaging Software in a German Hospital—An Exploratory Investigation among Physicians},
	year = {2022},
	journal = {International Journal of Environmental Research and Public Health},
	volume = {19},
	number = {19},
	doi = {10.3390/ijerph191912618},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139976639&doi=10.3390%2fijerph191912618&partnerID=40&md5=4a32a9efe61fc4cad500dd629c94a065},
	abstract = {Internationally, evidence exists that physicians use instant messaging services for communication tasks in everyday clinical practice However, there are only few data on physicians in Germany in this regard. Therefore, at the initiation of our project “DocTalk-Dialog meets Chatbot: Collaborative Learning and Teaching in the Process of Work”, we conducted a stakeholder survey with an exploratory research approach. The aim was to gain initial insights into use of instant messaging software and attitudes towards data security and advantages and disadvantages before implementing a data-secure in-house messaging platform. N = 70 physicians at Charité-Universitätsmedizin Berlin completed an exploratory questionnaire with closed and open-ended questions. Quantitative data were analyzed using descriptive statistics and qualitative data using thematic analysis. The use of messenger software was not widespread in the sample studied. Physicians most frequently used face-to-face contact for communication. On average, up to ten instant messages were exchanged per day, mainly among colleagues, to answer mutual questions, and to send pictures. With a high awareness of privacy-related restrictions among participating physicians, advantages such as fast and uncomplicated communication were also highlighted. An instant messenger solution that complies with the German data protection guidelines is needed and should be investigated in more detail. © 2022 by the authors.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kiuchi2023,
	author = {Kiuchi, Keita and Otsu, Kouyou and Hayashi, Yugo},
	title = {Psychological insights into the research and practice of embodied conversational agents, chatbots and social assistive robots: a systematic meta-review},
	year = {2023},
	journal = {Behaviour and Information Technology},
	doi = {10.1080/0144929X.2023.2286528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178458126&doi=10.1080%2f0144929X.2023.2286528&partnerID=40&md5=f4775b9a3169affc79b02631e35bcfb1},
	abstract = {This study presents a systematic literature search and narrative meta-review of the current state of research on conversational agents (CAs), including embodied CAs, chatbots, and social assistive robots (SARs). The investigation identifies 1,830 academic articles, of which 315 articles satisfied the inclusion criteria for the review. Systematic reviews across various fields are reported, including mental disorders, neurodevelopmental disorders, dementia/cognitive impairment, other medical conditions, elderly support, health promotion, mental health, education, industrial applications, agent characteristics, and robot characteristics. The study highlights challenges in current CA research, such as the scarcity of high-quality comparative studies and the acceptance of CAs by users and caregivers, particularly in elderly support. The article also categorises ethical discussions into nine elements: privacy, safety, innovation, user acceptance, psychological attachment, care philosophy, evaluation, social systems compatibility, and rule development. It also offers insights into the development of future guidelines. The role of CAs in fostering human relationships through their conversational function is emphasised to provide guidance for subsequent CA research and social implementation. As advancements in CA technology and research continue to progress, there is an increasing demand for sophisticated psychological investigations addressing relationships, emotions, and the self. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kopplin2022232,
	author = {Kopplin, Cristopher Siegfried},
	title = {CHATBOTS IN THE WORKPLACE: A TECHNOLOGY ACCEPTANCE STUDY APPLYING USES AND GRATIFICATIONS IN COWORKING SPACES},
	year = {2022},
	journal = {Journal of Organizational Computing and Electronic Commerce},
	volume = {32},
	number = {3-4},
	pages = {232 – 257},
	doi = {10.1080/10919392.2023.2215666},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161562343&doi=10.1080%2f10919392.2023.2215666&partnerID=40&md5=9f04f1cd8875cb1c38046c238b69bc51},
	abstract = {The uses and gratifications approach is used to examine chatbot acceptance in coworking spaces, identifying how coworkers perceive the technology and may use it to facilitate their tasks. To do so, potential influence factors shaping technology acceptance are explored, and a sample of 101 German coworkers is employed to confirm the framework drawing on a quantitative combination of partial least squares structural equation modeling and necessary condition analysis. Instrumental and non-instrumental gratifications, as well as social norm, influence chatbot acceptance in the form of sufficient and necessary conditions, and social norm appears to have a more substantial impact than hedonic factors in terms of sufficiency. However, social norm is not a necessary condition. A moderator analysis reveals that privacy concerns, age and gender do not affect individuals’ intention to use a chatbot. Coworking space providers thus benefit from establishing a standard chatbot solution to leverage social norm, and the chosen solution needs to fulfill hedonic expectations in addition to being useful. Software vendors may also integrate dedicated interfaces to powerful solutions such as ChatGPT. © 2023 Taylor & Francis Group, LLC.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vanezi2023,
	author = {Vanezi, Evangelia and Kallenou, Aliki and Papadopoulos, George A.},
	title = {Saving the Day for Users in Web Platforms: A Chatbot-based Solution for Privacy},
	year = {2023},
	journal = {Proceedings of the 2023 IEEE International Conference on Behavioural and Social Computing, BESC 2023},
	doi = {10.1109/BESC59560.2023.10386087},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184654664&doi=10.1109%2fBESC59560.2023.10386087&partnerID=40&md5=f177513ed4638bc7d646b8f00f861002},
	abstract = {In the rapidly evolving online digital landscape, privacy is an issue of great importance for individuals, while the use of web platforms handling personal data has reached significant levels. Regulations like the European Union General Data Protection Regulation (GDPR) enforce systems to integrate an array of privacy features allowing users to exercise their privacy rights and shield their personal data. However, users tend to find themselves overwhelmed or even ignorant, unsure of how to locate and use them. Research also showed that users find privacy policies hard to read and tend to skip them. This work introduces a novel approach to addressing these privacy concerns by presenting a user-centric solution to enhancing user privacy in web platforms and empowering users in handling their own personal data: An easy-To-use chatbot-based solution. The tool aims to provide a user-friendly and intuitive all-in-one interface, allowing users to easily navigate and access privacy features and manage their personal data while retrieving information about privacy aspects effortlessly from one location. An admin panel enables the customisation of important privacy parameters. We present the design and development process, evaluation and results.  © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Goktas20232697,
	author = {Goktas, Polat and Karakaya, Gul and Kalyoncu, Ali Fuat and Damadoglu, Ebru},
	title = {Artificial Intelligence Chatbots in Allergy and Immunology Practice: Where Have We Been and Where Are We Going?},
	year = {2023},
	journal = {Journal of Allergy and Clinical Immunology: In Practice},
	volume = {11},
	number = {9},
	pages = {2697 – 2700},
	doi = {10.1016/j.jaip.2023.05.042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163359601&doi=10.1016%2fj.jaip.2023.05.042&partnerID=40&md5=fce2e8b92224cdb45e44351d927d1a27},
	abstract = {Artificial intelligence (AI) is rapidly becoming a valuable tool in healthcare, providing clinicians with a new AI lens perspective for patient care, diagnosis, and treatment. This article explores the potential applications, benefits, and challenges of AI chatbots in clinical settings, with a particular emphasis on ChatGPT 4.0 (OpenAI - Chat generative pretrained transformer 4.0), especially in the field of allergy and immunology. AI chatbots have shown considerable promise in various medical domains, including radiology and dermatology, by improving patient engagement, diagnostic accuracy, and personalized treatment plans. ChatGPT 4.0, developed by OpenAI, is good at understanding and replying to prompts in a way that makes sense. However, it is critical to address the potential biases, data privacy issues, ethical considerations, and the need for verification of AI-generated findings. When used responsibly, AI chatbots can significantly enhance clinical practice in allergy and immunology. However, there are still challenges in using this technology that require ongoing research and collaboration between AI developers and medical specialists. To this end, the ChatGPT 4.0 platform has the potential to enhance patient engagement, improve diagnostic accuracy, and provide personalized treatment plans in allergy and immunology practice. However, limitations and risks must be addressed to ensure their safe and effective use in clinical practice. © 2023 The Authors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Tran202263,
	author = {Tran, Lan Anh and Hensen, Benedikt and Klamma, Ralf and Chantaraskul, Soamsiri},
	title = {Privacy and Security in Mixed Reality Learning Environments by Input and User/Bot Interaction Protection},
	year = {2022},
	journal = {ACM International Conference Proceeding Series},
	pages = {63 – 71},
	doi = {10.1145/3512353.3512363},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126631945&doi=10.1145%2f3512353.3512363&partnerID=40&md5=b0ac966a83a3ccf6fa6880e3ee6b615b},
	abstract = {Mixed reality is known as an advanced technology that provides a new approach for learning environments. Such environments allow learners to interact with both virtual and real worlds and bringing in potential enhancements to the learning process at the same time. For example, chatbots can facilitate the learning process. However, security and privacy settings for interacting with chatbots in such mixed reality environments are complex. In this paper, we introduce a mixed reality virtual assistant that is integrated into the collaborative environment of our existing application VIAProMa. This embodied chatbot allows lecturers and students to participate in mixed reality and online classrooms in real-time. The participants can interact with each other via VIAProMa's avatar representations and can communicate with the chatbot that is represented by the mixed reality bot. The bot is realized by connecting a Slack chatbot with the mixed reality learning environment. It is displayed as an intuitive 3D model and is able to communicate with the users in spoken language. In this environment, privacy and security settings are conducted to protect the user input and user interaction with the bot. The evaluation results show that the system works stably with good performance. All the visualizations and features are well designed and were understood by the users. Users preferred the speech interface with the bot over a textual interface. The research has a strong impact on the design of security and privacy features for mixed reality environments in general.  © 2022 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Gallent-Iglesias202387,
	author = {Gallent-Iglesias, Dana and Serantes-Raposo, Santiago and López-Riobóo-Botana, Iñigo and Gonzalez-Vázquez, Sonia and Fernandez-Graña, Pablo Manuel},
	title = {IVAMED: Intelligent Virtual Assistant for Medical Diagnosis},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3516},
	pages = {87 – 92},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176449982&partnerID=40&md5=f13871392a1efa004ad463ee109f239f},
	abstract = {The recent advancements in deep learning have led to a myriad of approaches for medical diagnosis and assistance. Some topics such as data labelling, data curation, human-in-the-loop, explainability or privacy-preserving methodologies are hot topics for applied machine learning in the healthcare context. In this domain, we normally expect a three-way interaction (doctor-system-patient), so that interfaces play a crucial role. Remotely managed VR (Virtual Reality) systems help us to enhance communication and feedback between doctors and patients in situations where in-person assistance is not feasible. Moreover, the recent breakthroughs with LLMs (Large Language Models) enable us to use natural language as additional interface, considering NLU (Natural Language Understanding) for intent recognition, ASR (Automatic Speech Recognition) and TTS (Text-To-Speech). In the context of the CEL.IA network, we present IVAMED (Intelligent Virtual Assistant for MEdical Diagnosis), a chatbot-oriented application in a VR environment for remote medical assistance. We tackle the situation in which face-to-face assistance is not possible. We provide the tools for remote interaction and guided diagnosis. We propose the evaluation of the MoCA (Montreal Cognitive Assessment) test for early detection of MCI (Mild Cognitive Impairment) and the BDI (Beck Depression Inventory) test for measuring characteristic attitudes and symptoms of depression.  © 2023 Copyright for this paper by its authors.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Al-Shafei2024,
	author = {Al-Shafei, Mohamed},
	title = {Navigating Human-Chatbot Interactions: An Investigation into Factors Influencing User Satisfaction and Engagement},
	year = {2024},
	journal = {International Journal of Human-Computer Interaction},
	doi = {10.1080/10447318.2023.2301252},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182192946&doi=10.1080%2f10447318.2023.2301252&partnerID=40&md5=2118881e2e3d5634f7d449a663978fe2},
	abstract = {With the increasing integration of chatbots in various customer service contexts, understanding factors that influence user interactions is of paramount importance. While chatbots bring several benefits, their efficacy largely depends on user satisfaction, loyalty, and perceived utility. This study explores the nuances of consumer interactions with chatbot services and identifies ways to enhance these interactions for successful goal achievement. Through a multiple qualitative method approach, involving reflections, interviews, and focus groups, conducted an exhaustive investigation of the consumer experiences with chatbot interactions. The study focused on elements such as anthropomorphism, communication styles, service scripts, consumer emotions, and privacy and trust concerns. Our findings highlight that the anthropomorphism and communication style of chatbots significantly affect user satisfaction. The perceived utility of chatbots was found to be a multifaceted construct that substantially influences consumer engagement. Furthermore, this study identified negative emotions and privacy concerns as key determinants of consumer-chatbot interactions. These findings necessitate meticulous attention in chatbot design and function. By using the affordance theory, this research offers insights into managing the complex dynamics of satisfaction and dissatisfaction factors within consumer-chatbot engagements, contributing crucially to the human-computer interaction literature. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Briganti20241565,
	author = {Briganti, Giovanni},
	title = {How ChatGPT works: a mini review},
	year = {2024},
	journal = {European Archives of Oto-Rhino-Laryngology},
	volume = {281},
	number = {3},
	pages = {1565 – 1569},
	doi = {10.1007/s00405-023-08337-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177560311&doi=10.1007%2fs00405-023-08337-7&partnerID=40&md5=82feeca8034a7ca063ec6a7b8fe4cf33},
	abstract = {Objective: This paper offers a mini-review of OpenAI's language model, ChatGPT, detailing its mechanisms, applications in healthcare, and comparisons with other large language models (LLMs). Methods: The underlying technology of ChatGPT is outlined, focusing on its neural network architecture, training process, and the role of key elements such as input embedding, encoder, decoder, attention mechanism, and output projection. The advancements in GPT-4, including its capacity for internet connection and the integration of plugins for enhanced functionality are discussed. Results: ChatGPT can generate creative, coherent, and contextually relevant sentences, making it a valuable tool in healthcare for patient engagement, medical education, and clinical decision support. Yet, like other LLMs, it has limitations, including a lack of common sense knowledge, a propensity for hallucination of facts, a restricted context window, and potential privacy concerns. Conclusion: Despite the limitations, LLMs like ChatGPT offer transformative possibilities for healthcare. With ongoing research in model interpretability, common-sense reasoning, and handling of longer context windows, their potential is vast. It is crucial for healthcare professionals to remain informed about these technologies and consider their ethical integration into practice. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2023.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xu20231113,
	author = {Xu, XiaoShu and Wang, Xibing and Zhang, Yunfeng and Ma, Wenjuan},
	title = {Can ChatGPT Facilitate the Implementation of Personal Learning Environments in Tertiary Education: Benefits and Risks},
	year = {2023},
	journal = {Proceedings of International Conference on Research in Education and Science},
	volume = {9},
	number = {1},
	pages = {1113 – 1123},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184343527&partnerID=40&md5=e2b0d059dfb5a60e2b798c92886b0f12},
	abstract = {The integration of ChatGPT in Personal Learning Environments (PLEs) has emerged as a promising approach to personalized learning in tertiary education. ChatGPT is believed to have the potential to transform traditional higher education into a more personalized, quality-driven, and student-centered learning experience that fosters critical thinking, self-regulated learning, and creativity. While recent studies have highlighted the potential benefits of ChatGPT in enhancing personalized learning experiences, there are several risks and challenges that need to be addressed. This paper reviews relevant literature on ChatGPT and PLEs and identifies key risks and challenges associated with their integration, including ethical concerns, data privacy, technical issues, and user acceptance. Meanwhile, the paper also proposes ways and thoughts for the future implementation of ChatGPT in PLEs. The paper concludes that ChatGPT has significant potential to facilitate a new round of educational revolution which pushes educators to reconsider why to teach, how to teach, and what to teach. © 2023 Published by the ISTES Organization},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lappeman2023337,
	author = {Lappeman, James and Marlie, Siddeeqah and Johnson, Tamryn and Poggenpoel, Sloane},
	title = {Trust and digital privacy: willingness to disclose personal information to banking chatbot services},
	year = {2023},
	journal = {Journal of Financial Services Marketing},
	volume = {28},
	number = {2},
	pages = {337 – 357},
	doi = {10.1057/s41264-022-00154-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128828206&doi=10.1057%2fs41264-022-00154-z&partnerID=40&md5=81143273edf753cda01ec6b7d8cb306e},
	abstract = {This study explored digital privacy concerns in the use of chatbots as a digital banking service. Three dimensions of trust were tested in relation to user self-disclosure in order to better understand the consumer-chatbot experience in banking. The methodology selected for this research study followed a conclusive, pre-experimental, two-group one-shot case study research design which made use of a non-probability snowballing sampling technique. Privacy concerns were found to have a significantly negative relationship with user self-disclosure in both treatment groups. Respondents exposed to their preferred banking brand experienced lower user self-disclosure and brand trust than those exposed to a fictitious banking brand within the South African context. It is recommended that companies using chatbots focus on easing privacy concerns and build foundations of trust. The gains that chatbots have made in the form of increased productivity and quality of customer service rely on relationships with users who need to disclose personal information. Through this study, we concluded that, despite its power to influence decision-making, the power of a brand is not enough for consumers to considerably increase self-disclosure. Rather, a bridge of trust (through education, communication and product development) is needed that encompasses all three elements of trust, which are brand trust, cognitive trust and emotional trust. Limited research exists on the relationship between financial services marketing and chatbot adoption. Thus, this study addressed a theoretical gap, by adding brand trust to existing studies on cognitive and emotional trust regarding user self-disclosure. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Hendrickx2021373,
	author = {Hendrickx, Iris and Cena, Federica and Basar, Erkan and Di Caro, Luigi and Kunneman, Florian and Musi, Elena and Musto, Cataldo and Rapp, Amon and Van Waterschoot, Jelte},
	title = {Towards a New generation of Personalized Intelligent Conversational Agents},
	year = {2021},
	journal = {UMAP 2021 - Adjunct Publication of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
	pages = {373 – 374},
	doi = {10.1145/3450614.3461453},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109218496&doi=10.1145%2f3450614.3461453&partnerID=40&md5=fa1895e100de96eb6ceb986363525c7c},
	abstract = {The Personalized Intelligent Conversational Agents workshop focuses on both long-term engaging spoken dialogue systems and text-based chatbots, as well as conversational recommender systems. The goal of the workshop is to stimulate discussion around problems, challenges, possible solutions and research directions regarding the exploitation of natural language processing and machine learning techniques to learn user features and to use them to personalize the dialogue in the next generation of intelligent conversational agents.  © 2021 Owner/Author.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Wickramarathna202230,
	author = {Wickramarathna, Maleesha and De Silva, Kithmini and Lekamalage, Vihanga and Senanayake, Janith and Perera, Jeewaka and Ruggahakotuwa, Laneesha},
	title = {Oxygen: A Distributed Health Care Framework for Patient Health Record Management and Pharmaceutical Diagnosis},
	year = {2022},
	journal = {4th International Conference on Advancements in Computing, ICAC 2022 - Proceeding},
	pages = {30 – 35},
	doi = {10.1109/ICAC57685.2022.10025250},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148598258&doi=10.1109%2fICAC57685.2022.10025250&partnerID=40&md5=67fb84dc6e56ee09a4d23409c86fde6f},
	abstract = {With the COVID-19 pandemic, the world is confronting various healthcare issues, and healthcare automation is more crucial than ever. The pandemic has revealed the limitations of existing digital healthcare systems to manage public health emergencies. There is no registered population for many healthcare institutions in Sri Lanka, as a result, there is a communication gap. Electronic Health Record systems (EHRs) are becoming popular to share patient details but accessing scattered data across several EHRs while safeguarding patient privacy remains a challenge. Most of these medical records are in printed format and manually entering those into EHR systems is time-consuming and error prone. Not only that pharmaceutical error is a critical healthcare problem, but it is even riskier to visit doctors for pharmaceutical diagnosis during a pandemic. This research introduces a Blockchain-based patient health record system, an Optical Character Recognition (OCR) and Natural Language Processing (NLP) based Medical Document Scanner, a Drug Identifier based on Image Processing and a Medical Chatbot powered by NLP as four novel approaches to address these issues. Altogether with the results, this research aims at introducing a solution for the limitations in healthcare while providing a distributed healthcare framework for the healthcare community worldwide. © 2022 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jose20221891,
	author = {Jose, Christin and Wang, Joseph and Strimel, Grant P. and Khursheed, Mohammad Omar and Mishchenko, Yuriy and Kulis, Brian},
	title = {Latency Control for Keyword Spotting},
	year = {2022},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	volume = {2022-September},
	pages = {1891 – 1895},
	doi = {10.21437/Interspeech.2022-10608},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140059692&doi=10.21437%2fInterspeech.2022-10608&partnerID=40&md5=a307fb881cb744301d53b1be9edac0f4},
	abstract = {Conversational agents commonly utilize keyword spotting (KWS) to initiate voice interaction with the user. For user experience and privacy considerations, existing approaches to KWS largely focus on accuracy, which can often come at the expense of introduced latency. To address this tradeoff, we propose a novel approach to control KWS model latency and which generalizes to any loss function without explicit knowledge of the keyword endpoint. Through a single, tunable hyperparameter, our approach enables one to balance detection latency and accuracy for the targeted application. Empirically, we show that our approach gives superior performance under latency constraints when compared to existing methods. Namely, we make a substantial 25% relative false accepts improvement for a fixed latency target when compared to the baseline state-of-the-art. We also show that when our approach is used in conjunction with a max-pooling loss, we are able to improve relative false accepts by 25% at a fixed latency when compared to cross entropy loss. Copyright © 2022 ISCA.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sarker2023,
	author = {Sarker, Arpita and Jesser, Alexander and Speidel, Markus},
	title = {Advancing Decentralized IoT with Privacy-preserving AI: Harnessing Federated Learning and NLP Techniques},
	year = {2023},
	journal = {2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things, AIBThings 2023 - Proceedings},
	doi = {10.1109/AIBThings58340.2023.10292448},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178520224&doi=10.1109%2fAIBThings58340.2023.10292448&partnerID=40&md5=ee23642fa5d4aa101304c7a9dabaeef0},
	abstract = {This study introduces a cross-platform application built on the Flutter framework that employs federated learning (FL) and natural language processing (NLP) for personalized event discovery. The system includes an NLP-based chatbot and utilizes a second-generation Matrix homeserver known as Dendrite for decentralized communication. The system design is configured to ensure data privacy. The FL model runs on users' devices, employing data such as browsing history and application usage patterns to build user-interest profiles and provide personalized event suggestions. A practical use case underscores the implementation of end-To-end encrypted communication, indicating the system's commitment to ensuring privacy and security. The integration of FL and NLP into an IoT context demonstrates a significant advancement in privacy-preserving, personalized applications.  © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Matulis20232808,
	author = {Matulis, John and McCoy, Rozalina},
	title = {Relief in Sight? Chatbots, In-baskets, and the Overwhelmed Primary Care Clinician},
	year = {2023},
	journal = {Journal of General Internal Medicine},
	volume = {38},
	number = {12},
	pages = {2808 – 2815},
	doi = {10.1007/s11606-023-08271-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163659754&doi=10.1007%2fs11606-023-08271-8&partnerID=40&md5=446c992048766a3197cd81dee5ae7580},
	abstract = {The recent emergence of publically facing artificial intelligence (AI) chatbots has generated vigorous discussion in the lay public around the possibilities, liabilities, and uncertainties of the integration of such technology into everyday life. As primary care clinicians continue to struggle against ever-increasing loads of asynchronous, electronic work, the potential for AI to improve the quality and efficiency of this work looms large. In this essay, we discuss the basic premise of open-access AI chatbots such as CHATGPT, review prior applications of AI in healthcare, and preview some possible AI chatbot–assisted in-basket assistance including scenarios of communicating test results with patients, providing patient education, and clinical decision support in history taking, review of prior diagnostic test characteristics, and common management scenarios. We discuss important concerns related to the future adoption of this technology including the transparency of the training data used in developing these models, the level of oversight and trustworthiness of the information generated, and possible impacts on equity, bias, and patient privacy. A stepwise and balanced approach to simultaneously understand the capabilities and address the concerns associated with these tools will be needed before these tools can improve patient care. © 2023, The Author(s), under exclusive licence to Society of General Internal Medicine.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Guida2021,
	author = {Guida, Sergio},
	title = {Privacy policies between perception and learning through Legal Design: Ideas for an Educational Chatbot combining rights'awareness, optimized user experience and training efficacy},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {3100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126392292&partnerID=40&md5=142fa04cbd4041cb5d8437e0df13b409},
	abstract = {Legal Design is inspired by the concepts of Design Thinking and User Experience: the ultimate goal is to make citizens more aware with well-designed rules and procedures in terms of 'Proactive law', providing policies that are truly user-centered, tailored to the cognitive needs both expressed and hidden through continuous and transparent communication. Talking about privacy, there is a clear need, starting from the analysis of the texts of some documents (e.g.'information on the use of personal data' and 'consent'), to translate 'bureaucratic' requests into simpler questions, as to allow the more intuitive, usable and inclusive user experience. So, in the wake of the GDPR view too, we have been experimenting how the advantages of communicating and involving the citizens by design and by default can prove to be the right key so that the privacy documents they encounter in the everyday life can finally transmit transparency and a sense of control of the situation that are both much more substantial and immediately perceptible. The future developments of the project will be focused on enhancing access to safeguards for digital privacy, as well as citizens' awareness of their digital rights, a fortiori for the weakest subjects, with a chatbot providing personalized educational/assistance services. In the final stage I will be concepting a prototype of such an educational chatbot ranging between rights to be deployed, user experience to be exalted and training effectiveness to be ensured. © 2021 Copyright for this paper by its authors.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Talib2023173,
	author = {Talib, Abdelmoumen and Housni, Mohamed and Radid, Mohamed},
	title = {Utilizing M-Technologies for AI-Driven Career Guidance in Morocco: An Innovative Mobile Approach},
	year = {2023},
	journal = {International Journal of Interactive Mobile Technologies},
	volume = {17},
	number = {24},
	pages = {173 – 188},
	doi = {10.3991/IJIM.V17I24.44263},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183846791&doi=10.3991%2fIJIM.V17I24.44263&partnerID=40&md5=ab34a9311e97cb6d3ca936cda1499390},
	abstract = {In today's interconnected world, the significance of effective career guidance has been magnified. With the advent of mobile technologies, e-orientation and artificial intelligence (AI)-orientation systems offer a promising avenue for personalized career guidance. This paper delves into the potential of transitioning from traditional e-orientation to advanced AI-orientation systems in Morocco by employing large language models (LLMs) such as LLAMA2, GPT, and PaLM. These LLMs, renowned for their human-like text generation and contextual understanding, are proposed as the backbone for AI chatbots that can serve as virtual career counselors. Accessible via mobile platforms, these mobile chatbot interfaces can provide real-time insights on career paths, educational prerequisites, and job market dynamics and outlooks. Despite challenges such as Internet reliability, data privacy, and legislative regulations, the integration of AI-orientated systems into mobile platforms can revolutionize career guidance for Moroccan students. This paper presents a detailed roadmap and implementation for embedding these innovative technologies into Morocco's educational framework. © (2023) by the authors of this article. Published under CC-BY.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Moore2024,
	author = {Moore, Richard and Al-Tamimi, Abdel-Karim and Freeman, Elizabeth},
	title = {Investigating the Potential of a Conversational Agent (Phyllis) to Support Adolescent Health and Overcome Barriers to Physical Activity: Co-Design Study},
	year = {2024},
	journal = {JMIR Formative Research},
	volume = {8},
	number = {1},
	doi = {10.2196/51571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184870579&doi=10.2196%2f51571&partnerID=40&md5=2bc060c5e8d0c282dd2ed8119507c29c},
	abstract = {Background: Conversational agents (CAs) are a promising solution to support people in improving physical activity (PA) behaviors. However, there is a lack of CAs targeted at adolescents that aim to provide support to overcome barriers to PA. This study reports the results of the co-design, development, and evaluation of a prototype CA called “Phyllis” to support adolescents in overcoming barriers to PA with the aim of improving PA behaviors. The study presents one of the first theory-driven CAs that use existing research, a theoretical framework, and a behavior change model. Objective: The aim of the study is to use a mixed methods approach to investigate the potential of a CA to support adolescents in overcoming barriers to PA and enhance their confidence and motivation to engage in PA. Methods: The methodology involved co-designing with 8 adolescents to create a relational and persuasive CA with a suitable persona and dialogue. The CA was evaluated to determine its acceptability, usability, and effectiveness, with 46 adolescents participating in the study via a web-based survey. Results: The co-design participants were students aged 11 to 13 years, with a sex distribution of 56% (5/9) female and 44% (4/9) male, representing diverse ethnic backgrounds. Participants reported 37 specific barriers to PA, and the most common barriers included a “lack of confidence,” “fear of failure,” and a “lack of motivation.” The CA’s persona, named “Phyllis,” was co-designed with input from the students, reflecting their preferences for a friendly, understanding, and intelligent personality. Users engaged in 61 conversations with Phyllis and reported a positive user experience, and 73% of them expressed a definite intention to use the fully functional CA in the future, with a net promoter score indicating a high likelihood of recommendation. Phyllis also performed well, being able to recognize a range of different barriers to PA. The CA’s persuasive capacity was evaluated in modules focusing on confidence and motivation, with a significant increase in students’ agreement in feeling confident and motivated to engage in PA after interacting with Phyllis. Adolescents also expect to have a personalized experience and be able to personalize all aspects of the CA. Conclusions: The results showed high acceptability and a positive user experience, indicating the CA’s potential. Promising outcomes were observed, with increasing confidence and motivation for PA. Further research and development are needed to create further interventions to address other barriers to PA and assess long-term behavior change. Addressing concerns regarding bias and privacy is crucial for achieving acceptability in the future. The CA’s potential extends to health care systems and multimodal support, providing valuable insights for designing digital health interventions including tackling global inactivity issues among adolescents. © 2024 JMIR Publications Inc.. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus}
}

@ARTICLE{Hasal2021,
	author = {Hasal, Martin and Nowaková, Jana and Ahmed Saghair, Khalifa and Abdulla, Hussam and Snášel, Václav and Ogiela, Lidia},
	title = {Chatbots: Security, privacy, data protection, and social aspects},
	year = {2021},
	journal = {Concurrency and Computation: Practice and Experience},
	volume = {33},
	number = {19},
	doi = {10.1002/cpe.6426},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107027640&doi=10.1002%2fcpe.6426&partnerID=40&md5=37385e4313531804f13c98d4d46cea2a},
	abstract = {Chatbots are artificial communication systems becoming increasingly popular and not all their security questions are clearly solved. People use chatbots for assistance in shopping, bank communication, meal delivery, healthcare, cars, and many other actions. However, it brings an additional security risk and creates serious security challenges which have to be handled. Understanding the underlying problems requires defining the crucial steps in the techniques used to design chatbots related to security. There are many factors increasing security threats and vulnerabilities. All of them are comprehensively studied, and security practices to decrease security weaknesses are presented. Modern chatbots are no longer rule-based models, but they employ modern natural language and machine learning techniques. Such techniques learn from a conversation, which can contain personal information. The paper discusses circumstances under which such data can be used and how chatbots treat them. Many chatbots operate on a social/messaging platform, which has their terms and conditions about data. The paper aims to present a comprehensive study of security aspects in communication with chatbots. The article could open a discussion and highlight the problems of data storage and usage obtained from the communication user—chatbot and propose some standards to protect the user. © 2021 The Authors. Concurrency and Computation: Practice and Experience published by John Wiley & Sons Ltd.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 47}
}

@ARTICLE{Kazim2021,
	author = {Kazim, Emre and Koshiyama, Adriano Soares and Hilliard, Airlie and Polle, Roseline},
	title = {Systematizing audit in algorithmic recruitment},
	year = {2021},
	journal = {Journal of Intelligence},
	volume = {9},
	number = {3},
	doi = {10.3390/jintelligence9030046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115781934&doi=10.3390%2fjintelligence9030046&partnerID=40&md5=6e5fff246bbd335a925680dc300655c5},
	abstract = {Business psychologists study and assess relevant individual differences, such as intelligence and personality, in the context of work. Such studies have informed the development of artificial intelligence systems (AI) designed to measure individual differences. This has been capitalized on by companies who have developed AI-driven recruitment solutions that include aggregation of appropriate candidates (Hiretual), interviewing through a chatbot (Paradox), video interview assessment (MyInterview), and CV-analysis (Textio), as well as estimation of psychometric characteristics through image-(Traitify) and game-based assessments (HireVue) and video interviews (Cammio). However, driven by concern that such high-impact technology must be used responsibly due to the potential for unfair hiring to result from the algorithms used by these tools, there is an active effort towards proving mechanisms of governance for such automation. In this article, we apply a systematic algorithm audit framework in the context of the ethically critical industry of algorithmic recruitment systems, exploring how audit assessments on AI-driven systems can be used to assure that such systems are being responsibly deployed in a fair and well-governed manner. We outline sources of risk for the use of algorithmic hiring tools, suggest the most appropriate opportunities for audits to take place, recommend ways to measure bias in algorithms, and discuss the transparency of algorithms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Cai2023283,
	author = {Cai, Ziyan and Chang, Xin and Li, Ping},
	title = {HCPP: A Data-Oriented Framework to Preserve Privacy during Interactions with Healthcare Chatbot},
	year = {2023},
	journal = {Conference Proceedings of the IEEE International Performance, Computing, and Communications Conference},
	pages = {283 – 290},
	doi = {10.1109/IPCCC59175.2023.10253855},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182926549&doi=10.1109%2fIPCCC59175.2023.10253855&partnerID=40&md5=4ce8d326b852a2f7c3673fa77e7f241e},
	abstract = {Healthcare chatbots are becoming increasingly popular, but with their use comes the issues of excessive personal information collection and privacy leakage. To address this issue, we propose a Healthcare Chatbot-based Privacy Preserving (HCPP) Framework that adopts a data-oriented approach to reduce the excessive disclosure of personal information. HCPP consists of two main components: the Healthcare Chatbot-based Minimized Personal Information (HCMPI) method and the Healthcare Chatbot-based Zero Knowledge Proof (HCZKP) method. HCMPI leverages large language models (LLMs) to minimize the acquisition of unnecessary personal health information without significantly affecting healthcare service. HCZKP further encrypts a part of the minimized information, making the data available but invisible. The experimental evaluation results demonstrate the effectiveness and feasibility of our approach. © 2023 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Seckelmann20231,
	author = {Seckelmann, Margrit},
	title = {Artificial intelligence in administration: The draft of a European AI Regulation and the handling of information technology risks; [KüNSTLICHE INTELLIGENZ IN DER VERWALTUNG Der Entwurf einer europäischen KI-Verordnung und der Umgang mit informationstechnischen Risiken]},
	year = {2023},
	journal = {Verwaltung},
	volume = {56},
	number = {1},
	pages = {1 – 29},
	doi = {10.3790/verw.56.1.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173910054&doi=10.3790%2fverw.56.1.1&partnerID=40&md5=c7b18d8d6434889dcd288cb670ea6e43},
	abstract = {Applications of artificially intelligent systems have recently enjoyed increasing popularity, as demonstrated in particular by the presentation of the chatbot "ChatGPT" by the AI research company OpenAI. ChatGPT also demonstrates the potential of AI more generally: Such applications can already be used by private individuals, for example to get support in writing exams or theses, and it seems likely that one day similar AI systems, such as chatbots, could also be used in public administration. This does, nevertheless raise some questions in the public administrative sector: predictive decisions, involving the need for careful risk evaluation, are not new to German administrative law, as is apparent from fields like atomic energy law. Still, it is questionable how far the administration should entrust such procedures of probabilistic risk analysis to AI applications and to what extent this could be legally acceptable? The analysis in this paper is devoted to answering this question. Part II begins by clarifying some of the key concepts underpinning the notion of artificial intelligence, including 'digitalisation', 'algorithms', 'big data analytics' and 'machine learning'. The different (supervised and unsupervised) methods of machine learning are considered, as are their modes of operation and selected problems they raise, such as the possibility of bias and lack of transparency and comprehensibility. Part III then examines in detail the European Commission's draft regulation on AI (Proposal for a Regulation of the European Parliament and of the Council laying down harmonised Rules on Artificial Intelligence (Artificial Intelligence Act), COM(2021) 206 final, 2021/0106 (COD)). This legislative initiative pursues in particular the goal of regulating AI applications and harmonising the regulations on them across the Union. The regulatory scope as well as the area of application and the contents of the draft AI regulation are first shown and possible deficits are discussed. A problematic aspect is the systematic relationship of the draft regulation to other existing EU regulatory acts, such as the General Data Protection Regulation and its Art. 22 GDPR. Following on from the discussion of these problematic aspects, Part IV presents the current draft status of the AI Regulation as well as analysing the (critical) reaction, especially of the Federal Republic of Germany, to the same. PartVconcludes by assessing the implications of the above for use of AI systems in the German public administration, and provides a final evaluation of the draft AI regulation in the latter context. A key issue remains how far it is acceptable for AI-predictions, based on the behavior of a class of persons, to be applied to individual citizens. © 2023 Duncker und Humblot GmbH. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dwivedi2023,
	author = {Dwivedi, Yogesh K. and Kshetri, Nir and Hughes, Laurie and Slade, Emma Louise and Jeyaraj, Anand and Kar, Arpan Kumar and Baabdullah, Abdullah M. and Koohang, Alex and Raghavan, Vishnupriya and Ahuja, Manju and Albanna, Hanaa and Albashrawi, Mousa Ahmad and Al-Busaidi, Adil S. and Balakrishnan, Janarthanan and Barlette, Yves and Basu, Sriparna and Bose, Indranil and Brooks, Laurence and Buhalis, Dimitrios and Carter, Lemuria and Chowdhury, Soumyadeb and Crick, Tom and Cunningham, Scott W. and Davies, Gareth H. and Davison, Robert M. and Dé, Rahul and Dennehy, Denis and Duan, Yanqing and Dubey, Rameshwar and Dwivedi, Rohita and Edwards, John S. and Flavián, Carlos and Gauld, Robin and Grover, Varun and Hu, Mei-Chih and Janssen, Marijn and Jones, Paul and Junglas, Iris and Khorana, Sangeeta and Kraus, Sascha and Larsen, Kai R. and Latreille, Paul and Laumer, Sven and Malik, F. Tegwen and Mardani, Abbas and Mariani, Marcello and Mithas, Sunil and Mogaji, Emmanuel and Nord, Jeretta Horn and O'Connor, Siobhan and Okumus, Fevzi and Pagani, Margherita and Pandey, Neeraj and Papagiannidis, Savvas and Pappas, Ilias O. and Pathak, Nishith and Pries-Heje, Jan and Raman, Ramakrishnan and Rana, Nripendra P. and Rehm, Sven-Volker and Ribeiro-Navarrete, Samuel and Richter, Alexander and Rowe, Frantz and Sarker, Suprateek and Stahl, Bernd Carsten and Tiwari, Manoj Kumar and van der Aalst, Wil and Venkatesh, Viswanath and Viglia, Giampaolo and Wade, Michael and Walton, Paul and Wirtz, Jochen and Wright, Ryan},
	title = {“So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
	year = {2023},
	journal = {International Journal of Information Management},
	volume = {71},
	doi = {10.1016/j.ijinfomgt.2023.102642},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149886538&doi=10.1016%2fj.ijinfomgt.2023.102642&partnerID=40&md5=07a92f9d615bd212010a31ca0fe70bec},
	abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT's capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT's use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts. © 2023 The Authors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 482}
}

@ARTICLE{Adam2023,
	author = {Adam, Martin and Benlian, Alexander},
	title = {From web forms to chatbots: The roles of consistency and reciprocity for user information disclosure},
	year = {2023},
	journal = {Information Systems Journal},
	doi = {10.1111/isj.12490},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178177566&doi=10.1111%2fisj.12490&partnerID=40&md5=e1abcec22f828125b1e4dee85a672c70},
	abstract = {Interactive decision aids (IDAs) on websites often require users to disclose relevant information (e.g., preferences, contact information) to help users in making decisions (e.g., product choice). With technological advances in IDAs, websites increasingly switch from static, non-conversational IDAs (e.g., web forms) to conversational ones (e.g., chatbots) to boost user information disclosure that nurtures the websites' economic viability. While this novel form of IDAs is already widely employed in practice, information systems research has yet to examine the defining dialogue design features of conversational IDAs and their effects on eliciting user information. Drawing on persuasion theory and particularly on consistency and reciprocity as influence techniques, we develop a research model around two crucial dialogue design features of conversational IDAs. Specifically, we investigate the distinct and joint effects of conversational style (i.e., absence vs. presence of a conversational presentation of requests) and reciprocation triggers (i.e., absence vs. presence of reciprocity-inducing information) on user information disclosure (i.e., email addresses). By combining the complementary properties of a randomised field experiment (N = 386) and a follow-up online experiment (N = 182), we empirically provide evidence in support of the distinct and joint effects of conversational style and reciprocation triggers of IDAs on user information disclosure. Moreover, we demonstrate that these dialogue design features have indirect effects on information disclosure via perceptions of social presence and privacy concerns. Thus, our paper provides theoretical and practical insights into whether, how, and why critical IDA dialogue design features can better elicit user information for website services. © 2023 The Authors. Information Systems Journal published by John Wiley & Sons Ltd.},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Semnani20232387,
	author = {Semnani, Sina J. and Yao, Violet Z. and Zhang, Heidi C. and Lam, Monica S.},
	title = {WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia},
	year = {2023},
	journal = {Findings of the Association for Computational Linguistics: EMNLP 2023},
	pages = {2387 – 2413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178631676&partnerID=40&md5=fd9d856f7fc450df8af8af07e5804e77},
	abstract = {This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment. Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM. WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments. © 2023 Association for Computational Linguistics.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Song2022,
	author = {Song, Mengmeng and Du, Jingzhe and Xing, Xinyu and Mou, Jian},
	title = {Should the chatbot “save itself” or “be helped by others”? The influence of service recovery types on consumer perceptions of recovery satisfaction},
	year = {2022},
	journal = {Electronic Commerce Research and Applications},
	volume = {55},
	doi = {10.1016/j.elerap.2022.101199},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138060925&doi=10.1016%2fj.elerap.2022.101199&partnerID=40&md5=2bdc113abb59c10118799e383b809033},
	abstract = {The application of artificial intelligence is considered essential to adapt to a new cycle of industrial transformation and technological advancements in many fields and industries. The extensive use of artificial intelligence technology is expected to improve the level and quality of services provided by companies adopting these methods. In this study, we propose a novel approach to self-recovery by chatbot systems after service failures based on social response theory. Moreover, we explore differences in consumer perceptions of different service recovery types and their impact on recovery satisfaction, and discusses whether the intelligence of the computational agent also has an effect. We present the results of three scenario-based experiments, which demonstrate the positive effect of chatbot self-recovery on consumer satisfaction, and show the mediating paths of service recovery types in terms of perceived functional value and privacy risks as well as the boundary condition of the level of robot intelligence. This work expands the range of applications of chatbots in the service industry and provides a new framework for the governance of artificial intelligence. © 2022 Elsevier B.V.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}@CONFERENCE{Kumar20201627,
	author = {Kumar, Jeya Amantha and Silva, Paula Alexandra},
	title = {Work-in-progress: A preliminary study on students' acceptance of chatbots for studio-based learning},
	year = {2020},
	journal = {IEEE Global Engineering Education Conference, EDUCON},
	volume = {2020-April},
	pages = {1627 – 1631},
	doi = {10.1109/EDUCON45650.2020.9125183},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087877672&doi=10.1109%2fEDUCON45650.2020.9125183&partnerID=40&md5=8a984b08dd1cfc21c36d79fc09df5fa4},
	abstract = {Studio-based learning (SBL) is a pedagogy that encompasses collaborative active learning strategies focusing on creativity, peer learning, and problem-solving. The iterative design procedures and critique sessions are the primary learning practices of SBL, which relays to a need to have effective communication between instructors and students. Due to this, we designed a Telegram chatbot using the TextIt to facilitate some of the interactions that are common in SBL. In this preliminary study, students' acceptance of chatbot was investigated based on the Technology Acceptance Model (TAM) using a mix-method approach. Preliminary results indicated positive acceptance and intention to use chatbots due to ease of use, mobile accessibility, human-like communications, and privacy in communicating and providing feedback. © 2020 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Bosse2015650,
	author = {Bosse, Tibor and Provoost, Simon},
	title = {Integrating conversation trees and cognitive models within an ECA for aggression de-escalation training},
	year = {2015},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9387},
	pages = {650 – 659},
	doi = {10.1007/978-3-319-25524-8_48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950336129&doi=10.1007%2f978-3-319-25524-8_48&partnerID=40&md5=e289e5b528b5a1db01513dffee4ac153},
	abstract = {Traditionally, Embodied Conversational Agents communicate with humans using dialogue systems based on conversation trees. To enhance the flexibility and variability of dialogues, this paper proposes an approach to integrate conversation trees with cognitive models. The approach is illustrated by a case study in the domain of aggression de-escalation training, and a preliminary evaluation in the context of a practical application is presented. © Springer International Publishing Switzerland 2015.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@CONFERENCE{Luria2020,
	author = {Luria, Michal and Zheng, Rebecca and Huffman, Bennett and Huang, Shuangni and Zimmerman, John and Forlizzi, Jodi},
	title = {Social Boundaries for Personal Agents in the Interpersonal Space of the Home},
	year = {2020},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3313831.3376311},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090149393&doi=10.1145%2f3313831.3376311&partnerID=40&md5=1242778052df38657f5c08e3145f042b},
	abstract = {The presence of voice activated personal assistants (VAPAs) in people's homes rises each year [31]. Industry efforts are invested in making interactions with VAPAs more personal by leveraging information from messages and calendars, and by accessing user accounts for 3rd party services. However, the use of personal data becomes more complicated in interpersonal spaces, such as people's homes. Should a shared agent access the information of many users? If it does, how should it navigate issues of privacy and control? Designers currently lack guidelines to help them design appropriate agent behaviors. We used Speed Dating to explore inchoate social mores around agent actions within a home, including issues of proactivity, interpersonal conflict, and agent prevarication. Findings offer new insights on how more socially sophisticated agents might sense, make judgements about, and navigate social roles and individuals. We discuss how our findings might impact future research and future agent behaviors. © 2020 Owner/Author.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36}
}

@ARTICLE{Bahja202032,
	author = {Bahja, Mohammed and Hammad, Rawad and Butt, Gibran},
	title = {A User-Centric Framework for Educational Chatbots Design and Development},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12424 LNCS},
	pages = {32 – 43},
	doi = {10.1007/978-3-030-60117-1_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094133150&doi=10.1007%2f978-3-030-60117-1_3&partnerID=40&md5=4baee2d8c75b50a1f03bcff28e812b66},
	abstract = {Increasing frequency of epidemics, such as SARS-CoV, MERS-CoV, Ebola, and the recent COVID-19, have affected various sectors, especially education. As a result, emphasis on e-learning and distance learning has been increasing in recent years. The growing numbers of mobile users and access to the internet across the world has created more favorable conditions for adopting distance learning on a wider scale. However, lessons learnt from current experiments have highlighted poor student engagement with learning processes, hence a user-centric approach to design and develop educational chatbots is presented. A User-centric approach enables developers to consider the following: learners’ and teachers’ technological skills and competencies, attitudes, and perceptions and behaviour; conceptual concerns, such as pedagogical integration on online platforms, assessment procedures, varying learning culture and lifestyles; technical concerns, such as privacy, security, performance, ubiquity; and regulatory concerns, such as policies, frameworks, standards, ethics, roles and responsibilities have been identified in this study. To address these concerns, there is the need for user-centric design and collaborative approaches to the development of distance learning tools. Considering the abovementioned challenges and the growing emphasis on distance learning, we propose chatbot learning as an effective and efficient tool for delivering such learning. In this regard, a user-centric framework for designing chatbot learning applications and a collaborative user-centric design methodology for developing chatbot learning applications is proposed and discussed. © 2020, Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Sannon2020,
	author = {Sannon, Shruti and Stoll, Brett and Difranzo, Dominic and Jung, Malte F. and Bazarova, Natalya N.},
	title = {“I just shared your responses”: Extending communication privacy management theory to interactions with conversational agents},
	year = {2020},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	volume = {4},
	number = {GROUP},
	doi = {10.1145/3375188},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077779524&doi=10.1145%2f3375188&partnerID=40&md5=d2fb1e1b5b052270bcdb66dd4fb50664},
	abstract = {Conversational agents are increasingly becoming integrated into everyday technologies and can collect large amounts of data about users. As these agents mimic interpersonal interactions, we draw on communication privacy management theory to explore people's privacy expectations with conversational agents. We conducted a 3x3 factorial experiment in which we manipulated agents' social interactivity and data sharing practices to understand how these factors influence people's judgments about potential privacy violations and their evaluations of agents. Participants perceived agents that shared response data with advertisers more negatively compared to agents that shared such data with only their companies; perceptions of privacy violations did not differ between agents that shared data with their companies and agents that did not share information at all. Participants also perceived the socially interactive agent's sharing practices less negatively than those of the other agents, highlighting a potential privacy vulnerability that users are exposed to in interactions with socially interactive conversational agents. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20}
}

@CONFERENCE{Roussou2019,
	author = {Roussou, Maria and Perry, Sara and Katifori, Akrivi and Vassos, Stavros and Tzouganatou, Angeliki and McKinney, Sierra},
	title = {Transformation through provocation? Designing a ‘bot of conviction’ to challenge conceptions and evoke critical reflection},
	year = {2019},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	doi = {10.1145/3290605.3300857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067615313&doi=10.1145%2f3290605.3300857&partnerID=40&md5=ab8c04e96ab8576470a51ac574b70571},
	abstract = {Can a chatbot enable us to change our conceptions, to be critically reflective? To what extent can interaction with a technologically “minimal” medium such as a chatbot evoke emotional engagement in ways that can challenge us to act on the world? In this paper, we discuss the design of a provocative bot, a “bot of conviction”, aimed at triggering conversations on complex topics (e.g. death, wealth distribution, gender equality, privacy) and, ultimately, soliciting specific actions from the user it converses with. We instantiate our design with a use case in the cultural sector, specifically a Neolithic archaeological site that acts as a stage of conversation on such hard themes. Our larger contributions include an interaction framework for bots of conviction, insights gained from an iterative process of participatory design and evaluation, and a vision for bot interaction mechanisms that can apply to the HCI community more widely. © 2019 Association for Computing Machinery.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@CONFERENCE{Gondaliya2020235,
	author = {Gondaliya, Krishna and Butakov, Sergey and Zavarsky, Pavol},
	title = {SLA as a mechanism to manage risks related to chatbot services.},
	year = {2020},
	journal = {Proceedings - 2020 IEEE 6th Intl Conference on Big Data Security on Cloud, BigDataSecurity 2020, 2020 IEEE Intl Conference on High Performance and Smart Computing, HPSC 2020 and 2020 IEEE Intl Conference on Intelligent Data and Security, IDS 2020},
	pages = {235 – 240},
	doi = {10.1109/BigDataSecurity-HPSC-IDS49724.2020.00050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087914640&doi=10.1109%2fBigDataSecurity-HPSC-IDS49724.2020.00050&partnerID=40&md5=d0206f9a3795783e74d1bb73e5c73499},
	abstract = {Intelligent Chatbot services become one of the mainstream applications in user help and many other areas. Apart from bringing numerous benefits to users these services may bring additional risks to the companies that employ them. The study starts with the review of the scale of chatbot industry and common use cases by focusing on their applications industry tendencies. Review of functionality and architecture of typical chatbot services shows the potential risks associated with chatbots. Analysis of such risks in the paper helped to build a checklist that security managers can use to assess risks prior to chatbot implementation. The proposed checklist was tested by reviewing a number of Service Level Agreements (SLA) of real chatbot providers. © 2020 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Lin201986,
	author = {Lin, Pei-Chun and Yankson, Benjamin and Lu, Zhihui and Hung, Patrick C.K.},
	title = {Children privacy identification system in LINE chatbot for smart toys},
	year = {2019},
	journal = {IEEE International Conference on Cloud Computing, CLOUD},
	volume = {2019-July},
	pages = {86 – 90},
	doi = {10.1109/CLOUD.2019.00026},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072338496&doi=10.1109%2fCLOUD.2019.00026&partnerID=40&md5=ea40d0f4e65aaf24f3fefecf347fe750},
	abstract = {Children's privacy concerns about smart toys are becoming more and more critical in the toy industry. Parents and guardians continue to strive to protect their children from unnecessary privacy risks such as collection, and unconsented use of or access to their children's information. However, there is still no standardized privacy framework, which focuses on smart toys in this paradigm; making it difficult to determine possible privacy violation in for example determining whether a phrase shared with a smart toy is sensitive or not. To overcome this challenge, we build a privacy identification system through Chatbot technology. We call this system a Children Privacy Identification (CPI) system. To develop CPI system, we divide our research works into two parts: (1) Collect the phrase from the smart toys; and (2) Explore privacy Identification based on Personally Identifiable Information (PII) and Children's Online Privacy Protection Act (COPPA). For illustration, we integrate the CPI system in LINE Chatbot. The result shows that people feel more comfortable in talking to LINE Chatbot with privacy protection. © 2019 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Väänänen2020143,
	author = {Väänänen, Kaisa and Hiltunen, Aleksi and Varsaluoma, Jari and Pietilä, Iikka},
	title = {CivicBots – Chatbots for Supporting Youth in Societal Participation},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11970 LNCS},
	pages = {143 – 157},
	doi = {10.1007/978-3-030-39540-7_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079104131&doi=10.1007%2f978-3-030-39540-7_10&partnerID=40&md5=a770f9d26fa2addfbd3490441f0c2682},
	abstract = {Supporting young people to participate in societal development is an important factor in achieving sustainable future. Digital solutions can be designed to help youth participate in civic activities, such as city planning and legislation. To this end, we are using human-centered approach to study how digital tools can help youth discuss their ideas on various societal issues. Chatbots are conversational agents that have potential to trigger and support thought processes, as well as online activities. In this context, we are exploring how chatbots – which we call CivicBots – can be used to support youth (16–27 years) in societal participation. We created three scenarios for CivicBots and evaluated them with the youth in an online survey (N = 54). Positive perceptions of the youth concerning CivicBots suggest that CivicBots can advance equality and they may be able to reach youth better than a real person. On the negative side, CivicBots may cause unpleasant interactions by their over-proactive behaviour, and trustworthiness is affected by fears that the bot does not respect user’s privacy, or that it provides biased or limited information about societally important issues. © 2020, Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Prakash20201,
	author = {Prakash, Ashish Viswanath and Das, Saini},
	title = {Intelligent Conversational Agents in Mental Healthcare Services: A Thematic Analysis of User Perceptions},
	year = {2020},
	journal = {Pacific Asia Journal of the Association for Information Systems},
	volume = {12},
	number = {2},
	pages = {1 – 34},
	doi = {10.17705/1pais.12201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169025731&doi=10.17705%2f1pais.12201&partnerID=40&md5=2913c14d435b4efbea71c4a8a20e1cb0},
	abstract = {Background: The emerging Artificial Intelligence (AI) based Conversational Agents (CA) capable of delivering evidence-based psychotherapy presents a unique opportunity to solve longstanding issues such as social stigma and demand-supply imbalance associated with traditional mental health care services. However, the emerging literature points to several socio-ethical challenges which may act as inhibitors to the adoption in the minds of the consumers. We also observe a paucity of research focusing on determinants of adoption and use of AI-based CAs in mental healthcare. In this setting, this study aims to understand the factors influencing the adoption and use of Intelligent CAs in mental healthcare by examining the perceptions of actual users. Method: The study followed a qualitative approach based on netnography and used a rigorous iterative thematic analysis of publicly available user reviews of popular mental health chatbots to develop a comprehensive framework of factors influencing the user’s decision to adopt mental healthcare CA. Results: We developed a comprehensive thematic map comprising of four main themes, namely, perceived risk, perceived benefits, trust, and perceived anthropomorphism, along with its 12 constituent subthemes that provides a visualization of the factors that govern the user’s adoption and use of mental healthcare CA. Conclusions: Insights from our research could guide future research on mental healthcare CA use behavior. Additionally, it could also aid designers in framing better design decisions that meet consumer expectations. Our research could also guide healthcare policymakers and regulators in integrating this technology into formal healthcare delivery systems. © 2020, Association for Information Systems. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 33}
}

@ARTICLE{Patil20194296,
	author = {Patil, Kanchan and Kulkarni, Mugdha S.},
	title = {Artificial intelligence in financial services: Customer chatbot advisor adoption},
	year = {2019},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	volume = {9},
	number = {1},
	pages = {4296 – 4303},
	doi = {10.35940/ijitee.A4928.119119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075213653&doi=10.35940%2fijitee.A4928.119119&partnerID=40&md5=8b1ee635f387a124edf722635a5757f1},
	abstract = {The growing sophistication technology has helped us exchange Information at our fingertips, eliminating the need for human support.” A platform designed to understand, learn and converse like a human and answer ad-hoc queries in real time is commonly referred to as a Chabot”. Chabot advisor is Artificial intelligence (AI) computer program that impersonates human communication in its natural format including text or spoken language using a technique such as NLP, image processing or video processing along with the end task completion as instructed by the user [1]. The purpose of the paper was to examine what are the drivers for Chabot advisor services adoption (CBA), focusing on financial services. This study presents the explanatory Chabot advisor services factors by extending the Technology Acceptance Model (TAM). The construct in the research are like perceived privacy, perceived security, enjoyment and social influence. This empirical study was conducted in Pune city in India by collecting primary data from 310 online financial services customers. Data collected was analyzed using structural equation modeling using PLS-SEM.The outcome of this study is vital to financial companies like banks, policymakers, technology services adoption literature and provide customer-centric financial services. ©BEIESP.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@ARTICLE{Uohara2020361,
	author = {Uohara, Michael Y. and Weinstein, James N. and Rhew, David C.},
	title = {The Essential Role of Technology in the Public Health Battle against COVID-19},
	year = {2020},
	journal = {Population Health Management},
	volume = {23},
	number = {5},
	pages = {361 – 367},
	doi = {10.1089/pop.2020.0187},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092681418&doi=10.1089%2fpop.2020.0187&partnerID=40&md5=44c125a84afaf99472ea7a6955252991},
	abstract = {Technology has played an important role in responding to the novel coronavirus (SARS-CoV-2) and subsequent COVID-19 pandemic. The virus's blend of lethality and transmissibility have challenged officials and exposed critical limitations of the traditional public health apparatus. However, throughout this pandemic, technology has answered the call for a new form of public health that illustrates opportunities for enhanced agility, scale, and responsiveness. The authors share the Microsoft perspective and illustrate how technology has helped transform the public health landscape with new and refined capabilities - the efficacy and impact of which will be determined by history. Technologies like chatbot and virtualized patient care offer a mechanism to triage and distribute care at scale. Artificial intelligence and high-performance computing have accelerated research into understanding the virus and developing targeted therapeutics to treat infection and prevent transmission. New mobile contact tracing protocols that preserve patient privacy and civil liberties were developed in response to public concerns, creating new opportunities for privacy-sensitive technologies that aid efforts to prevent and control outbreaks. While much progress is still needed, the COVID-19 pandemic has highlighted technology's importance to public health security and pandemic preparedness. Future multi-stakeholder collaborations, including those with technology organizations, are needed to facilitate progress in overcoming the current pandemic, setting the stage for improved pandemic preparedness in the future. As lessons are assessed from the current pandemic, public officials should consider technology's role and continue to seek opportunities to supplement and improve on traditional approaches.  © Copyright 2020, Mary Ann Liebert, Inc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Gulenko20147,
	author = {Gulenko, Iwan},
	title = {Chatbot for IT security training: Using motivational interviewing to improve security behaviour},
	year = {2014},
	journal = {CEUR Workshop Proceedings},
	volume = {1197},
	pages = {7 – 16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925244807&partnerID=40&md5=7e7b543484f443c7b00628529aea2784},
	abstract = {We conduct a pre-study with 25 participants on Mechanical Turk to find out which security behavioural problems are most important for online users. These questions are based on motivational interviewing (MI), an evidence-based treatment methodology that enables to train people about different kinds of behavioural changes. Based on that the chatbot is developed using Artificial Intelligence Markup Language (AIML). The chatbot is trained to speak about three topics: passwords, privacy and secure browsing. These three topics were 'most-wanted' by the users of the pre-study. With the chatbot three training sessions with people are conducted.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Srivastava201912,
	author = {Srivastava, Saurabh and Prabhakar, T.V.},
	title = {Hospitality of chatbot building platforms},
	year = {2019},
	journal = {SQUADE 2019 - Proceedings of the 2nd ACM SIGSOFT International Workshop on Software Qualities and Their Dependencies, co-located with ESEC/FSE 2019},
	pages = {12 – 19},
	doi = {10.1145/3340495.3342751},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076478035&doi=10.1145%2f3340495.3342751&partnerID=40&md5=e843563d3049e4d2b9860b299c0c9798},
	abstract = {The temptation to be able to talk to a machine is not new. Recent advancements in the field of Natural Language Understanding has made it possible to build conversational components that can be plugged inside an application, similar to other components. These components, called chatbots, can be created from scratch or with the help of commercially available platforms. These platforms make it easier to build and deploy chatbots, often without writing a single line of code. However, similar to any other software component, chatbots also have quality concerns. Despite significant contributions in the field, an architectural perspective of building chatbots with desired quality requirements is missing in the literature. In the current work, we highlight the impact of features provided by these platforms (along with their quality) on the application design process and overall quality attributes. We propose a methodological framework to evaluate support provided by a chatbot platform towards achieving quality in the application. The framework, called Hospitality Framework, is based on software architectural body of knowledge, especially architectural tactics. The framework produces a metric, called Hospitality Index, which has utilities for making various design decisions for the overall application. We present the use of our framework on a simple use case to highlight the phases of evaluation.We showcase the process by picking three popular chatbot platforms - Watson Assistant, DialogFlow and Lex, over four quality attributes - Modifiability, Security & Privacy, Interoperability and Reliability. Our results show that different platforms provide different support for these four quality attributes. Copyright © SQUADE 2019 ACM SIGSOFT International Workshop on Software Qualities and Their Dependencies, co-located with ESEC/FSE 2019.All right reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Bahja202020,
	author = {Bahja, Mohammed and Abuhwaila, Nour and Bahja, Julia},
	title = {An Antenatal Care Awareness Prototype Chatbot Application Using a User-Centric Design Approach},
	year = {2020},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {12424 LNCS},
	pages = {20 – 31},
	doi = {10.1007/978-3-030-60117-1_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094113713&doi=10.1007%2f978-3-030-60117-1_2&partnerID=40&md5=1593f2dc3e36fb4c2ea7b1167c339c38},
	abstract = {The frequency of occurrence of severe infectious diseases, such as SARS-CoV, MERS-CoV, Ebola and COVID-19, has been increasing in recent years, thus putting pressure on the delivery of healthcare services. Pregnant women are some of the most vulnerable patients, as they are more prone to infections and have limited mobility due to their health situation. In addition, preventive measures, such as social distancing and lockdowns have affected their access to healthcare services. Considering these issues, in this study, a prototype chatbot application that can provide antenatal care and support for pregnant women from the comfort of their home is proposed. A user-centric design methodology was adopted, where two midwives, one obstetrician and eleven pregnant women participated in the design and development process by providing regular reviews at various stages of the process. In addition, an online User Experience Questionnaire was employed for collecting the users’ experiences after engaging with the protype application for two weeks. The findings reveal that the proposed chatbot application (Alia) is effective in terms of attractiveness, perspicuity, efficiency, stimulation, and novelty. In addition, concerns related to dependability (privacy and security) and supportability were identified. Finally, UEQ scales of pragmatic quality (1.12) and hedonic quality (1.11) related to the chatbot application Alia, reflected good usability, reliability and quality aspects. © 2020, Springer Nature Switzerland AG.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Feng2019169,
	author = {Feng, Yebo and Li, Jun and Jiao, Lei and Wu, Xintao},
	title = {BotFlowMon: Learning-based, Content-Agnostic Identification of Social Bot Traffic Flows},
	year = {2019},
	journal = {2019 IEEE Conference on Communications and Network Security, CNS 2019},
	pages = {169 – 177},
	doi = {10.1109/CNS.2019.8802706},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071724650&doi=10.1109%2fCNS.2019.8802706&partnerID=40&md5=f66cd67c59326a6969799a521972cbdd},
	abstract = {With the fast-growing popularity of online social networks (OSN), maintaining the security of OSN ecosystems becomes essential for the public. Among all the security threats facing OSN, malicious social bots have become the most common and detrimental. These bot programs are often employed to violate users' privacy, distribute spam, and disturb the financial market, posing a compelling need for effective social bot detection solutions. Unlike traditional bot detection approaches that have strict requirements on data sources (e.g., private payload information, social relationships, or activity histories), this paper proposes a detection method called BotFlowMon that relies only on NetFlow data as input to identify OSN bot traffic, where every NetFlow record is a summary of a traffic flow on the Internet and contains no payload content. BotFlowMon introduces several new algorithms and techniques to help use machine learning to classify the social bot traffic from the real OSN user traffic, including aggregating NetFlow records to obtain transaction data, fusing transaction data to extract features and visualize flows, as well as subdividing transactions into basic actions. Our evaluation shows that with 535GB raw NetFlow records as input, BotFlowMon can efficiently classify the traffic from social bots, including chatbot, amplification bot, post bot, crawler bot, and hybrid bot, with 92.33-93.61 % accuracy. © 2019 IEEE.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Arora201815,
	author = {Arora, Priya and Chaspari, Theodora},
	title = {Exploring Siamese neural network architectures for preserving speaker identity in speech emotion classification},
	year = {2018},
	journal = {Proceedings of the 4th Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction, MA3HMI 2018 - In conjunction with ICMI 2018},
	pages = {15 – 18},
	doi = {10.1145/3279972.3279980},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058175685&doi=10.1145%2f3279972.3279980&partnerID=40&md5=df3053ad6f6699356bcab7aa1cea1c5a},
	abstract = {Voice-enabled communication is increasingly being used in real-world applications, such as the ones involving conversational bots or “chatbots”. Chatbots can spark and sustain user engagement by effectively recognizing their emotions and acting upon them. However, the majority of emotion recognition systems rely on rich spectrotemporal acoustic features. Beyond the emotion-related information, such systems tend to preserve information relevant to the identity of the speaker, therefore raising major privacy concerns from the users. This paper introduces two hybrid architectures for privacy-preserving emotion recognition from speech. These architectures rely on a Siamese neural network, whose input and intermediate layers are transformed using various privacy-performing operations in order to retain emotion-dependent content and suppress information related to the identity of a speaker. The proposed approach is evaluated through emotion classification and speaker identification performance metrics. Results indicate that the proposed framework can achieve up to 67.4% for classifying between happy, sad, frustrated, anger, neutral and other emotions, obtained from the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. At the same time, the proposed approach reduces speaker identification accuracy to 50%, compared to 81%, the latter being achieved through a feedforward neural network solely trained on the speaker identification task using the same input features. © 2018 Association for Computing Machinery.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Rese2020,
	author = {Rese, Alexandra and Ganster, Lena and Baier, Daniel},
	title = {Chatbots in retailers’ customer communication: How to measure their acceptance?},
	year = {2020},
	journal = {Journal of Retailing and Consumer Services},
	volume = {56},
	doi = {10.1016/j.jretconser.2020.102176},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086591217&doi=10.1016%2fj.jretconser.2020.102176&partnerID=40&md5=af3c9259879f038ff2aed735cd169d1c},
	abstract = {Currently, online retailers evaluate whether chatbots—software programs that interact with users using natural languages—could improve their customers' satisfaction. In a retail context, chatbots allow humans to pose shopping-related questions and receive answers in natural language without waiting for a salesperson or using other automated communication forms. However, until now, it has been unclear which customers accept this new communication form and which factors determine their acceptance. In this paper, we contrast the well-known technology acceptance model (TAM) with the lesser known uses and gratifications (U&G) theory, applying both approaches to measure the acceptance of the text-based “Emma” chatbot by its target segment. “Emma” was developed for the prepurchase phase of online fashion retailing and integrated into Facebook Messenger by the major German online retailer Zalando. Data were collected from 205 German Millennial respondents in a usability study. The results show that both utilitarian factors such as “authenticity of conversation” and “perceived usefulness,” as well as hedonic factors such as “perceived enjoyment”, positively influence the acceptance of “Emma”. However, privacy concerns and the immaturity of the technology had a negative effect on usage intention and frequency. The predictive power of both models was similar, showing little deviation, but U&G gives alternative insights into the customers’ motivation to use “Emma” compared to the TAM. © 2020 Elsevier Ltd},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 154}
}

@ARTICLE{AlGosaibi2020260,
	author = {AlGosaibi, Abdulelah Abdallah and Sait, Abdul Rahaman Wahab and AlOthman, Abdulaziz Fahad and AlHamed, Shadan},
	title = {Developing an intelligent framework for improving the quality of service in the government organizations in the Kingdom of Saudi Arabia},
	year = {2020},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {11},
	number = {12},
	pages = {260 – 268},
	doi = {10.14569/IJACSA.2020.0111233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101461712&doi=10.14569%2fIJACSA.2020.0111233&partnerID=40&md5=09f20888f6e6b5720dd3b91bef263b5c},
	abstract = {—The Kingdom of Saudi Arabia is enhancing the services and applications in government organizations through the number of systems that generate a massive amount of data through Big Data technology. Recently, the Global Artificial Intelligent Summit 2020, Saudi Data and Artificial Intelligence Authority (SDAIA), NEOM have launched an Artificial Intelligence (AI) strategy that aligns with the Kingdom Vision 2030. AI opens a wide door for opportunities and new strategies that will narrow the gap in the skillset of individuals and promote research and innovation in the IT industry. Organizations lack advanced techniques to evaluate the performance of individuals and departments that supports improving the quality of service. The introduction of AI-based applications in the government and private sectors will facilitate decision-makers in tracking and optimizing the efficiency of departments and individuals. This research aims to develop an intelligent framework for government organizations to improve the quality of services rendered to customers and businesses. In addition, it highlights the importance of AI policies in archiving metadata. This paper presents a framework for an organization that contains Chatbot, Sentiment Analysis, and Key Performance Indicators to improve the services. A synthetic dataset is employed as a testbed to evaluate the performance of the framework. The outcome of this study shows that the proposed framework able to improve the performance of organizations. Using this proposed framework, organizations can build a mechanism for their workforce to retrieve meaningful information. Moreover, it provides significant features include efficient data extraction, data management, and AI-based security for effective document management. © 2020 Science and Information Organization. All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chi2017542,
	author = {Chi, Nai-Ching and Sparks, Olivia and Lin, Shih-Yin and Lazar, Amanda and Thompson, Hilaire J. and Demiris, George},
	title = {Pilot testing a digital pet avatar for older adults},
	year = {2017},
	journal = {Geriatric Nursing},
	volume = {38},
	number = {6},
	pages = {542 – 547},
	doi = {10.1016/j.gerinurse.2017.04.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018939180&doi=10.1016%2fj.gerinurse.2017.04.002&partnerID=40&md5=d3de5b51a279b2be8fdcafec5ca81f5b},
	abstract = {Social isolation in older adults is a major public health concern. An embodied conversational agent (ECA) has the potential to enhance older adults’ social interaction. However, little is known about older adults’ experience with an ECA. In this paper, we conducted a pilot study to examine the perceived acceptance and utility of a tablet-based conversational agent in the form of an avatar (termed “digital pet”) for older adults. We performed secondary analysis of data collected from a study that employed the use of a digital pet in ten older adults’ homes for three months. Most of the participants enjoyed the companionship, entertainment, reminders, and instant assistance from the digital pet. However, participants identified limited conversational ability and technical issues as system challenges. Privacy, dependence, and cost were major concerns. Future applications should maximize the agent's conversational ability and the system's overall usability. Our results can inform future designs of conversational agents for older adults, which need to include older adults as system co-designers to maximize usability and acceptance. © 2017 Elsevier Inc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27}
}

@CONFERENCE{Milton2018,
	author = {Milton, R. and Hay, D. and Gray, S. and Buyuklieva, B. and Hudson-Smith, A.},
	title = {Smart IoT and soft AI},
	year = {2018},
	journal = {IET Conference Publications},
	volume = {2018},
	number = {CP740},
	doi = {10.1049/cp.2018.0016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048431023&doi=10.1049%2fcp.2018.0016&partnerID=40&md5=9e7f3fd61a6baa88c462d800a1e2182b},
	abstract = {Soft artificial intelligence (AI) is defined as non-sentient AI designed to perform close to human level in one specific domain. This is in contrast to “Artificial General Intelligence” (AGI) which solves the problem for human level intelligence across all domains. Soft AI is a reality now in the new generation of smart Internet of Things devices like Amazon’s Alexa, Apple’s Siri or Microsoft’s Cortana, giving rise to concerns about privacy and how the technology is being used. This research is based around an experiment in “AI as a service” where fifteen chatbot agents using Google’s “Dialogflow” are deployed around the Queen Elizabeth Olympic Park in London for the general public to interact with. The physical devices are 3D printed representations of creatures living in the park, designed to fit with the park’s biodiversity remit. Park visitors interact with the creatures via their mobile phones, engaging in a conversation where the creature offers to tell them a memory in exchange for one of their own, while warning them that anything they say might be repeated to others. The scope of the work presented here is as follows. After explaining the details of the deployment and three month study, the conversational data collected from visitors is then anal-ysed. Following a review of the current literature, techniques for working with the unstructured natural language data are developed, leading to recommendations for the design of future conversational “chatbot” agents. The results show distinct patterns of conversation, from simple and direct “verb plus noun” commands to complex sentence structure. How users interact with the agents, given that they are conversing with a mechanism, is discussed and contrasted with the memories that they have agreed to share. The conclusion drawn from this work is that, while the current generation of devices only listen for commands from users, there is a danger that smart IoT devices in the future can be used as active information probes unless properly understood and regulated. We finish with observations on privacy and security based on our experiences here. © 2018 Institution of Engineering and Technology. All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Thomaz202043,
	author = {Thomaz, Felipe and Salge, Carolina and Karahanna, Elena and Hulland, John},
	title = {Learning from the Dark Web: leveraging conversational agents in the era of hyper-privacy to enhance marketing},
	year = {2020},
	journal = {Journal of the Academy of Marketing Science},
	volume = {48},
	number = {1},
	pages = {43 – 63},
	doi = {10.1007/s11747-019-00704-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075185738&doi=10.1007%2fs11747-019-00704-3&partnerID=40&md5=4e93fed65a8f584a08f0ed5fdb7198ab},
	abstract = {The Web is a constantly evolving, complex system, with important implications for both marketers and consumers. In this paper, we contend that over the next five to ten years society will see a shift in the nature of the Web, as consumers, firms and regulators become increasingly concerned about privacy. In particular, we predict that, as a result of this privacy-focus, various information sharing and protection practices currently found on the Dark Web will be increasingly adapted in the overall Web, and in the process, firms will lose much of their ability to fuel a modern marketing machinery that relies on abundant, rich, and timely consumer data. In this type of controlled information-sharing environment, we foresee the emersion of two distinct types of consumers: (1) those generally willing to share their information with marketers (Buffs), and (2) those who generally deny access to their personal information (Ghosts). We argue that one way marketers can navigate this new environment is by effectively designing and deploying conversational agents (CAs), often referred to as “chatbots.” In particular, we propose that CAs may be used to understand and engage both types of consumers, while providing personalization, and serving both as a form of differentiation and as an important strategic asset for the firm—one capable of eliciting self-disclosure of otherwise private consumer information. © 2019, The Author(s).},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 102}
}

@CONFERENCE{Baudart201899,
	author = {Baudart, Guillaume and Dolby, Julian and Duesterwald, Evelyn and Hirzel, Martin and Shinnar, Avraham},
	title = {Protecting chatbots from toxic content},
	year = {2018},
	journal = {Onward! 2018 - Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2018},
	pages = {99 – 110},
	doi = {10.1145/3276954.3276958},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093738628&doi=10.1145%2f3276954.3276958&partnerID=40&md5=d16e8a099896bc18c52701d201b7af18},
	abstract = {There is a paradigm shift in web-based services towards conversational user interfaces. Companies increasingly offer conversational interfaces, or chatbots, to let their customers or employees interact with their services in a more flexible and mobile manner. Unfortunately, this new paradigm faces a major problem, namely toxic content in user inputs. Toxic content in user inputs to chatbots may cause privacy concerns, may be adversarial or malicious, and can cause the chatbot provider substantial economic, reputational, or legal harm. We address this problem with an interdisciplinary approach, drawing upon programming languages, cloud computing, and other disciplines to build protections for chatbots. Our solution, called BotShield, is non-intrusive in that it does not require changes to existing chatbots or underlying conversational platforms. This paper introduces novel security mechanisms, articulates their security guarantees, and illustrates them via case studies.  © 2018 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Williams2020314,
	author = {Williams, Nigel L. and Ferdinand, Nicole and Bustard, John},
	title = {From WOM to aWOM – the evolution of unpaid influence: a perspective article},
	year = {2020},
	journal = {Tourism Review},
	volume = {75},
	number = {1},
	pages = {314 – 318},
	doi = {10.1108/TR-05-2019-0171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074495423&doi=10.1108%2fTR-05-2019-0171&partnerID=40&md5=606663eb06e78d0c6a4e8f253259ed32},
	abstract = {Purpose: Advances in artificial intelligence (AI) natural language processing may see the emergence of algorithmic word of mouth (aWOM), content created and shared by automated tools. As AI tools improve, aWOM will increase in volume and sophistication, displacing eWOM as an influence on customer decision-making. The purpose of this paper is to provide an overview of the socio technological trends that have encouraged the evolution of informal infulence strategies from WOM to aWOM. Design/methodology/approach: This paper examines the origins and path of development of influential customer communications from word of mouth (WOM) to electronic word of mouth (eWOM) and the emerging trend of aWOM. The growth of aWOM is theorized as a result of new developments in AI natural language processing tools along with autonomous distribution systems in the form of software robots and virtual assistants. Findings: aWOM may become a dominant source of information for tourists, as it can support multimodal delivery of useful contextual information. Individuals, organizations and social media platforms will have to ensure that aWOM is developed and deployed responsibly and ethically. Practical implications: aWOM may emerge as the dominant source of information for tourist decision-making, displacing WOM or eWOM. aWOM may also impact online opinion leaders, as they may be challenged by algorithmically generated content. aWOM tools may also generate content using sensors on personal devices, creating privacy and information security concerns if users did not give permission for such activities. Originality/value: This paper is the first to theorize the emergence of aWOM as autonomous AI communication within the framework of unpaid influence or WOM. As customer engagement will increasingly occur in algorithmic environments that comprise person–machine interactions, aWOM will influence future tourism research and practice. © 2019, Emerald Publishing Limited.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@ARTICLE{Griffin2020504,
	author = {Griffin, Ashley C. and Xing, Zhaopeng and Khairat, Saif and Wang, Yue and Bailey, Stacy and Arguello, Jaime and Chung, Arlene E.},
	title = {Conversational Agents for Chronic Disease Self-Management: A Systematic Review},
	year = {2020},
	journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
	volume = {2020},
	pages = {504 – 513},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105317634&partnerID=40&md5=37f3a3bf32d886b8dc9d46d9265f006a},
	abstract = {We conducted a systematic literature review to assess how conversational agents have been used to facilitate chronic disease self-management. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework was used. Literature was searched across five databases, and we included full-text articles that contained primary research findings for text-based conversational agents focused on self-management for chronic diseases in adults. 1,606 studies were identified, and 12 met inclusion criteria. Outcomes were largely focused on usability of conversational agents, and participants mostly reported positive attitudes with some concerns related to privacy and shallow content. In several studies, there were improvements on the Patient Health Questionnaire (p<0.05), Generalized Anxiety Disorder Scale (p=0.004), Perceived Stress Scale (p=0.048), Flourishing Scale (p=0.032), and Overall Anxiety Severity and Impairment Scale (p<0.05). There is early evidence that suggests conversational agents are acceptable, usable, and may be effective in supporting self-management, particularly for mental health. ©2020 AMIA - All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Laban2020,
	author = {Laban, Guy and Araujo, Theo},
	title = {The Effect of Personalization Techniques in Users' Perceptions of Conversational Recommender Systems},
	year = {2020},
	journal = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents, IVA 2020},
	doi = {10.1145/3383652.3423890},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096954637&doi=10.1145%2f3383652.3423890&partnerID=40&md5=0e018747c6b5ebfecc7db0201a7ac0dc},
	abstract = {Conversational recommender systems provide users with individually tailored recommendations in a flowing dialogue. These require users to disclose information proactively or reactively for receiving personalized recommendations, which can trigger users' resistance to the platform and to the recommendations. Accordingly, this study examined the extent to which user-initiated and system-initiated recommendations provided by a conversational recommender system influenced users' perceptions of it. The results of an online experiment entail that when recommendations are system-initiated, as compared to user-initiated, users perceive to be in less control and perceive the system as riskier. Furthermore, the results stress that systems that provide user-initiated or system-initiated recommendations do not differ in users' perceptions of anthropomorphism. © 2020 Owner/Author.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@CONFERENCE{Hernández-Trapote2008305,
	author = {Hernández-Trapote, Álvaro and López-Mencía, Beatriz and Díaz, David and Fernández-Pozo, Rubén and Caminero, Javier},
	title = {Embodied conversational agents for Voice-Biometric interfaces},
	year = {2008},
	journal = {ICMI'08: Proceedings of the 10th International Conference on Multimodal Interfaces},
	pages = {305 – 311},
	doi = {10.1145/1452392.1452454},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-63449104556&doi=10.1145%2f1452392.1452454&partnerID=40&md5=b12fbdb8584819659f5117121a747bbc},
	abstract = {In this article we present a research scheme which aims to analyze the use of Embodied Conversational Agent (ECA) technology to improve the robustness and acceptability of speaker enrolment and verification dialogues designed to provide secure access through natural and intuitive speaker recognition. In order to find out the possible effects of the visual information channel provided by the ECA, tests were carried out in which users were divided into two groups, each interacting with a different interface (metaphor): an ECA Metaphor group -with an ECA-, and a VOICE Metaphor group -without an ECA-. Our evaluation methodology is based on the ITU-T P.851 recommendation for spoken dialogue system evaluation, which we have complemented to cover particular aspects with regard to the two major extra elements we have incorporated: secure access and an ECA. Our results suggest that likeability-type factors and system capabilities are perceived more positively by the ECA metaphor users than by the VOICE metaphor users. However, the ECA's presence seems to intensify users' privacy concerns. Copyright 2008 ACM.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Ngo2014222,
	author = {Ngo, Thi Duyen and Vu, Thi Hong Nhan and Nguyen, Viet Ha and Bui, The Duy},
	title = {Improving simulation of continuous emotional facial expressions by analyzing videos of human facial activities},
	year = {2014},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {8861},
	pages = {222 – 237},
	doi = {10.1007/978-3-319-13191-7_18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910138259&doi=10.1007%2f978-3-319-13191-7_18&partnerID=40&md5=7eaaad2b759f5f4e2adc7e44cc454b5e},
	abstract = {Conversational agents are receiving significant attention from multiagent and human computer interaction research societies. In order to make conversational agents more believable and friendly, giving them the ability to express emotions is one of research fields which have drawn a lot of attention lately. In this paper, we propose a work on analysis of how emotional facial activities happen temporally. Our goal is to find the temporal patterns of facial activity of six basic emotions in order to improve the simulation of continuous emotional facial expressions on a 3D face of an embodied agent. Using facial expression recognition techniques, we first analyze a spontaneous video database in order to consider how facial activities are related to six basic emotions temporally. From there, we bring out the general temporal patterns for facial expressions of the six basic emotions. Then, based on the temporal patterns, we propose a scheme for displaying continuous emotional states of a conversational agent on a 3D face. © Springer International Publishing Switzerland 2014.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Inkster2018,
	author = {Inkster, Becky and Sarda, Shubhankar and Subramanian, Vinod},
	title = {An empathy-driven, conversational artificial intelligence agent (Wysa) for digital mental well-being: Real-world data evaluation mixed-methods study},
	year = {2018},
	journal = {JMIR mHealth and uHealth},
	volume = {6},
	number = {11},
	doi = {10.2196/12106},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060334446&doi=10.2196%2f12106&partnerID=40&md5=ee4042f0f43089954f87f8eb3205565f},
	abstract = {Background: A World Health Organization 2017 report stated that major depression affects almost 5% of the human population. Major depression is associated with impaired psychosocial functioning and reduced quality of life. Challenges such as shortage of mental health personnel, long waiting times, perceived stigma, and lower government spends pose barriers to the alleviation of mental health problems. Face-to-face psychotherapy alone provides only point-in-time support and cannot scale quickly enough to address this growing global public health challenge. Artificial intelligence (AI)-enabled, empathetic, and evidence-driven conversational mobile app technologies could play an active role in filling this gap by increasing adoption and enabling reach. Although such a technology can help manage these barriers, they should never replace time with a health care professional for more severe mental health problems. However, app technologies could act as a supplementary or intermediate support system. Mobile mental well-being apps need to uphold privacy and foster both short-and long-term positive outcomes. Objective: This study aimed to present a preliminary real-world data evaluation of the effectiveness and engagement levels of an AI-enabled, empathetic, text-based conversational mobile mental well-being app, Wysa, on users with self-reported symptoms of depression. Methods: In the study, a group of anonymous global users were observed who voluntarily installed the Wysa app, engaged in text-based messaging, and self-reported symptoms of depression using the Patient Health Questionnaire-9. On the basis of the extent of app usage on and between 2 consecutive screening time points, 2 distinct groups of users (high users and low users) emerged. The study used mixed-methods approach to evaluate the impact and engagement levels among these users. The quantitative analysis measured the app impact by comparing the average improvement in symptoms of depression between high and low users. The qualitative analysis measured the app engagement and experience by analyzing in-app user feedback and evaluated the performance of a machine learning classifier to detect user objections during conversations. Results: The average mood improvement (ie, difference in pre-and post-self-reported depression scores) between the groups (ie, high vs low users; n=108 and n=21, respectively) revealed that the high users group had significantly higher average improvement (mean 5.84 [SD 6.66]) compared with the low users group (mean 3.52 [SD 6.15]); Mann-Whitney P=.03 and with a moderate effect size of 0.63. Moreover, 67.7% of user-provided feedback responses found the app experience helpful and encouraging. Conclusions: The real-world data evaluation findings on the effectiveness and engagement levels of Wysa app on users with self-reported symptoms of depression show promise. However, further work is required to validate these initial findings in much larger samples and across longer periods. © Becky Inkster, Shubhankar Sarda, Vinod Subramanian.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 327}
}

@ARTICLE{Roca2020,
	author = {Roca, Surya and Sancho, Jorge and García, José and Alesanco, Álvaro},
	title = {Microservice chatbot architecture for chronic patient support},
	year = {2020},
	journal = {Journal of Biomedical Informatics},
	volume = {102},
	doi = {10.1016/j.jbi.2019.103305},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077322086&doi=10.1016%2fj.jbi.2019.103305&partnerID=40&md5=bf7bbf24a0a82d95fd11aae98395bea1},
	abstract = {Chatbots are able to provide support to patients suffering from very different conditions. Patients with chronic diseases or comorbidities could benefit the most from chatbots which can keep track of their condition, provide specific information, encourage adherence to medication, etc. To perform these functions, chatbots need a suitable underlying software architecture. In this paper, we introduce a chatbot architecture for chronic patient support grounded on three pillars: scalability by means of microservices, standard data sharing models through HL7 FHIR and standard conversation modeling using AIML. We also propose an innovative automation mechanism to convert FHIR resources into AIML files, thus facilitating the interaction and data gathering of medical and personal information that ends up in patient health records. To align the way people interact with each other using messaging platforms with the chatbot architecture, we propose these very same channels for the chatbot-patient interaction, paying special attention to security and privacy issues. Finally, we present a monitored-data study performed in different chronic diseases, and we present a prototype implementation tailored for one specific chronic disease, psoriasis, showing how this new architecture allows the change, the addition or the improvement of different parts of the chatbot in a dynamic and flexible way, providing a substantial improvement in the development of chatbots used as virtual assistants for chronic patients. © 2019 Elsevier Inc.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 56}
}
