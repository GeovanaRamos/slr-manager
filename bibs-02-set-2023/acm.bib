@inproceedings{10.1145/3582515.3609536,
author = {Montagna, Sara and Ferretti, Stefano and Klopfenstein, Lorenz Cuno and Florio, Antonio and Pengo, Martino Francesco},
title = {Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609536},
doi = {10.1145/3582515.3609536},
abstract = {Chronic patient self-management is crucial for maintaining physical and psychological health, reducing pressure on healthcare systems, and promoting patient empowerment. Digital technologies, particularly chatbots, have emerged as powerful tools for supporting patients in managing their chronic conditions. Large language models (LLMs), such as GPT-4, have shown potential in improving chatbot-based systems in healthcare. However, their adoption in clinical practice faces challenges, including reliability, the need for clinical trials, and privacy concerns. This paper proposes a general architecture for developing an LLM-based chatbot system that supports chronic patients while addressing privacy and security concerns. The architecture is designed to be independent of specific technologies and health conditions, focusing on data protection legislation compliance. A prototype of the system has been developed for hypertension management, demonstrating its potential for motivating patients to monitor their blood pressure and adhere to prescriptions.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {205–212},
numpages = {8},
keywords = {chatbot, hypertension, personal data store, healthcare data privacy},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@inproceedings{10.1145/3340495.3342751,
author = {Srivastava, Saurabh and Prabhakar, T.V.},
title = {Hospitality of Chatbot Building Platforms},
year = {2019},
isbn = {9781450368575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340495.3342751},
doi = {10.1145/3340495.3342751},
abstract = {The temptation to be able to talk to a machine is not new. Recent advancements in the field of Natural Language Understanding has made it possible to build conversational components that can be plugged inside an application, similar to other components. These components, called chatbots, can be created from scratch or with the help of commercially available platforms. These platforms make it easier to build and deploy chatbots, often without writing a single line of code. However, similar to any other software component, chatbots also have quality concerns. Despite significant contributions in the field, an architectural perspective of building chatbots with desired quality requirements is missing in the literature. In the current work, we highlight the impact of features provided by these platforms (along with their quality) on the application design process and overall quality attributes. We propose a methodological framework to evaluate support provided by a chatbot platform towards achieving quality in the application. The framework, called Hospitality Framework, is based on software architectural body of knowledge, especially architectural tactics. The framework produces a metric, called Hospitality Index, which has utilities for making various design decisions for the overall application. We present the use of our framework on a simple use case to highlight the phases of evaluation. We showcase the process by picking three popular chatbot platforms - Watson Assistant, DialogFlow and Lex, over four quality attributes - Modifiability, Security \&amp; Privacy, Interoperability and Reliability. Our results show that different platforms provide different support for these four quality attributes.},
booktitle = {Proceedings of the 2nd ACM SIGSOFT International Workshop on Software Qualities and Their Dependencies},
pages = {12–19},
numpages = {8},
keywords = {Conversational Systems, Hospitality, Quality Attributes},
location = {Tallinn, Estonia},
series = {SQUADE 2019}
}

@inproceedings{10.1145/3512353.3512363,
author = {Tran, Lan Anh and Hensen, Benedikt and Klamma, Ralf and Chantaraskul, Soamsiri},
title = {Privacy and Security in Mixed Reality Learning Environments by Input and User/Bot Interaction Protection},
year = {2022},
isbn = {9781450395571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512353.3512363},
doi = {10.1145/3512353.3512363},
abstract = {Mixed reality is known as an advanced technology that provides a new approach for learning environments. Such environments allow learners to interact with both virtual and real worlds and bringing in potential enhancements to the learning process at the same time. For example, chatbots can facilitate the learning process. However, security and privacy settings for interacting with chatbots in such mixed reality environments are complex. In this paper, we introduce a mixed reality virtual assistant that is integrated into the collaborative environment of our existing application VIAProMa. This embodied chatbot allows lecturers and students to participate in mixed reality and online classrooms in real-time. The participants can interact with each other via VIAProMa’s avatar representations and can communicate with the chatbot that is represented by the mixed reality bot. The bot is realized by connecting a Slack chatbot with the mixed reality learning environment. It is displayed as an intuitive 3D model and is able to communicate with the users in spoken language. In this environment, privacy and security settings are conducted to protect the user input and user interaction with the bot. The evaluation results show that the system works stably with good performance. All the visualizations and features are well designed and were understood by the users. Users preferred the speech interface with the bot over a textual interface. The research has a strong impact on the design of security and privacy features for mixed reality environments in general.},
booktitle = {Proceedings of the 2022 4th Asia Pacific Information Technology Conference},
pages = {63–71},
numpages = {9},
keywords = {social bots, mixed reality, learning environment, collaboration},
location = {Virtual Event, Thailand},
series = {APIT '22}
}

@inproceedings{10.1145/3313831.3376315,
author = {Strengers, Yolande and Qu, Lizhen and Xu, Qiongkai and Knibbe, Jarrod},
title = {Adhering, Steering, and Queering: Treatment of Gender in Natural Language Generation},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376315},
doi = {10.1145/3313831.3376315},
abstract = {Natural Language Generation (NLG) supports the creation of personalized, contextualized, and targeted content. However, the algorithms underpinning NLG have come under scrutiny for reinforcing gender, racial, and other problematic biases. Recent research in NLG seeks to remove these biases through principles of fairness and privacy. Drawing on gender and queer theories from sociology and Science and Technology studies, we consider how NLG can contribute towards the advancement of gender equity in society. We propose a conceptual framework and technical parameters for aligning NLG with feminist HCI qualities. We present three approaches: (1) adhering to current approaches of removing sensitive gender attributes, (2) steering gender differences away from the norm, and (3) queering gender by troubling stereotypes. We discuss the advantages and limitations of these approaches across three hypothetical scenarios; newspaper headlines, job advertisements, and chatbots. We conclude by discussing considerations for implementing this framework and related ethical and equity agendas.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {feminist hci, natural language generation},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3290605.3300857,
author = {Roussou, Maria and Perry, Sara and Katifori, Akrivi and Vassos, Stavros and Tzouganatou, Angeliki and McKinney, Sierra},
title = {Transformation through Provocation?},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300857},
doi = {10.1145/3290605.3300857},
abstract = {Can a chatbot enable us to change our conceptions, to be critically reflective? To what extent can interaction with a technologically 'minimal' medium such as a chatbot evoke emotional engagement in ways that can challenge us to act on the world? In this paper, we discuss the design of a provocative bot, a 'bot of conviction', aimed at triggering conversations on complex topics (e.g. death, wealth distribution, gender equality, privacy) and, ultimately, soliciting specific actions from the user it converses with. We instantiate our design with a use case in the cultural sector, specifically a Neolithic archaeological site that acts as a stage of conversation on such hard themes. Our larger contributions include an interaction framework for bots of conviction, insights gained from an iterative process of participatory design and evaluation, and a vision for bot interaction mechanisms that can apply to the HCI community more widely.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {chatbots, provocative interaction, conversational agents, cultural informatics, ux design, emotional engagement},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3411764.3445312,
author = {Langevin, Raina and Lordon, Ross J and Avrahami, Thi and Cowan, Benjamin R. and Hirsch, Tad and Hsieh, Gary},
title = {Heuristic Evaluation of Conversational Agents},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445312},
doi = {10.1145/3411764.3445312},
abstract = {Conversational interfaces have risen in popularity as businesses and users adopt a range of conversational agents, including chatbots and voice assistants. Although guidelines have been proposed, there is not yet an established set of usability heuristics to guide and evaluate conversational agent design. In this paper, we propose a set of heuristics for conversational agents adapted from Nielsen’s heuristics and based on expert feedback. We then validate the heuristics through two rounds of evaluations conducted by participants on two conversational agents, one chatbot and one voice-based personal assistant. We find that, when using our heuristics to evaluate both interfaces, evaluators were able to identify more usability issues than when using Nielsen’s heuristics. We propose that our heuristics successfully identify issues related to dialogue content, interaction design, help and guidance, human-like characteristics, and data privacy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {632},
numpages = {15},
keywords = {heuristic evaluation, user interface design, conversational agents},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3449171,
author = {Tian, Xiaoyi and Risha, Zak and Ahmed, Ishrat and Lekshmi Narayanan, Arun Balajiee and Biehl, Jacob},
title = {Let's Talk It Out: A Chatbot for Effective Study Habit Behavioral Change},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449171},
doi = {10.1145/3449171},
abstract = {Research has shown study habits and skills to be correlated with academic success, calling for a deeper comprehension of these behaviors and processes to design effective interventions for struggling students. Chatbots have recently been used as a persuasive technology to help support behavioral change, making them an intriguing design space for students' study habits and skills. This paper investigated the feasibility of using chatbots for promoting behavioral change of college students majoring in Computer Science (CS). We conducted semi-structured interviews with CS peer-tutors and surveyed university freshmen to understand students' study habits and identify technical intervention opportunities. Inspired by the findings, we designed StudyBuddy, a chatbot prototype deployed in Slack that periodically sends tips, provides assessments of students' study habits via surveys, helps the students break down assignments, recommends academic resources, and sends reminders. We evaluated the usability of the prototype in-depth with 8 students (both first-year and senior students) and 5 course instructors followed by a large scale evaluative survey (n=117) using video of the prototype. Our research identified important design challenges such as building trust and preserving privacy, limiting interaction costs, and supporting both immediate and long-term sustainable support. Likewise, we proposed design recommendations that demonstrate context awareness, personalize the experience based on user preferences, and adapt over time as students mature and grow.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {97},
numpages = {32},
keywords = {persuasive technology, behavioral change, study habits and skills, computer science, chatbot}
}

@inproceedings{10.1145/3276954.3276958,
author = {Baudart, Guillaume and Dolby, Julian and Duesterwald, Evelyn and Hirzel, Martin and Shinnar, Avraham},
title = {Protecting Chatbots from Toxic Content},
year = {2018},
isbn = {9781450360319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276954.3276958},
doi = {10.1145/3276954.3276958},
abstract = {There is a paradigm shift in web-based services towards conversational user interfaces. Companies increasingly offer conversational interfaces, or chatbots, to let their customers or employees interact with their services in a more flexible and mobile manner. Unfortunately, this new paradigm faces a major problem, namely toxic content in user inputs. Toxic content in user inputs to chatbots may cause privacy concerns, may be adversarial or malicious, and can cause the chatbot provider substantial economic, reputational, or legal harm. We address this problem with an interdisciplinary approach, drawing upon programming languages, cloud computing, and other disciplines to build protections for chatbots. Our solution, called BotShield, is non-intrusive in that it does not require changes to existing chatbots or underlying conversational platforms. This paper introduces novel security mechanisms, articulates their security guarantees, and illustrates them via case studies.},
booktitle = {Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {99–110},
numpages = {12},
keywords = {Homomorphic Redaction, Chatbot, Context Digression},
location = {Boston, MA, USA},
series = {Onward! 2018}
}

@inproceedings{10.1145/3279972.3279980,
author = {Arora, Priya and Chaspari, Theodora},
title = {Exploring Siamese Neural Network Architectures for Preserving Speaker Identity in Speech Emotion Classification},
year = {2018},
isbn = {9781450360760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279972.3279980},
doi = {10.1145/3279972.3279980},
abstract = {Voice-enabled communication is increasingly being used in real-world applications, such as the ones involving conversational bots or "chatbots". Chatbots can spark and sustain user engagement by effectively recognizing their emotions and acting upon them. However, the majority of emotion recognition systems rely on rich spectrotemporal acoustic features. Beyond the emotion-related information, such systems tend to preserve information relevant to the identity of the speaker, therefore raising major privacy concerns from the users. This paper introduces two hybrid architectures for privacy-preserving emotion recognition from speech. These architectures rely on a Siamese neural network, whose input and intermediate layers are transformed using various privacy-performing operations in order to retain emotion-dependent content and suppress information related to the identity of a speaker. The proposed approach is evaluated through emotion classification and speaker identification performance metrics. Results indicate that the proposed framework can achieve up to 67.4\% for classifying between happy, sad, frustrated, anger, neutral and other emotions, obtained from the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. At the same time, the proposed approach reduces speaker identification accuracy to 50\%, compared to 81\%, the latter being achieved through a feedforward neural network solely trained on the speaker identification task using the same input features.},
booktitle = {Proceedings of the 4th International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction},
pages = {15–18},
numpages = {4},
keywords = {Siamese neural network, multiplicative perturbation, repeated Gombertz Function, Emotionally-aware conversational agents, principal component analysis, speaker identity},
location = {Boulder, CO, USA},
series = {MA3HMI'18}
}

@inproceedings{10.1145/3517745.3561433,
author = {Edu, Jide and Mulligan, Cliona and Pierazzi, Fabio and Polakis, Jason and Suarez-Tangil, Guillermo and Such, Jose},
title = {Exploring the Security and Privacy Risks of Chatbots in Messaging Services},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561433},
doi = {10.1145/3517745.3561433},
abstract = {The unprecedented adoption of messaging platforms for work and recreation has made it an attractive target for malicious actors. In this context, third-party apps (so-called chatbots) offer a variety of attractive functionalities that support the experience in large channels. Unfortunately, under the current permission and deployment models, chatbots in messaging systems could steal information from channels without the victim's awareness. In this paper, we propose a methodology that incorporates static and dynamic analysis for automatically assessing security and privacy issues in messaging platform chatbots. We also provide preliminary findings from the popular Discord platform that highlight the risks that chatbots pose to users. Unlike other popular platforms like Slack or MS Teams, Discord does not implement user-permission checks---a task entrusted to third-party developers. Among others, we find that 55\% of chatbots from a leading Discord repository request the "administrator" permission, and only 4.35\% of chatbots with permissions actually provide a privacy policy.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {581–588},
numpages = {8},
keywords = {security and privacy, discord, chatbots, messaging platorms},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3523150.3523168,
author = {Duvvuri, Venkata and Guan, Qihan and Daddala, Swetha and Harris, Mitch and Kaushik, Sudhakar},
title = {Predicting Depression Symptoms from Discord Chat Messaging Using AI Medical Chatbots},
year = {2022},
isbn = {9781450387477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523150.3523168},
doi = {10.1145/3523150.3523168},
abstract = {Depression is a chronic illness with even Olympic athletes [1] and top tennis players [2] withdrawing from competitions due to it. It's important to diagnose depression early. Traditional methods rely on questionnaires to evaluate depression. But they have their limitations due to inherent bias and inhibitions in self reporting. We propose an intelligent chatbot powered by AI approach to assist medical professionals diagnose depression. Specifically, the AI could predict depression symptoms in conversations with pre-care health care personnel and/or personal chats. Notably, due to sensitivity and privacy regulation (HIPPA) such data is not readily available [3]. To use AI driven medical assistants, and to overcome this limitation in training AI models, we propose to seed the models with recent chat engine (Discord) conversation dataset in the channel #depression. We achieve at least 73\% accuracy in predicting seven key symptoms for depression using our best ML models. Secondly, with our ensemble random forest model we could recall 30-69\% of depression symptoms with 60-99\% depression diagnosis accuracy which could be further tuned by medical professional if they know which ones of these symptoms is a key predictor of depression.},
booktitle = {Proceedings of the 2022 6th International Conference on Machine Learning and Soft Computing},
pages = {111–119},
numpages = {9},
keywords = {Support Vector Machine, Convolution Neural Networks, Random Forest, Artificial Intelligence, Discord chat, Medical Chatbots, Depression},
location = {Haikou, China},
series = {ICMLSC '22}
}

